{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10c850ef-84e0-4082-8285-c14dffb801b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler, Dataset\n",
    "\n",
    "import optuna\n",
    "from metrics import metrics_from_preds\n",
    "from preprocess import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cf8a64e-f46f-4b69-8d17-7bf10c68529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Para reproducibilidad estricta; si alguna op no soporta determinismo, solo avisa.\n",
    "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf5298d0-d82e-425d-98c2-0671ffeff358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cpu\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "# Forzamos CPU para bit-a-bit. (Si luego quieres GPU, cambia a 'cuda' sabiendo que habrá\n",
    "# pequeñas variaciones numéricas.)\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"Usando dispositivo:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5440f63-83e0-4e2f-b1a2-73d280dd9ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vids_training, labels_training, vids_val, labels_val, vids_test, labels_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f311b753-3bcd-4069-b8d2-db64a7a7d4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    \"\"\"Envuelve listas de (video, label).\n",
    "       Espera 'video' con shape (T, F). Devuelve (x: Tensor[T, F], y: LongTensor[]).\"\"\"\n",
    "    def __init__(self, videos, labels):\n",
    "        self.videos = videos\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.videos)\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(np.array(self.videos[idx]), dtype=torch.float32)  # (T, F)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)               # ()\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1401ac0-cf52-4610-b292-a925c729e9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_pad(batch):\n",
    "    \"\"\"Collate simple asumiendo T fijo en el dataset (sin padding).\"\"\"\n",
    "    xs, ys = zip(*batch)\n",
    "    x = torch.stack(xs, dim=0)  # (B, T, F)\n",
    "    y = torch.stack(ys, dim=0)  # (B,)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4eb74413-5ae7-4dc1-b438-1ae54e9786c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48c27f53-7581-49b9-bfd5-04f76b2e404e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VideoDataset(vids_training, labels_training)\n",
    "val_dataset   = VideoDataset(vids_val, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fad0710-5776-43e6-aa97-b09c5628e2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(42)\n",
    "perm = torch.randperm(len(train_dataset), generator=g).tolist()\n",
    "train_sampler = SubsetRandomSampler(perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "314eb590-7974-4b11-9bf2-27449964f9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_seq = DataLoader(\n",
    "    train_dataset, batch_size=32, shuffle=False, sampler=train_sampler,\n",
    "    num_workers=0, collate_fn=collate_pad\n",
    ")\n",
    "val_loader_seq = DataLoader(\n",
    "    val_dataset, batch_size=32, shuffle=False,\n",
    "    num_workers=0, collate_fn=collate_pad\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5245a60-5daa-4628-a54d-b85244285971",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size=3, dilation=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        pad = (kernel_size - 1) * dilation // 2  # 'same' padding aproximado\n",
    "        self.conv = nn.Conv1d(in_ch, out_ch, kernel_size, padding=pad, dilation=dilation)\n",
    "        self.norm = nn.BatchNorm1d(out_ch)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.act  = nn.ReLU(inplace=True)\n",
    "    def forward(self, x):  # x: (B, C, T)\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class MSTCN(nn.Module):\n",
    "    \"\"\"MS-TCN para clasificación de secuencias:\n",
    "       Entrada:  (B, T, F)\n",
    "       Conv1d:   (B, F, T)\n",
    "       Capas:    'layers' bloques con dilaciones 1,2,4,...\n",
    "       Salida:   (B, num_classes)\"\"\"\n",
    "    def __init__(self, in_feat=258, hidden=128, layers=3, dropout=0.1, kernel_size=3, num_classes=4):\n",
    "        super().__init__()\n",
    "        stages = []\n",
    "        in_ch = in_feat\n",
    "        for i in range(layers):\n",
    "            d = 2 ** i\n",
    "            stages.append(TemporalBlock(in_ch, hidden, kernel_size=kernel_size, dilation=d, dropout=dropout))\n",
    "            in_ch = hidden\n",
    "        self.stages = nn.ModuleList(stages)\n",
    "        self.head = nn.Linear(hidden, num_classes)\n",
    "\n",
    "    def forward(self, x):          # x: (B, T, F)\n",
    "        x = x.transpose(1, 2)      # -> (B, F, T)\n",
    "        for blk in self.stages:\n",
    "            x = blk(x)             # (B, hidden, T)\n",
    "        x = x.mean(dim=-1)         # GAP en T -> (B, hidden)\n",
    "        return self.head(x)        # (B, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c69cff2c-f2ac-411a-8272-42d50e245955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_ms_tcn(train_loader_seq, val_loader_seq, n_trials=10, max_epochs=50, save_prefix=\"hpo_ms_tcn\"):\n",
    "    set_seed(42)  # semilla global fija para el estudio\n",
    "\n",
    "    sampler = optuna.samplers.TPESampler(seed=42)\n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=sampler,\n",
    "                                pruner=optuna.pruners.MedianPruner())\n",
    "    all_trials = []\n",
    "\n",
    "    # Detectar automáticamente F (número de features)\n",
    "    sample_x, _ = next(iter(train_loader_seq))\n",
    "    in_feat = sample_x.shape[-1]\n",
    "\n",
    "    def objective(trial):\n",
    "        hidden  = trial.suggest_categorical(\"hidden\", [96, 128, 160, 192])\n",
    "        layers  = trial.suggest_int(\"layers\", 2, 5)\n",
    "        dropout = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
    "        ksz     = trial.suggest_categorical(\"kernel_size\", [3, 5, 7])\n",
    "        lr      = trial.suggest_float(\"lr\", 5e-5, 5e-3, log=True)\n",
    "        wd      = trial.suggest_float(\"weight_decay\", 1e-10, 1e-3, log=True)\n",
    "\n",
    "        model = MSTCN(in_feat=in_feat, hidden=hidden, layers=layers,\n",
    "                      dropout=dropout, kernel_size=ksz, num_classes=4).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "        best_val = 0.0\n",
    "        for epoch in range(max_epochs):\n",
    "            # --- Train ---\n",
    "            model.train()\n",
    "            for xb, yb in train_loader_seq:\n",
    "                xb = xb.to(device); yb = yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                out = model(xb)\n",
    "                loss = criterion(out, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # --- Validation ---\n",
    "            model.eval()\n",
    "            y_true, y_pred = [], []\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_loader_seq:\n",
    "                    xb = xb.to(device)\n",
    "                    preds = model(xb).argmax(dim=1)\n",
    "                    y_true.extend(yb.numpy())\n",
    "                    y_pred.extend(preds.cpu().numpy())\n",
    "            val_acc = metrics_from_preds(y_true, y_pred)[\"accuracy\"]\n",
    "\n",
    "            if val_acc > best_val:\n",
    "                best_val = val_acc\n",
    "\n",
    "            trial.report(val_acc, epoch)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "        all_trials.append({\"trial\": trial.number, \"params\": trial.params, \"best_val_acc\": best_val})\n",
    "        return best_val\n",
    "\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    # Guardar resultados\n",
    "    with open(f\"{save_prefix}_all.json\", \"w\") as f:\n",
    "        json.dump(all_trials, f, indent=2)\n",
    "    with open(f\"{save_prefix}_best.json\", \"w\") as f:\n",
    "        json.dump({\"best_params\": study.best_params, \"best_value\": study.best_value}, f, indent=2)\n",
    "\n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfe2d91a-5451-4ebb-94d5-6a582d128e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-30 21:42:31,756] A new study created in memory with name: no-name-be42e895-5cc6-4704-8355-a32c47a06e73\n",
      "[I 2025-11-30 21:43:19,279] Trial 0 finished with value: 0.9166666666666666 and parameters: {'hidden': 128, 'layers': 2, 'dropout': 0.07799726016810132, 'kernel_size': 5, 'lr': 0.0013035123791853842, 'weight_decay': 1.3934502251337584e-10}. Best is trial 0 with value: 0.9166666666666666.\n",
      "[I 2025-11-30 21:43:44,407] Trial 1 finished with value: 0.9166666666666666 and parameters: {'hidden': 96, 'layers': 2, 'dropout': 0.15212112147976886, 'kernel_size': 3, 'lr': 0.0008369042894376068, 'weight_decay': 9.472334467618544e-10}. Best is trial 0 with value: 0.9166666666666666.\n",
      "[I 2025-11-30 21:44:56,239] Trial 2 finished with value: 0.9111111111111111 and parameters: {'hidden': 192, 'layers': 2, 'dropout': 0.2571172192068058, 'kernel_size': 7, 'lr': 0.0001096524277832185, 'weight_decay': 2.853390105240223e-10}. Best is trial 0 with value: 0.9166666666666666.\n",
      "[I 2025-11-30 21:45:41,604] Trial 3 finished with value: 0.9222222222222223 and parameters: {'hidden': 128, 'layers': 2, 'dropout': 0.34211651325607845, 'kernel_size': 7, 'lr': 5.8579686961535335e-05, 'weight_decay': 0.0002318690670290197}. Best is trial 3 with value: 0.9222222222222223.\n",
      "[I 2025-11-30 21:46:41,117] Trial 4 finished with value: 0.9277777777777778 and parameters: {'hidden': 128, 'layers': 4, 'dropout': 0.09242722776276352, 'kernel_size': 3, 'lr': 0.0030805247696904818, 'weight_decay': 1.5321449415450716e-06}. Best is trial 4 with value: 0.9277777777777778.\n",
      "[I 2025-11-30 21:46:41,995] Trial 5 pruned. \n",
      "[I 2025-11-30 21:46:54,562] Trial 6 pruned. \n",
      "[I 2025-11-30 21:46:57,821] Trial 7 pruned. \n",
      "[I 2025-11-30 21:46:58,809] Trial 8 pruned. \n",
      "[I 2025-11-30 21:47:01,130] Trial 9 pruned. \n",
      "[I 2025-11-30 21:47:48,890] Trial 10 finished with value: 0.9166666666666666 and parameters: {'hidden': 128, 'layers': 3, 'dropout': 0.0037736596581466053, 'kernel_size': 3, 'lr': 0.004637678633541706, 'weight_decay': 2.5531292949199678e-08}. Best is trial 4 with value: 0.9277777777777778.\n",
      "[I 2025-11-30 21:47:50,185] Trial 11 pruned. \n",
      "[I 2025-11-30 21:47:51,667] Trial 12 pruned. \n",
      "[I 2025-11-30 21:47:52,923] Trial 13 pruned. \n",
      "[I 2025-11-30 21:48:09,401] Trial 14 pruned. \n",
      "[I 2025-11-30 21:48:10,887] Trial 15 pruned. \n",
      "[I 2025-11-30 21:48:11,906] Trial 16 pruned. \n",
      "[I 2025-11-30 21:48:12,903] Trial 17 pruned. \n",
      "[I 2025-11-30 21:48:15,542] Trial 18 pruned. \n",
      "[I 2025-11-30 21:48:17,446] Trial 19 pruned. \n",
      "[I 2025-11-30 21:49:13,267] Trial 20 finished with value: 0.9055555555555556 and parameters: {'hidden': 128, 'layers': 2, 'dropout': 0.4131335100926455, 'kernel_size': 7, 'lr': 0.000742859113095686, 'weight_decay': 1.3274412053286292e-07}. Best is trial 4 with value: 0.9277777777777778.\n",
      "[I 2025-11-30 21:49:14,298] Trial 21 pruned. \n",
      "[I 2025-11-30 21:49:15,356] Trial 22 pruned. \n",
      "[I 2025-11-30 21:50:14,597] Trial 23 finished with value: 0.9166666666666666 and parameters: {'hidden': 128, 'layers': 3, 'dropout': 0.1400581744958521, 'kernel_size': 5, 'lr': 0.0009387331318914297, 'weight_decay': 2.5677062988088583e-09}. Best is trial 4 with value: 0.9277777777777778.\n",
      "[I 2025-11-30 21:50:15,335] Trial 24 pruned. \n",
      "[I 2025-11-30 21:50:16,350] Trial 25 pruned. \n",
      "[I 2025-11-30 21:50:17,947] Trial 26 pruned. \n",
      "[I 2025-11-30 21:50:52,386] Trial 27 finished with value: 0.9444444444444444 and parameters: {'hidden': 96, 'layers': 2, 'dropout': 0.31953519251095136, 'kernel_size': 7, 'lr': 0.0002591144667122849, 'weight_decay': 1.9440339271153755e-07}. Best is trial 27 with value: 0.9444444444444444.\n",
      "[I 2025-11-30 21:50:53,274] Trial 28 pruned. \n",
      "[I 2025-11-30 21:50:53,941] Trial 29 pruned. \n",
      "[I 2025-11-30 21:50:54,597] Trial 30 pruned. \n",
      "[I 2025-11-30 21:50:55,079] Trial 31 pruned. \n",
      "[I 2025-11-30 21:50:55,743] Trial 32 pruned. \n",
      "[I 2025-11-30 21:51:51,561] Trial 33 finished with value: 0.9388888888888889 and parameters: {'hidden': 160, 'layers': 2, 'dropout': 0.035846429868023603, 'kernel_size': 7, 'lr': 0.00034521429763049235, 'weight_decay': 9.996832227896143e-09}. Best is trial 27 with value: 0.9444444444444444.\n",
      "[I 2025-11-30 21:51:52,735] Trial 34 pruned. \n",
      "[I 2025-11-30 21:51:53,832] Trial 35 pruned. \n",
      "[I 2025-11-30 21:51:55,377] Trial 36 pruned. \n",
      "[I 2025-11-30 21:51:56,749] Trial 37 pruned. \n",
      "[I 2025-11-30 21:51:58,746] Trial 38 pruned. \n",
      "[I 2025-11-30 21:51:59,207] Trial 39 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor resultado: 0.9444444444444444\n",
      "Mejores hiperparámetros: {'hidden': 96, 'layers': 2, 'dropout': 0.31953519251095136, 'kernel_size': 7, 'lr': 0.0002591144667122849, 'weight_decay': 1.9440339271153755e-07}\n",
      "Epoch 001 | Val Acc: 0.7222\n",
      "Epoch 002 | Val Acc: 0.7556\n",
      "Epoch 003 | Val Acc: 0.8611\n",
      "Epoch 004 | Val Acc: 0.5889\n",
      "Epoch 005 | Val Acc: 0.7500\n",
      "Epoch 006 | Val Acc: 0.9000\n",
      "Epoch 007 | Val Acc: 0.8500\n",
      "Epoch 008 | Val Acc: 0.7722\n",
      "Epoch 009 | Val Acc: 0.8500\n",
      "Epoch 010 | Val Acc: 0.8056\n",
      "Epoch 011 | Val Acc: 0.8222\n",
      "Epoch 012 | Val Acc: 0.6722\n",
      "Epoch 013 | Val Acc: 0.7556\n",
      "Epoch 014 | Val Acc: 0.8944\n",
      "Epoch 015 | Val Acc: 0.7944\n",
      "Epoch 016 | Val Acc: 0.7056\n",
      "Epoch 017 | Val Acc: 0.6167\n",
      "Epoch 018 | Val Acc: 0.8056\n",
      "Epoch 019 | Val Acc: 0.8389\n",
      "Epoch 020 | Val Acc: 0.7667\n",
      "Epoch 021 | Val Acc: 0.8278\n",
      "Epoch 022 | Val Acc: 0.6389\n",
      "Epoch 023 | Val Acc: 0.8500\n",
      "Epoch 024 | Val Acc: 0.8556\n",
      "Epoch 025 | Val Acc: 0.8111\n",
      "Epoch 026 | Val Acc: 0.9000\n",
      "Epoch 027 | Val Acc: 0.7722\n",
      "Epoch 028 | Val Acc: 0.6889\n",
      "Epoch 029 | Val Acc: 0.7333\n",
      "Epoch 030 | Val Acc: 0.8389\n",
      "Epoch 031 | Val Acc: 0.8056\n",
      "Epoch 032 | Val Acc: 0.7167\n",
      "Epoch 033 | Val Acc: 0.8833\n",
      "Epoch 034 | Val Acc: 0.8222\n",
      "Epoch 035 | Val Acc: 0.7944\n",
      "Epoch 036 | Val Acc: 0.8222\n",
      "Epoch 037 | Val Acc: 0.7778\n",
      "Epoch 038 | Val Acc: 0.8833\n",
      "Epoch 039 | Val Acc: 0.9111\n",
      "Epoch 040 | Val Acc: 0.8667\n",
      "Epoch 041 | Val Acc: 0.8444\n",
      "Epoch 042 | Val Acc: 0.8556\n",
      "Epoch 043 | Val Acc: 0.8722\n",
      "Epoch 044 | Val Acc: 0.7056\n",
      "Epoch 045 | Val Acc: 0.9333\n",
      "Epoch 046 | Val Acc: 0.7722\n",
      "Epoch 047 | Val Acc: 0.8778\n",
      "Epoch 048 | Val Acc: 0.7889\n",
      "Epoch 049 | Val Acc: 0.8556\n",
      "Epoch 050 | Val Acc: 0.7556\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    study_ms_tcn = optimize_ms_tcn(train_loader_seq, val_loader_seq, n_trials=40, max_epochs=50)\n",
    "    print(\"Mejor resultado:\", study_ms_tcn.best_value)\n",
    "    print(\"Mejores hiperparámetros:\", study_ms_tcn.best_params)\n",
    "\n",
    "    # =============================================================================\n",
    "    # [9] Reentrenar el mejor modelo de forma 100% reproducible\n",
    "    # =============================================================================\n",
    "    best = study_ms_tcn.best_params\n",
    "    set_seed(42)  # entrenamiento único reproducible\n",
    "\n",
    "    # Recuperar in_feat\n",
    "    sample_x, _ = next(iter(train_loader_seq))\n",
    "    in_feat = sample_x.shape[-1]\n",
    "\n",
    "    model = MSTCN(\n",
    "        in_feat=in_feat,\n",
    "        hidden=best[\"hidden\"],\n",
    "        layers=best[\"layers\"],\n",
    "        dropout=best[\"dropout\"],\n",
    "        kernel_size=best[\"kernel_size\"],\n",
    "        num_classes=4\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=best[\"lr\"], weight_decay=best[\"weight_decay\"])\n",
    "\n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader_seq:\n",
    "            xb = xb.to(device); yb = yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        y_true, y_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader_seq:\n",
    "                xb = xb.to(device)\n",
    "                preds = model(xb).argmax(dim=1)\n",
    "                y_true.extend(yb.numpy())\n",
    "                y_pred.extend(preds.cpu().numpy())\n",
    "        val_acc = metrics_from_preds(y_true, y_pred)[\"accuracy\"]\n",
    "        print(f\"Epoch {epoch+1:03d} | Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b7e7fdc-8268-4038-a87c-b142948f2221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando los mejores hiperparámetros desde hpo_ms_tcn_best.json...\n",
      "Hiperparámetros cargados:\n",
      "{'hidden': 96, 'layers': 2, 'dropout': 0.31953519251095136, 'kernel_size': 7, 'lr': 0.0002591144667122849, 'weight_decay': 1.9440339271153755e-07}\n",
      "\n",
      "Entrenando el mejor modelo MS-TCN durante 50 épocas...\n",
      "Epoch [10/50], Loss: 0.2867\n",
      "Epoch [20/50], Loss: 0.1714\n",
      "Epoch [30/50], Loss: 0.1318\n",
      "Epoch [40/50], Loss: 0.0701\n",
      "Epoch [50/50], Loss: 0.0911\n",
      "Entrenamiento completado.\n",
      "\n",
      "--- Métricas del Modelo Final (MS-TCN) ---\n",
      "\n",
      "Resultados en el conjunto de Train:\n",
      "  accuracy: 0.9461\n",
      "  precision_macro: 0.9508\n",
      "  recall_macro: 0.9461\n",
      "  specificity_macro: 0.9820\n",
      "  f1_macro: 0.9458\n",
      "  balanced_accuracy: 0.9461\n",
      "  confusion_matrix: [[252, 1, 0, 2], [0, 246, 0, 9], [2, 38, 214, 1], [1, 1, 0, 253]]\n",
      "\n",
      "Resultados en el conjunto de Validation:\n",
      "  accuracy: 0.9000\n",
      "  precision_macro: 0.9220\n",
      "  recall_macro: 0.9000\n",
      "  specificity_macro: 0.9667\n",
      "  f1_macro: 0.9020\n",
      "  balanced_accuracy: 0.9000\n",
      "  confusion_matrix: [[42, 0, 0, 3], [0, 34, 0, 11], [1, 0, 42, 2], [1, 0, 0, 44]]\n",
      "\n",
      "Modelo guardado exitosamente en: ms_tcn_best_model.pth\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from metrics import metrics_from_preds\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# --- Cargar los mejores hiperparámetros ---\n",
    "print(\"Cargando los mejores hiperparámetros desde hpo_ms_tcn_best.json...\")\n",
    "try:\n",
    "    with open(\"hpo_ms_tcn_best.json\", \"r\") as f:\n",
    "        best_hyperparams = json.load(f)[\"best_params\"]\n",
    "    print(\"Hiperparámetros cargados:\")\n",
    "    print(best_hyperparams)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: El archivo 'hpo_ms_tcn_best.json' no fue encontrado.\")\n",
    "    print(\"Por favor, ejecuta primero la celda de optimización para generar este archivo.\")\n",
    "    raise\n",
    "\n",
    "# --- Configurar y entrenar el mejor modelo ---\n",
    "set_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Es necesario determinar el número de features de entrada desde los datos\n",
    "try:\n",
    "    sample_x, _ = next(iter(train_loader_seq))\n",
    "    in_feat = sample_x.shape[-1]\n",
    "except NameError:\n",
    "    print(\"Error: 'train_loader_seq' no está definido. Asegúrate de haber ejecutado las celdas de preparación de datos.\")\n",
    "    raise\n",
    "\n",
    "# Instanciar el modelo con los mejores hiperparámetros\n",
    "best_model = MSTCN(\n",
    "    in_feat=in_feat,\n",
    "    hidden=best_hyperparams[\"hidden\"],\n",
    "    layers=best_hyperparams[\"layers\"],\n",
    "    dropout=best_hyperparams[\"dropout\"],\n",
    "    kernel_size=best_hyperparams[\"kernel_size\"],\n",
    "    num_classes=4\n",
    ").to(device)\n",
    "\n",
    "# Definir criterio de pérdida y optimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(\n",
    "    best_model.parameters(),\n",
    "    lr=best_hyperparams[\"lr\"],\n",
    "    weight_decay=best_hyperparams[\"weight_decay\"]\n",
    ")\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "epochs = 50  # Usamos las mismas épocas que en la búsqueda de hiperparámetros\n",
    "print(f\"\\nEntrenando el mejor modelo MS-TCN durante {epochs} épocas...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    best_model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_loader_seq:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = best_model(xb)\n",
    "        loss = criterion(out, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss / len(train_loader_seq):.4f}\")\n",
    "\n",
    "print(\"Entrenamiento completado.\")\n",
    "\n",
    "# --- Evaluación del modelo y cálculo de métricas ---\n",
    "best_model.eval()\n",
    "all_metrics = {}\n",
    "\n",
    "def evaluate_seq_model(loader, model_name):\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            predictions = best_model(xb).argmax(dim=1)\n",
    "            y_true.extend(yb.numpy())\n",
    "            y_pred.extend(predictions.cpu().numpy())\n",
    "    \n",
    "    metrics = metrics_from_preds(y_true, y_pred)\n",
    "    all_metrics[model_name] = metrics\n",
    "\n",
    "print(\"\\n--- Métricas del Modelo Final (MS-TCN) ---\")\n",
    "evaluate_seq_model(train_loader_seq, \"Train\")\n",
    "evaluate_seq_model(val_loader_seq, \"Validation\")\n",
    "\n",
    "for split_name, metrics in all_metrics.items():\n",
    "    print(f\"\\nResultados en el conjunto de {split_name}:\")\n",
    "    for metric_name, value in metrics.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {metric_name}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {metric_name}: {value}\")\n",
    "\n",
    "# --- Guardar el modelo entrenado ---\n",
    "model_save_path = \"ms_tcn_best_model.pth\"\n",
    "torch.save(best_model.state_dict(), model_save_path)\n",
    "print(f\"\\nModelo guardado exitosamente en: {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2064fc-a886-4b6e-bd68-2b6ca19d7af0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
