import json
from metrics import metrics_from_preds
import torch
import torch.nn as nn
import torch.optim as optim

# --- Cargar los mejores hiperparámetros ---
print("Cargando los mejores hiperparámetros desde hpo_lstm_best.json...")
try:
    with open("hpo_lstm_best.json", "r") as f:
        best_hyperparams = json.load(f)["best_params"]
    print("Hiperparámetros cargados:")
    print(best_hyperparams)
except FileNotFoundError:
    print("Error: El archivo 'hpo_lstm_best.json' no fue encontrado.")
    print("Por favor, ejecuta primero la celda de optimización para generar este archivo.")
    raise

# --- Configurar y entrenar el mejor modelo ---
set_seed(42)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Es necesario determinar el número de features de entrada desde los datos
try:
    sample_x, _ = next(iter(train_loader_seq))
    input_size = sample_x.shape[-1]
except NameError:
    print("Error: 'train_loader_seq' no está definido. Asegúrate de haber ejecutado las celdas de preparación de datos.")
    raise

# Instanciar el modelo con los mejores hiperparámetros
best_model = LSTMClassifier(
    input_size=input_size,
    hidden_size=best_hyperparams["hidden_size"],
    num_layers=best_hyperparams["num_layers"],
    dropout=best_hyperparams["dropout"],
    bidirectional=best_hyperparams["bidirectional"],
    num_classes=4
).to(device)

# Definir criterio de pérdida y optimizador
criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(
    best_model.parameters(),
    lr=best_hyperparams["lr"],
    weight_decay=best_hyperparams["weight_decay"]
)

# Entrenamiento del modelo
epochs = 50  # Usamos las mismas épocas que en la búsqueda de hiperparámetros
print(f"\nEntrenando el mejor modelo LSTM durante {epochs} épocas...")

for epoch in range(epochs):
    best_model.train()
    total_loss = 0
    for xb, yb in train_loader_seq:
        xb, yb = xb.to(device), yb.to(device)
        optimizer.zero_grad()
        out = best_model(xb)
        loss = criterion(out, yb)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    
    if (epoch + 1) % 10 == 0:
        print(f"Epoch [{epoch+1}/{epochs}], Loss: {total_loss / len(train_loader_seq):.4f}")

print("Entrenamiento completado.")

# --- Evaluación del modelo y cálculo de métricas ---
best_model.eval()
all_metrics = {}

def evaluate_seq_model(loader, model_name):
    y_true, y_pred = [], []
    with torch.no_grad():
        for xb, yb in loader:
            xb = xb.to(device)
            predictions = best_model(xb).argmax(dim=1)
            y_true.extend(yb.numpy())
            y_pred.extend(predictions.cpu().numpy())
    
    metrics = metrics_from_preds(y_true, y_pred)
    all_metrics[model_name] = metrics

print("\n--- Métricas del Modelo Final (LSTM) ---")
evaluate_seq_model(train_loader_seq, "Train")
evaluate_seq_model(val_loader_seq, "Validation")

for split_name, metrics in all_metrics.items():
    print(f"\nResultados en el conjunto de {split_name}:")
    for metric_name, value in metrics.items():
        if isinstance(value, float):
            print(f"  {metric_name}: {value:.4f}")
        else:
            print(f"  {metric_name}: {value}")

# --- Guardar el modelo entrenado ---
model_save_path = "lstm_best_model.pth"
torch.save(best_model.state_dict(), model_save_path)
print(f"\nModelo guardado exitosamente en: {model_save_path}")
