{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b41868ed-50f1-451d-8a9e-de339562f3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import math\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# GCN\n",
    "import torch_geometric.nn as geom_nn\n",
    "\n",
    "# TCN\n",
    "from pytorch_tcn import TCN\n",
    "\n",
    "# Métricas y optimización\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaaedc7-1beb-466d-b2f3-0b3ee4c02f9c",
   "metadata": {},
   "source": [
    "# Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efe0935f-e427-42a0-a2bf-96d72fbc2ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de rutas y clases\n",
    "DATA_PATH = os.path.join(\"data\")\n",
    "actions = np.array(['TouchingChest', 'Hit', 'AleatoryMovement', 'Static'])\n",
    "n_people = 25\n",
    "\n",
    "# Configuración de dataset\n",
    "no_vids_pp = 15\n",
    "no_vids = n_people * no_vids_pp\n",
    "vid_length = 16\n",
    "label_map = {label: idx for idx, label in enumerate(actions)}\n",
    "\n",
    "# Partición de datos\n",
    "no_vid_for_training = 255 # 68% del conjunto total\n",
    "no_vid_for_val = 45 # 12% del conjunto total\n",
    "no_vid_for_test = 75 # 20% del conjunto total\n",
    "\n",
    "# Función auxiliar para cargar un conjunto (train/val/test)\n",
    "def load_videos(start_idx, end_idx, actions, data_path, vid_length, label_map):\n",
    "    videos, labels = [], []\n",
    "    for vid in range(start_idx, end_idx):\n",
    "        for action in actions:\n",
    "            frames = [\n",
    "                np.load(os.path.join(data_path, action, str(vid), f\"{frame_num}.npy\"))\n",
    "                for frame_num in range(vid_length)\n",
    "            ]\n",
    "            videos.append(frames)\n",
    "            labels.append(label_map[action])\n",
    "    return videos, labels\n",
    "\n",
    "# Cargar datasets\n",
    "vids_training, labels_training = load_videos(0, no_vid_for_training, actions, DATA_PATH, vid_length, label_map)\n",
    "vids_val, labels_val = load_videos(no_vid_for_training, no_vid_for_training + no_vid_for_val, actions, DATA_PATH, vid_length, label_map)\n",
    "vids_test, labels_test = load_videos(no_vid_for_training + no_vid_for_val, no_vids, actions, DATA_PATH, vid_length, label_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16940405-09f1-4bdd-8ff8-5d69f36a154f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1020\n",
      "180\n",
      "300\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "print(len(vids_training))\n",
    "print(len(vids_val))\n",
    "print(len(vids_test))\n",
    "\n",
    "print(len(vids_training) + len(vids_val) + len(vids_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3fd8e18-f554-4fc7-8dcb-06ba38592487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "[0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "print(len(labels_test))\n",
    "print(labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de97f5d3-01ff-46f5-9b03-1397bf457f8f",
   "metadata": {},
   "source": [
    "# Modelo 1: Transformer (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c9b11f9-a63e-47cf-91fc-242a7a10f634",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding sinusoidal para secuencias cortas (L=16).\"\"\"\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 16):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)  # (L, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)  # (L, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)   # pares\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)   # impares\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))    # (1, L, d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, L, d_model)\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ab8e872-4d2a-4ca1-8af2-ef9d976d91ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBaseline(nn.Module):\n",
    "    \"\"\"Clasificador con Transformer Encoder.\n",
    "    Entrada: (B, 16, 258)  -> Salida: logits (B, 4)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int = 258,\n",
    "        seq_len: int = 16,\n",
    "        num_classes: int = 4,\n",
    "        d_model: int = 128,\n",
    "        nhead: int = 4,\n",
    "        num_layers: int = 2,\n",
    "        dim_feedforward: int = 256,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # Proyección a la dimensión del modelo\n",
    "        self.input_proj = nn.Linear(in_features, d_model)\n",
    "\n",
    "        # PE + Encoder\n",
    "        self.pos_enc = PositionalEncoding(d_model, dropout=dropout, max_len=seq_len)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,   # acepta (B, L, E)\n",
    "            norm_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Cabeza de clasificación\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x esperado: (B, 16, 258)\n",
    "        x = self.input_proj(x)              # (B, L, d_model)\n",
    "        x = self.pos_enc(x)                 # (B, L, d_model)\n",
    "        x = self.encoder(x)                 # (B, L, d_model)\n",
    "        x = x.mean(dim=1)                   # pooling temporal -> (B, d_model)\n",
    "        logits = self.head(x)               # (B, num_classes)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "811890e0-a3e3-466a-ae41-be1687f3526f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "B = 8\n",
    "dummy = torch.randn(B, 16, 258)  # (batch, seq_len, features)\n",
    "model = TransformerBaseline()\n",
    "out = model(dummy)\n",
    "print(out.shape)  # torch.Size([8, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a7b0163-0c67-431a-852f-99c507751a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64dbffe7-fec1-49f4-a1a7-85a1c83fb7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dataset ---\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, videos, labels):\n",
    "        self.videos = videos\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.videos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.videos[idx], dtype=torch.float32)  # (16, 258)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44b991b3-12eb-48eb-bfe5-ff419ecc1861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear datasets\n",
    "train_dataset = VideoDataset(vids_training, labels_training)\n",
    "val_dataset   = VideoDataset(vids_val, labels_val)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2546856d-f355-44fe-9d46-f1788179bc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Modelo ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerBaseline().to(device)\n",
    "\n",
    "# --- Optimizer + Loss ---\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77a8cc74-7be8-4cf0-898c-ea16dbc98bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5736/1318499046.py:11: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
      "  x = torch.tensor(self.videos[idx], dtype=torch.float32)  # (16, 258)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=1.3877, Val Acc=0.4000\n",
      "Epoch 2: Train Loss=1.0412, Val Acc=0.7056\n",
      "Epoch 3: Train Loss=0.8809, Val Acc=0.7778\n",
      "Epoch 4: Train Loss=0.7484, Val Acc=0.7556\n",
      "Epoch 5: Train Loss=0.7177, Val Acc=0.8000\n",
      "Epoch 6: Train Loss=0.6343, Val Acc=0.7889\n",
      "Epoch 7: Train Loss=0.6424, Val Acc=0.7667\n",
      "Epoch 8: Train Loss=0.5949, Val Acc=0.8056\n",
      "Epoch 9: Train Loss=0.5259, Val Acc=0.8333\n",
      "Epoch 10: Train Loss=0.5428, Val Acc=0.8556\n",
      "Epoch 11: Train Loss=0.4960, Val Acc=0.8444\n",
      "Epoch 12: Train Loss=0.4529, Val Acc=0.8278\n",
      "Epoch 13: Train Loss=0.4187, Val Acc=0.8556\n",
      "Epoch 14: Train Loss=0.3683, Val Acc=0.8444\n",
      "Epoch 15: Train Loss=0.3492, Val Acc=0.9000\n",
      "Epoch 16: Train Loss=0.3780, Val Acc=0.8667\n",
      "Epoch 17: Train Loss=0.3019, Val Acc=0.8556\n",
      "Epoch 18: Train Loss=0.2841, Val Acc=0.8778\n",
      "Epoch 19: Train Loss=0.2780, Val Acc=0.8722\n",
      "Epoch 20: Train Loss=0.2659, Val Acc=0.8889\n",
      "Epoch 21: Train Loss=0.2283, Val Acc=0.8556\n",
      "Epoch 22: Train Loss=0.2396, Val Acc=0.8778\n",
      "Epoch 23: Train Loss=0.2118, Val Acc=0.8389\n",
      "Epoch 24: Train Loss=0.2312, Val Acc=0.9000\n",
      "Epoch 25: Train Loss=0.2199, Val Acc=0.8944\n",
      "Epoch 26: Train Loss=0.2207, Val Acc=0.8833\n",
      "Epoch 27: Train Loss=0.2025, Val Acc=0.8944\n",
      "Epoch 28: Train Loss=0.1746, Val Acc=0.8778\n",
      "Epoch 29: Train Loss=0.1296, Val Acc=0.9167\n",
      "Epoch 30: Train Loss=0.1367, Val Acc=0.8889\n",
      "Epoch 31: Train Loss=0.1539, Val Acc=0.9056\n",
      "Epoch 32: Train Loss=0.1572, Val Acc=0.8944\n",
      "Epoch 33: Train Loss=0.1553, Val Acc=0.9111\n",
      "Epoch 34: Train Loss=0.1146, Val Acc=0.8667\n",
      "Epoch 35: Train Loss=0.1243, Val Acc=0.9056\n",
      "Epoch 36: Train Loss=0.1684, Val Acc=0.8500\n",
      "Epoch 37: Train Loss=0.1105, Val Acc=0.8611\n",
      "Epoch 38: Train Loss=0.1154, Val Acc=0.9167\n",
      "Epoch 39: Train Loss=0.1682, Val Acc=0.9000\n",
      "Epoch 40: Train Loss=0.1217, Val Acc=0.8889\n",
      "Epoch 41: Train Loss=0.1461, Val Acc=0.8944\n",
      "Epoch 42: Train Loss=0.1034, Val Acc=0.9222\n",
      "Epoch 43: Train Loss=0.0506, Val Acc=0.9111\n",
      "Epoch 44: Train Loss=0.0311, Val Acc=0.9056\n",
      "Epoch 45: Train Loss=0.0457, Val Acc=0.8889\n",
      "Epoch 46: Train Loss=0.0735, Val Acc=0.8500\n",
      "Epoch 47: Train Loss=0.0912, Val Acc=0.9056\n",
      "Epoch 48: Train Loss=0.0739, Val Acc=0.8889\n",
      "Epoch 49: Train Loss=0.0996, Val Acc=0.8444\n",
      "Epoch 50: Train Loss=0.0966, Val Acc=0.9000\n",
      "Epoch 51: Train Loss=0.1464, Val Acc=0.8944\n",
      "Epoch 52: Train Loss=0.1117, Val Acc=0.8944\n",
      "Epoch 53: Train Loss=0.1141, Val Acc=0.9278\n",
      "Epoch 54: Train Loss=0.0899, Val Acc=0.8667\n",
      "Epoch 55: Train Loss=0.0545, Val Acc=0.9333\n",
      "Epoch 56: Train Loss=0.0729, Val Acc=0.9056\n",
      "Epoch 57: Train Loss=0.0527, Val Acc=0.9056\n",
      "Epoch 58: Train Loss=0.0594, Val Acc=0.8667\n",
      "Epoch 59: Train Loss=0.0446, Val Acc=0.9056\n",
      "Epoch 60: Train Loss=0.0285, Val Acc=0.8889\n",
      "Epoch 61: Train Loss=0.0779, Val Acc=0.8722\n",
      "Epoch 62: Train Loss=0.1474, Val Acc=0.8667\n",
      "Epoch 63: Train Loss=0.1164, Val Acc=0.8778\n",
      "Epoch 64: Train Loss=0.0779, Val Acc=0.9056\n",
      "Epoch 65: Train Loss=0.0335, Val Acc=0.8944\n",
      "Epoch 66: Train Loss=0.0509, Val Acc=0.8889\n",
      "Epoch 67: Train Loss=0.0744, Val Acc=0.8278\n",
      "Epoch 68: Train Loss=0.0728, Val Acc=0.8889\n",
      "Epoch 69: Train Loss=0.0432, Val Acc=0.9056\n",
      "Epoch 70: Train Loss=0.0420, Val Acc=0.8944\n",
      "Epoch 71: Train Loss=0.0665, Val Acc=0.9000\n",
      "Epoch 72: Train Loss=0.0519, Val Acc=0.9111\n",
      "Epoch 73: Train Loss=0.0750, Val Acc=0.9056\n",
      "Epoch 74: Train Loss=0.0365, Val Acc=0.8778\n",
      "Epoch 75: Train Loss=0.0546, Val Acc=0.8722\n",
      "Epoch 76: Train Loss=0.0681, Val Acc=0.8278\n",
      "Epoch 77: Train Loss=0.0424, Val Acc=0.9000\n",
      "Epoch 78: Train Loss=0.0282, Val Acc=0.8889\n",
      "Epoch 79: Train Loss=0.0625, Val Acc=0.8611\n",
      "Epoch 80: Train Loss=0.0737, Val Acc=0.9222\n",
      "Epoch 81: Train Loss=0.0502, Val Acc=0.8500\n",
      "Epoch 82: Train Loss=0.0402, Val Acc=0.9167\n",
      "Epoch 83: Train Loss=0.0328, Val Acc=0.9000\n",
      "Epoch 84: Train Loss=0.0452, Val Acc=0.9111\n",
      "Epoch 85: Train Loss=0.1174, Val Acc=0.8667\n",
      "Epoch 86: Train Loss=0.0609, Val Acc=0.9000\n",
      "Epoch 87: Train Loss=0.0506, Val Acc=0.9056\n",
      "Epoch 88: Train Loss=0.0212, Val Acc=0.8944\n",
      "Epoch 89: Train Loss=0.0240, Val Acc=0.8944\n",
      "Epoch 90: Train Loss=0.0334, Val Acc=0.8722\n",
      "Epoch 91: Train Loss=0.0313, Val Acc=0.8889\n",
      "Epoch 92: Train Loss=0.0328, Val Acc=0.8778\n",
      "Epoch 93: Train Loss=0.0087, Val Acc=0.8722\n",
      "Epoch 94: Train Loss=0.0048, Val Acc=0.9222\n",
      "Epoch 95: Train Loss=0.0042, Val Acc=0.8778\n",
      "Epoch 96: Train Loss=0.0169, Val Acc=0.9000\n",
      "Epoch 97: Train Loss=0.0724, Val Acc=0.8778\n",
      "Epoch 98: Train Loss=0.1115, Val Acc=0.9056\n",
      "Epoch 99: Train Loss=0.0878, Val Acc=0.8722\n",
      "Epoch 100: Train Loss=0.0177, Val Acc=0.9056\n"
     ]
    }
   ],
   "source": [
    "# --- Loop entrenamiento (baseline, pocos epochs) ---\n",
    "for epoch in range(100):  # valores arbitrarios\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # --- Validación ---\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            outputs = model(X)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    val_acc = correct / total\n",
    "    print(f\"Epoch {epoch+1}: Train Loss={avg_loss:.4f}, Val Acc={val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377a6d8e-649c-461d-8641-a220be360a2a",
   "metadata": {},
   "source": [
    "# Modelo 2: GCN (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b30cc30f-f5ea-4c99-8404-c0cc3b952bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports específicos para GCN (aditivos, sin conflictos) ---\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader as GeoDataLoader  # evita conflicto con torch.utils.data.DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3dcbd6fd-1273-467e-9c7f-9ee9eb4d8bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNBaseline(nn.Module):\n",
    "    \"\"\"GCN simple para clasificación de 4 clases.\n",
    "    Entrada: grafo de un frame/video -> salida: logits (B, 4)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=258, hidden_channels=256, num_classes=4):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = nn.Linear(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # x: nodos (N, in_channels), edge_index: aristas, batch: asignación de nodos a grafos\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)  # pooling global\n",
    "        return self.lin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4c016f6-7083-4e78-8430-0f85ff489251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "\n",
    "def build_graph(frames, label):\n",
    "    \"\"\"\n",
    "    Convierte un video (16 x 258) en un grafo:\n",
    "    - 16 nodos (uno por frame).\n",
    "    - Cada nodo con 258 features.\n",
    "    - Aristas entre frames consecutivos.\n",
    "    \"\"\"\n",
    "    x = torch.tensor(frames, dtype=torch.float32)  # (16, 258)\n",
    "\n",
    "    # Conexiones secuenciales (cadena temporal)\n",
    "    edge_index = []\n",
    "    for i in range(len(frames) - 1):\n",
    "        edge_index.append([i, i+1])\n",
    "        edge_index.append([i+1, i])\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()  # (2, E)\n",
    "\n",
    "    y = torch.tensor([label], dtype=torch.long)  # etiqueta del grafo\n",
    "    return Data(x=x, edge_index=edge_index, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9aca56f6-b830-4f0c-b5ca-b006711ff736",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoGraphDataset(InMemoryDataset):\n",
    "    def __init__(self, videos, labels, transform=None):\n",
    "        self.videos = videos\n",
    "        self.labels = labels\n",
    "        super().__init__('.', transform)\n",
    "\n",
    "        self.data_list = [build_graph(v, l) for v, l in zip(videos, labels)]\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def get(self, idx):\n",
    "        return self.data_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1cd7f103-0f53-40fe-8101-25d49e76bf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2970/3748798589.py:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
      "  x = torch.tensor(frames, dtype=torch.float32)  # (16, 258)\n"
     ]
    }
   ],
   "source": [
    "# Crear datasets para train/val\n",
    "train_graph_dataset = VideoGraphDataset(vids_training, labels_training)\n",
    "val_graph_dataset   = VideoGraphDataset(vids_val, labels_val)\n",
    "\n",
    "# DataLoaders de PyG\n",
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "train_loader = GeoDataLoader(train_graph_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = GeoDataLoader(val_graph_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3304e4f-872d-45ab-a3af-838ef986d2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=1.2613, Train Acc=0.4206 | Val Loss=1.0034, Val Acc=0.6111\n",
      "Epoch 2: Train Loss=1.0457, Train Acc=0.5402 | Val Loss=0.8558, Val Acc=0.6778\n",
      "Epoch 3: Train Loss=0.9655, Train Acc=0.6049 | Val Loss=0.8443, Val Acc=0.6556\n",
      "Epoch 4: Train Loss=0.8831, Train Acc=0.6392 | Val Loss=0.8298, Val Acc=0.6389\n",
      "Epoch 5: Train Loss=0.9067, Train Acc=0.6284 | Val Loss=1.0534, Val Acc=0.5222\n",
      "Epoch 6: Train Loss=0.8241, Train Acc=0.6598 | Val Loss=0.6928, Val Acc=0.7833\n",
      "Epoch 7: Train Loss=0.7367, Train Acc=0.7127 | Val Loss=0.6928, Val Acc=0.7722\n",
      "Epoch 8: Train Loss=0.6795, Train Acc=0.7559 | Val Loss=0.6175, Val Acc=0.8278\n",
      "Epoch 9: Train Loss=0.6455, Train Acc=0.7706 | Val Loss=0.5923, Val Acc=0.8111\n",
      "Epoch 10: Train Loss=0.6340, Train Acc=0.7520 | Val Loss=0.5786, Val Acc=0.8056\n",
      "Epoch 11: Train Loss=0.5939, Train Acc=0.7716 | Val Loss=0.5935, Val Acc=0.7778\n",
      "Epoch 12: Train Loss=0.5428, Train Acc=0.8059 | Val Loss=0.5596, Val Acc=0.8389\n",
      "Epoch 13: Train Loss=0.5632, Train Acc=0.8020 | Val Loss=0.5204, Val Acc=0.8222\n",
      "Epoch 14: Train Loss=0.5412, Train Acc=0.8098 | Val Loss=0.9069, Val Acc=0.6167\n",
      "Epoch 15: Train Loss=0.5298, Train Acc=0.8206 | Val Loss=0.5604, Val Acc=0.8111\n",
      "Epoch 16: Train Loss=0.4534, Train Acc=0.8392 | Val Loss=0.4745, Val Acc=0.8444\n",
      "Epoch 17: Train Loss=0.4503, Train Acc=0.8373 | Val Loss=0.4839, Val Acc=0.8667\n",
      "Epoch 18: Train Loss=0.4406, Train Acc=0.8441 | Val Loss=0.5131, Val Acc=0.8278\n",
      "Epoch 19: Train Loss=0.4256, Train Acc=0.8480 | Val Loss=0.5573, Val Acc=0.8000\n",
      "Epoch 20: Train Loss=0.3927, Train Acc=0.8706 | Val Loss=0.5160, Val Acc=0.8056\n",
      "Epoch 21: Train Loss=0.3824, Train Acc=0.8637 | Val Loss=0.4902, Val Acc=0.8333\n",
      "Epoch 22: Train Loss=0.3743, Train Acc=0.8725 | Val Loss=0.5062, Val Acc=0.8500\n",
      "Epoch 23: Train Loss=0.4179, Train Acc=0.8539 | Val Loss=0.5375, Val Acc=0.8000\n",
      "Epoch 24: Train Loss=0.3704, Train Acc=0.8725 | Val Loss=0.6242, Val Acc=0.7889\n",
      "Epoch 25: Train Loss=0.3448, Train Acc=0.8784 | Val Loss=0.5798, Val Acc=0.7889\n",
      "Epoch 26: Train Loss=0.3426, Train Acc=0.8814 | Val Loss=0.5319, Val Acc=0.8389\n",
      "Epoch 27: Train Loss=0.3650, Train Acc=0.8657 | Val Loss=0.6408, Val Acc=0.7833\n",
      "Epoch 28: Train Loss=0.3308, Train Acc=0.8784 | Val Loss=0.5490, Val Acc=0.8000\n",
      "Epoch 29: Train Loss=0.3282, Train Acc=0.8843 | Val Loss=0.5653, Val Acc=0.8000\n",
      "Epoch 30: Train Loss=0.3609, Train Acc=0.8608 | Val Loss=0.4661, Val Acc=0.8444\n",
      "Epoch 31: Train Loss=0.3485, Train Acc=0.8775 | Val Loss=0.5178, Val Acc=0.8389\n",
      "Epoch 32: Train Loss=0.3002, Train Acc=0.8833 | Val Loss=0.7004, Val Acc=0.7778\n",
      "Epoch 33: Train Loss=0.2971, Train Acc=0.8961 | Val Loss=0.6654, Val Acc=0.7833\n",
      "Epoch 34: Train Loss=0.3087, Train Acc=0.8824 | Val Loss=0.4215, Val Acc=0.8833\n",
      "Epoch 35: Train Loss=0.2817, Train Acc=0.8922 | Val Loss=0.5977, Val Acc=0.7944\n",
      "Epoch 36: Train Loss=0.2442, Train Acc=0.9108 | Val Loss=0.4960, Val Acc=0.8333\n",
      "Epoch 37: Train Loss=0.3052, Train Acc=0.8912 | Val Loss=0.5970, Val Acc=0.7500\n",
      "Epoch 38: Train Loss=0.2672, Train Acc=0.9049 | Val Loss=0.7241, Val Acc=0.7556\n",
      "Epoch 39: Train Loss=0.2774, Train Acc=0.8990 | Val Loss=0.5192, Val Acc=0.7889\n",
      "Epoch 40: Train Loss=0.2486, Train Acc=0.9127 | Val Loss=0.5794, Val Acc=0.8000\n",
      "Epoch 41: Train Loss=0.2717, Train Acc=0.9020 | Val Loss=0.6436, Val Acc=0.7389\n",
      "Epoch 42: Train Loss=0.2790, Train Acc=0.8971 | Val Loss=0.6493, Val Acc=0.8056\n",
      "Epoch 43: Train Loss=0.2248, Train Acc=0.9196 | Val Loss=0.6362, Val Acc=0.7111\n",
      "Epoch 44: Train Loss=0.2582, Train Acc=0.9108 | Val Loss=0.6576, Val Acc=0.7389\n",
      "Epoch 45: Train Loss=0.2170, Train Acc=0.9255 | Val Loss=0.6980, Val Acc=0.7444\n",
      "Epoch 46: Train Loss=0.2282, Train Acc=0.9235 | Val Loss=0.7152, Val Acc=0.7611\n",
      "Epoch 47: Train Loss=0.2218, Train Acc=0.9196 | Val Loss=0.6989, Val Acc=0.7833\n",
      "Epoch 48: Train Loss=0.1949, Train Acc=0.9284 | Val Loss=0.7715, Val Acc=0.7778\n",
      "Epoch 49: Train Loss=0.2201, Train Acc=0.9088 | Val Loss=0.7073, Val Acc=0.7778\n",
      "Epoch 50: Train Loss=0.2087, Train Acc=0.9245 | Val Loss=0.7210, Val Acc=0.8000\n",
      "Epoch 51: Train Loss=0.2167, Train Acc=0.9294 | Val Loss=0.6985, Val Acc=0.7944\n",
      "Epoch 52: Train Loss=0.1875, Train Acc=0.9333 | Val Loss=0.8824, Val Acc=0.7944\n",
      "Epoch 53: Train Loss=0.2017, Train Acc=0.9294 | Val Loss=0.5511, Val Acc=0.7889\n",
      "Epoch 54: Train Loss=0.2375, Train Acc=0.9137 | Val Loss=0.7435, Val Acc=0.7667\n",
      "Epoch 55: Train Loss=0.1893, Train Acc=0.9294 | Val Loss=0.7203, Val Acc=0.7611\n",
      "Epoch 56: Train Loss=0.1861, Train Acc=0.9441 | Val Loss=0.8941, Val Acc=0.7722\n",
      "Epoch 57: Train Loss=0.1673, Train Acc=0.9392 | Val Loss=0.6659, Val Acc=0.7833\n",
      "Epoch 58: Train Loss=0.1707, Train Acc=0.9363 | Val Loss=0.6912, Val Acc=0.8278\n",
      "Epoch 59: Train Loss=0.2061, Train Acc=0.9235 | Val Loss=0.7149, Val Acc=0.7833\n",
      "Epoch 60: Train Loss=0.2650, Train Acc=0.9108 | Val Loss=0.7425, Val Acc=0.7833\n",
      "Epoch 61: Train Loss=0.1788, Train Acc=0.9294 | Val Loss=0.6222, Val Acc=0.7667\n",
      "Epoch 62: Train Loss=0.1569, Train Acc=0.9480 | Val Loss=0.6643, Val Acc=0.7778\n",
      "Epoch 63: Train Loss=0.1612, Train Acc=0.9490 | Val Loss=0.6593, Val Acc=0.7833\n",
      "Epoch 64: Train Loss=0.1745, Train Acc=0.9373 | Val Loss=0.7555, Val Acc=0.7889\n",
      "Epoch 65: Train Loss=0.1657, Train Acc=0.9382 | Val Loss=0.5803, Val Acc=0.8000\n",
      "Epoch 66: Train Loss=0.1879, Train Acc=0.9304 | Val Loss=0.5861, Val Acc=0.8056\n",
      "Epoch 67: Train Loss=0.1745, Train Acc=0.9284 | Val Loss=0.7660, Val Acc=0.7278\n",
      "Epoch 68: Train Loss=0.1773, Train Acc=0.9392 | Val Loss=0.8439, Val Acc=0.7611\n",
      "Epoch 69: Train Loss=0.1520, Train Acc=0.9471 | Val Loss=0.7933, Val Acc=0.7611\n",
      "Epoch 70: Train Loss=0.1514, Train Acc=0.9461 | Val Loss=0.6389, Val Acc=0.8167\n",
      "Epoch 71: Train Loss=0.3475, Train Acc=0.8971 | Val Loss=1.0680, Val Acc=0.7333\n",
      "Epoch 72: Train Loss=0.2310, Train Acc=0.9049 | Val Loss=0.9970, Val Acc=0.7667\n",
      "Epoch 73: Train Loss=0.2371, Train Acc=0.9078 | Val Loss=0.8073, Val Acc=0.7667\n",
      "Epoch 74: Train Loss=0.1561, Train Acc=0.9412 | Val Loss=0.8389, Val Acc=0.7667\n",
      "Epoch 75: Train Loss=0.1387, Train Acc=0.9520 | Val Loss=0.8335, Val Acc=0.7611\n",
      "Epoch 76: Train Loss=0.1493, Train Acc=0.9539 | Val Loss=0.7010, Val Acc=0.8222\n",
      "Epoch 77: Train Loss=0.1405, Train Acc=0.9539 | Val Loss=0.8535, Val Acc=0.7778\n",
      "Epoch 78: Train Loss=0.1153, Train Acc=0.9627 | Val Loss=0.9274, Val Acc=0.7278\n",
      "Epoch 79: Train Loss=0.1264, Train Acc=0.9578 | Val Loss=0.6968, Val Acc=0.8222\n",
      "Epoch 80: Train Loss=0.1389, Train Acc=0.9539 | Val Loss=0.7221, Val Acc=0.7500\n",
      "Epoch 81: Train Loss=0.1261, Train Acc=0.9637 | Val Loss=0.8306, Val Acc=0.6889\n",
      "Epoch 82: Train Loss=0.1450, Train Acc=0.9480 | Val Loss=1.0644, Val Acc=0.7389\n",
      "Epoch 83: Train Loss=0.1125, Train Acc=0.9667 | Val Loss=1.0214, Val Acc=0.7556\n",
      "Epoch 84: Train Loss=0.1054, Train Acc=0.9657 | Val Loss=0.7381, Val Acc=0.7778\n",
      "Epoch 85: Train Loss=0.1284, Train Acc=0.9549 | Val Loss=1.2132, Val Acc=0.7778\n",
      "Epoch 86: Train Loss=0.1793, Train Acc=0.9382 | Val Loss=1.0699, Val Acc=0.7722\n",
      "Epoch 87: Train Loss=0.1545, Train Acc=0.9431 | Val Loss=1.1134, Val Acc=0.7778\n",
      "Epoch 88: Train Loss=0.1729, Train Acc=0.9373 | Val Loss=0.7903, Val Acc=0.7722\n",
      "Epoch 89: Train Loss=0.1344, Train Acc=0.9520 | Val Loss=0.7935, Val Acc=0.8111\n",
      "Epoch 90: Train Loss=0.1011, Train Acc=0.9725 | Val Loss=0.9438, Val Acc=0.7944\n",
      "Epoch 91: Train Loss=0.1059, Train Acc=0.9627 | Val Loss=0.8298, Val Acc=0.7389\n",
      "Epoch 92: Train Loss=0.0959, Train Acc=0.9696 | Val Loss=0.8511, Val Acc=0.7222\n",
      "Epoch 93: Train Loss=0.1860, Train Acc=0.9343 | Val Loss=0.9644, Val Acc=0.7778\n",
      "Epoch 94: Train Loss=0.2257, Train Acc=0.9275 | Val Loss=1.3083, Val Acc=0.7167\n",
      "Epoch 95: Train Loss=0.2094, Train Acc=0.9314 | Val Loss=1.0707, Val Acc=0.7833\n",
      "Epoch 96: Train Loss=0.1348, Train Acc=0.9539 | Val Loss=0.6515, Val Acc=0.8222\n",
      "Epoch 97: Train Loss=0.1165, Train Acc=0.9637 | Val Loss=0.8417, Val Acc=0.7944\n",
      "Epoch 98: Train Loss=0.1028, Train Acc=0.9676 | Val Loss=0.6567, Val Acc=0.8111\n",
      "Epoch 99: Train Loss=0.1043, Train Acc=0.9578 | Val Loss=1.2544, Val Acc=0.7833\n",
      "Epoch 100: Train Loss=0.0863, Train Acc=0.9725 | Val Loss=1.0551, Val Acc=0.7944\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GCNBaseline().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(100):  # baseline: 100 epochs arbitrarios\n",
    "    # --- TRAIN ---\n",
    "    model.train()\n",
    "    train_loss, correct, total = 0.0, 0, 0\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch.x, batch.edge_index, batch.batch)\n",
    "        loss = criterion(outputs, batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == batch.y).sum().item()\n",
    "        total += batch.y.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # --- VALIDATION ---\n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = batch.to(device)\n",
    "            outputs = model(batch.x, batch.edge_index, batch.batch)\n",
    "            loss = criterion(outputs, batch.y)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == batch.y).sum().item()\n",
    "            total += batch.y.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: \"\n",
    "          f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f} | \"\n",
    "          f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5f9efb-e22d-43ef-86f7-0a7d2055d5bc",
   "metadata": {},
   "source": [
    "# Modelo 3: Multiscale Temporal Convolution Network (MS-TCN) (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9daccd09-e6bc-46c9-a5aa-e857a4e5648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, videos, labels):\n",
    "        self.videos = videos\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.videos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.videos[idx], dtype=torch.float32)  # (16, 258)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84b52d47-f644-47ca-bc74-3c1f0ccb6fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSTCNBaseline(nn.Module):\n",
    "    def __init__(self, in_features=258, num_classes=4):\n",
    "        super().__init__()\n",
    "        self.tcn = TCN(\n",
    "            num_inputs=in_features,\n",
    "            num_channels=[128, 128],\n",
    "            kernel_size=3,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        self.head = nn.Linear(128, num_classes)  # 128 = último canal\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Entrada: (B, 16, 258)\n",
    "        x = x.transpose(1, 2)   # (B, 258, 16)\n",
    "        out = self.tcn(x)       # (B, 128, 16)\n",
    "        out = out.mean(dim=2)   # (B, 128)\n",
    "        return self.head(out)   # (B, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19cd9968-014e-40c1-a9e5-a90e5bda41c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VideoDataset(vids_training, labels_training)\n",
    "val_dataset   = VideoDataset(vids_val, labels_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88cd2f62-4248-4d57-9ec2-4f72959d7047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=1.3053, Train Acc=0.3990 | Val Loss=1.0964, Val Acc=0.5333\n",
      "Epoch 2: Train Loss=1.1073, Train Acc=0.5235 | Val Loss=0.8692, Val Acc=0.6889\n",
      "Epoch 3: Train Loss=0.9552, Train Acc=0.6216 | Val Loss=0.7433, Val Acc=0.7778\n",
      "Epoch 4: Train Loss=0.8425, Train Acc=0.6735 | Val Loss=0.7079, Val Acc=0.7500\n",
      "Epoch 5: Train Loss=0.7550, Train Acc=0.7118 | Val Loss=0.5522, Val Acc=0.8500\n",
      "Epoch 6: Train Loss=0.6992, Train Acc=0.7353 | Val Loss=0.5395, Val Acc=0.8444\n",
      "Epoch 7: Train Loss=0.6629, Train Acc=0.7382 | Val Loss=0.6286, Val Acc=0.7500\n",
      "Epoch 8: Train Loss=0.5761, Train Acc=0.7853 | Val Loss=0.4569, Val Acc=0.8833\n",
      "Epoch 9: Train Loss=0.5308, Train Acc=0.8069 | Val Loss=0.4759, Val Acc=0.8000\n",
      "Epoch 10: Train Loss=0.5197, Train Acc=0.8020 | Val Loss=0.5810, Val Acc=0.8556\n",
      "Epoch 11: Train Loss=0.4902, Train Acc=0.8176 | Val Loss=0.5778, Val Acc=0.8389\n",
      "Epoch 12: Train Loss=0.4566, Train Acc=0.8314 | Val Loss=0.4915, Val Acc=0.8333\n",
      "Epoch 13: Train Loss=0.4059, Train Acc=0.8549 | Val Loss=0.5019, Val Acc=0.8667\n",
      "Epoch 14: Train Loss=0.4022, Train Acc=0.8549 | Val Loss=0.3934, Val Acc=0.8500\n",
      "Epoch 15: Train Loss=0.3627, Train Acc=0.8706 | Val Loss=0.3870, Val Acc=0.8778\n",
      "Epoch 16: Train Loss=0.3819, Train Acc=0.8706 | Val Loss=0.3650, Val Acc=0.8833\n",
      "Epoch 17: Train Loss=0.3432, Train Acc=0.8775 | Val Loss=0.4489, Val Acc=0.8389\n",
      "Epoch 18: Train Loss=0.3013, Train Acc=0.8941 | Val Loss=0.4785, Val Acc=0.8722\n",
      "Epoch 19: Train Loss=0.3096, Train Acc=0.8941 | Val Loss=0.3821, Val Acc=0.8667\n",
      "Epoch 20: Train Loss=0.3777, Train Acc=0.8686 | Val Loss=0.5285, Val Acc=0.8333\n",
      "Epoch 21: Train Loss=0.3152, Train Acc=0.8755 | Val Loss=0.5056, Val Acc=0.8167\n",
      "Epoch 22: Train Loss=0.2756, Train Acc=0.9078 | Val Loss=0.3403, Val Acc=0.8944\n",
      "Epoch 23: Train Loss=0.2909, Train Acc=0.8863 | Val Loss=0.4259, Val Acc=0.8667\n",
      "Epoch 24: Train Loss=0.2520, Train Acc=0.9167 | Val Loss=0.3570, Val Acc=0.8944\n",
      "Epoch 25: Train Loss=0.2197, Train Acc=0.9324 | Val Loss=0.5641, Val Acc=0.8389\n",
      "Epoch 26: Train Loss=0.2278, Train Acc=0.9196 | Val Loss=0.6325, Val Acc=0.7889\n",
      "Epoch 27: Train Loss=0.2823, Train Acc=0.8980 | Val Loss=0.3972, Val Acc=0.8722\n",
      "Epoch 28: Train Loss=0.2402, Train Acc=0.9078 | Val Loss=0.4697, Val Acc=0.8778\n",
      "Epoch 29: Train Loss=0.1950, Train Acc=0.9333 | Val Loss=0.4321, Val Acc=0.8667\n",
      "Epoch 30: Train Loss=0.2017, Train Acc=0.9333 | Val Loss=0.4916, Val Acc=0.8500\n",
      "Epoch 31: Train Loss=0.1788, Train Acc=0.9382 | Val Loss=0.4511, Val Acc=0.8556\n",
      "Epoch 32: Train Loss=0.1742, Train Acc=0.9412 | Val Loss=0.5519, Val Acc=0.8389\n",
      "Epoch 33: Train Loss=0.1937, Train Acc=0.9392 | Val Loss=0.4441, Val Acc=0.8667\n",
      "Epoch 34: Train Loss=0.2082, Train Acc=0.9324 | Val Loss=0.5174, Val Acc=0.8500\n",
      "Epoch 35: Train Loss=0.1763, Train Acc=0.9412 | Val Loss=0.3684, Val Acc=0.8889\n",
      "Epoch 36: Train Loss=0.2137, Train Acc=0.9225 | Val Loss=0.4219, Val Acc=0.8778\n",
      "Epoch 37: Train Loss=0.1915, Train Acc=0.9324 | Val Loss=0.4257, Val Acc=0.8889\n",
      "Epoch 38: Train Loss=0.1590, Train Acc=0.9539 | Val Loss=0.4291, Val Acc=0.8833\n",
      "Epoch 39: Train Loss=0.1254, Train Acc=0.9637 | Val Loss=0.5010, Val Acc=0.8667\n",
      "Epoch 40: Train Loss=0.1610, Train Acc=0.9471 | Val Loss=0.4126, Val Acc=0.8444\n",
      "Epoch 41: Train Loss=0.1841, Train Acc=0.9451 | Val Loss=0.4088, Val Acc=0.8667\n",
      "Epoch 42: Train Loss=0.1391, Train Acc=0.9559 | Val Loss=0.3958, Val Acc=0.8722\n",
      "Epoch 43: Train Loss=0.1318, Train Acc=0.9559 | Val Loss=0.5053, Val Acc=0.8444\n",
      "Epoch 44: Train Loss=0.1239, Train Acc=0.9618 | Val Loss=0.5222, Val Acc=0.8667\n",
      "Epoch 45: Train Loss=0.1048, Train Acc=0.9676 | Val Loss=0.4723, Val Acc=0.8833\n",
      "Epoch 46: Train Loss=0.1191, Train Acc=0.9559 | Val Loss=0.6783, Val Acc=0.8444\n",
      "Epoch 47: Train Loss=0.1179, Train Acc=0.9578 | Val Loss=0.5358, Val Acc=0.8722\n",
      "Epoch 48: Train Loss=0.1143, Train Acc=0.9618 | Val Loss=0.6168, Val Acc=0.8000\n",
      "Epoch 49: Train Loss=0.1145, Train Acc=0.9657 | Val Loss=0.4679, Val Acc=0.8722\n",
      "Epoch 50: Train Loss=0.0954, Train Acc=0.9725 | Val Loss=0.5342, Val Acc=0.8611\n",
      "Epoch 51: Train Loss=0.0969, Train Acc=0.9755 | Val Loss=0.5389, Val Acc=0.8389\n",
      "Epoch 52: Train Loss=0.1071, Train Acc=0.9686 | Val Loss=0.5414, Val Acc=0.8722\n",
      "Epoch 53: Train Loss=0.0933, Train Acc=0.9716 | Val Loss=0.6579, Val Acc=0.8278\n",
      "Epoch 54: Train Loss=0.1627, Train Acc=0.9402 | Val Loss=0.6029, Val Acc=0.8389\n",
      "Epoch 55: Train Loss=0.1124, Train Acc=0.9676 | Val Loss=0.5478, Val Acc=0.8722\n",
      "Epoch 56: Train Loss=0.0639, Train Acc=0.9882 | Val Loss=0.5706, Val Acc=0.8389\n",
      "Epoch 57: Train Loss=0.0805, Train Acc=0.9745 | Val Loss=0.6403, Val Acc=0.8333\n",
      "Epoch 58: Train Loss=0.0865, Train Acc=0.9725 | Val Loss=0.7121, Val Acc=0.8278\n",
      "Epoch 59: Train Loss=0.1173, Train Acc=0.9578 | Val Loss=0.5443, Val Acc=0.8444\n",
      "Epoch 60: Train Loss=0.0963, Train Acc=0.9686 | Val Loss=0.6242, Val Acc=0.8556\n",
      "Epoch 61: Train Loss=0.0581, Train Acc=0.9873 | Val Loss=0.5448, Val Acc=0.8722\n",
      "Epoch 62: Train Loss=0.0517, Train Acc=0.9882 | Val Loss=0.5988, Val Acc=0.8500\n",
      "Epoch 63: Train Loss=0.0386, Train Acc=0.9912 | Val Loss=0.5888, Val Acc=0.8667\n",
      "Epoch 64: Train Loss=0.0500, Train Acc=0.9912 | Val Loss=0.7205, Val Acc=0.8278\n",
      "Epoch 65: Train Loss=0.0675, Train Acc=0.9775 | Val Loss=0.5149, Val Acc=0.8722\n",
      "Epoch 66: Train Loss=0.1443, Train Acc=0.9480 | Val Loss=0.3914, Val Acc=0.9000\n",
      "Epoch 67: Train Loss=0.0907, Train Acc=0.9696 | Val Loss=0.5043, Val Acc=0.8944\n",
      "Epoch 68: Train Loss=0.0560, Train Acc=0.9824 | Val Loss=0.6817, Val Acc=0.8500\n",
      "Epoch 69: Train Loss=0.0480, Train Acc=0.9853 | Val Loss=0.4376, Val Acc=0.9000\n",
      "Epoch 70: Train Loss=0.0564, Train Acc=0.9814 | Val Loss=0.4629, Val Acc=0.9111\n",
      "Epoch 71: Train Loss=0.0495, Train Acc=0.9873 | Val Loss=0.4307, Val Acc=0.8722\n",
      "Epoch 72: Train Loss=0.0740, Train Acc=0.9775 | Val Loss=0.5013, Val Acc=0.8667\n",
      "Epoch 73: Train Loss=0.0675, Train Acc=0.9824 | Val Loss=0.8452, Val Acc=0.8000\n",
      "Epoch 74: Train Loss=0.0713, Train Acc=0.9716 | Val Loss=0.4003, Val Acc=0.8778\n",
      "Epoch 75: Train Loss=0.0562, Train Acc=0.9833 | Val Loss=0.8845, Val Acc=0.8167\n",
      "Epoch 76: Train Loss=0.0365, Train Acc=0.9902 | Val Loss=0.5513, Val Acc=0.8778\n",
      "Epoch 77: Train Loss=0.0463, Train Acc=0.9902 | Val Loss=0.6307, Val Acc=0.8667\n",
      "Epoch 78: Train Loss=0.0402, Train Acc=0.9882 | Val Loss=0.8069, Val Acc=0.8222\n",
      "Epoch 79: Train Loss=0.0473, Train Acc=0.9843 | Val Loss=0.4642, Val Acc=0.8833\n",
      "Epoch 80: Train Loss=0.0428, Train Acc=0.9902 | Val Loss=0.5660, Val Acc=0.8611\n",
      "Epoch 81: Train Loss=0.0587, Train Acc=0.9804 | Val Loss=0.8225, Val Acc=0.8556\n",
      "Epoch 82: Train Loss=0.0468, Train Acc=0.9814 | Val Loss=0.6329, Val Acc=0.8556\n",
      "Epoch 83: Train Loss=0.0709, Train Acc=0.9686 | Val Loss=0.5168, Val Acc=0.9000\n",
      "Epoch 84: Train Loss=0.0780, Train Acc=0.9686 | Val Loss=0.5714, Val Acc=0.8889\n",
      "Epoch 85: Train Loss=0.0638, Train Acc=0.9794 | Val Loss=0.4338, Val Acc=0.8778\n",
      "Epoch 86: Train Loss=0.0605, Train Acc=0.9784 | Val Loss=0.5949, Val Acc=0.8611\n",
      "Epoch 87: Train Loss=0.0226, Train Acc=0.9951 | Val Loss=0.7140, Val Acc=0.8556\n",
      "Epoch 88: Train Loss=0.0239, Train Acc=0.9941 | Val Loss=0.5853, Val Acc=0.8722\n",
      "Epoch 89: Train Loss=0.0356, Train Acc=0.9873 | Val Loss=0.6822, Val Acc=0.8667\n",
      "Epoch 90: Train Loss=0.0340, Train Acc=0.9873 | Val Loss=0.6282, Val Acc=0.8556\n",
      "Epoch 91: Train Loss=0.0428, Train Acc=0.9814 | Val Loss=0.5465, Val Acc=0.8833\n",
      "Epoch 92: Train Loss=0.0352, Train Acc=0.9863 | Val Loss=0.8368, Val Acc=0.8444\n",
      "Epoch 93: Train Loss=0.0379, Train Acc=0.9892 | Val Loss=0.6613, Val Acc=0.8722\n",
      "Epoch 94: Train Loss=0.0403, Train Acc=0.9873 | Val Loss=0.6480, Val Acc=0.8722\n",
      "Epoch 95: Train Loss=0.0427, Train Acc=0.9873 | Val Loss=0.5870, Val Acc=0.8389\n",
      "Epoch 96: Train Loss=0.0277, Train Acc=0.9922 | Val Loss=0.9774, Val Acc=0.8611\n",
      "Epoch 97: Train Loss=0.0650, Train Acc=0.9765 | Val Loss=0.5556, Val Acc=0.8500\n",
      "Epoch 98: Train Loss=0.0729, Train Acc=0.9745 | Val Loss=0.5030, Val Acc=0.8722\n",
      "Epoch 99: Train Loss=0.0663, Train Acc=0.9775 | Val Loss=0.5631, Val Acc=0.8444\n",
      "Epoch 100: Train Loss=0.0265, Train Acc=0.9961 | Val Loss=0.4558, Val Acc=0.8889\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MSTCNBaseline().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(100):  # baseline, 100 epochs arbitrarios\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_loss, correct, total = 0.0, 0, 0\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # Val\n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: \"\n",
    "          f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f} | \"\n",
    "          f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6916126d-a090-4c25-b20c-d0a858ddd0ba",
   "metadata": {},
   "source": [
    "# Modelo 4: Temporal Gate Unit (TGU) (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c74f299f-2c76-4a2a-a85e-6b853317abdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, videos, labels):\n",
    "        self.videos = videos\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.videos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.videos[idx], dtype=torch.float32)  # (16, 258)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d97d3f98-cc01-493d-950c-7f7a3141fca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TGUBlock(nn.Module):\n",
    "    \"\"\"Bloque temporal con gating multiplicativo.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.conv_filter = nn.Conv1d(in_channels, out_channels, kernel_size, padding=1)\n",
    "        self.conv_gate   = nn.Conv1d(in_channels, out_channels, kernel_size, padding=1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, L)\n",
    "        f = torch.tanh(self.conv_filter(x))\n",
    "        g = torch.sigmoid(self.conv_gate(x))\n",
    "        return self.dropout(f * g)  # gating multiplicativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8d783ca-bb45-4c7a-b805-b21e314d28b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TGUBaseline(nn.Module):\n",
    "    def __init__(self, in_features=258, num_classes=4, hidden=128):\n",
    "        super().__init__()\n",
    "        self.block1 = TGUBlock(in_features, hidden)\n",
    "        self.block2 = TGUBlock(hidden, hidden)\n",
    "        self.head   = nn.Linear(hidden, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Entrada: (B, 16, 258)\n",
    "        x = x.transpose(1, 2)     # (B, 258, 16)\n",
    "        x = self.block1(x)        # (B, hidden, L)\n",
    "        x = self.block2(x)        # (B, hidden, L)\n",
    "        x = x.mean(dim=2)         # pooling temporal -> (B, hidden)\n",
    "        return self.head(x)       # logits (B, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "772e5ff1-9191-4431-b263-62d2d94b7569",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VideoDataset(vids_training, labels_training)\n",
    "val_dataset   = VideoDataset(vids_val, labels_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d391b2d-57f7-4ce6-8804-2df181924b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TGUBaseline().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "647f37fc-c96c-4804-bf03-0fc36b39cf2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=1.3018, Train Acc=0.3775 | Val Loss=1.0532, Val Acc=0.5167\n",
      "Epoch 2: Train Loss=1.0619, Train Acc=0.5304 | Val Loss=0.8304, Val Acc=0.6667\n",
      "Epoch 3: Train Loss=0.9948, Train Acc=0.5745 | Val Loss=0.7928, Val Acc=0.7056\n",
      "Epoch 4: Train Loss=0.9409, Train Acc=0.6010 | Val Loss=0.7384, Val Acc=0.7222\n",
      "Epoch 5: Train Loss=0.8510, Train Acc=0.6510 | Val Loss=0.7178, Val Acc=0.7444\n",
      "Epoch 6: Train Loss=0.7724, Train Acc=0.6902 | Val Loss=0.6937, Val Acc=0.7333\n",
      "Epoch 7: Train Loss=0.7185, Train Acc=0.7176 | Val Loss=0.5469, Val Acc=0.8333\n",
      "Epoch 8: Train Loss=0.6494, Train Acc=0.7696 | Val Loss=0.6280, Val Acc=0.7333\n",
      "Epoch 9: Train Loss=0.6357, Train Acc=0.7559 | Val Loss=0.6409, Val Acc=0.7222\n",
      "Epoch 10: Train Loss=0.5979, Train Acc=0.7745 | Val Loss=0.6151, Val Acc=0.7611\n",
      "Epoch 11: Train Loss=0.6382, Train Acc=0.7618 | Val Loss=0.6210, Val Acc=0.7444\n",
      "Epoch 12: Train Loss=0.5369, Train Acc=0.8108 | Val Loss=0.4738, Val Acc=0.8500\n",
      "Epoch 13: Train Loss=0.4790, Train Acc=0.8225 | Val Loss=1.1138, Val Acc=0.7111\n",
      "Epoch 14: Train Loss=0.5111, Train Acc=0.8137 | Val Loss=0.5955, Val Acc=0.7111\n",
      "Epoch 15: Train Loss=0.4324, Train Acc=0.8441 | Val Loss=0.5044, Val Acc=0.8167\n",
      "Epoch 16: Train Loss=0.4030, Train Acc=0.8569 | Val Loss=0.6109, Val Acc=0.7833\n",
      "Epoch 17: Train Loss=0.4038, Train Acc=0.8588 | Val Loss=0.7434, Val Acc=0.7500\n",
      "Epoch 18: Train Loss=0.4221, Train Acc=0.8598 | Val Loss=0.6725, Val Acc=0.7722\n",
      "Epoch 19: Train Loss=0.4316, Train Acc=0.8284 | Val Loss=0.4705, Val Acc=0.8500\n",
      "Epoch 20: Train Loss=0.3514, Train Acc=0.8755 | Val Loss=0.6299, Val Acc=0.7944\n",
      "Epoch 21: Train Loss=0.3467, Train Acc=0.8696 | Val Loss=0.5847, Val Acc=0.8111\n",
      "Epoch 22: Train Loss=0.3021, Train Acc=0.8971 | Val Loss=0.7304, Val Acc=0.7278\n",
      "Epoch 23: Train Loss=0.3253, Train Acc=0.8775 | Val Loss=0.7020, Val Acc=0.7778\n",
      "Epoch 24: Train Loss=0.3180, Train Acc=0.8912 | Val Loss=1.0148, Val Acc=0.6944\n",
      "Epoch 25: Train Loss=0.3010, Train Acc=0.8873 | Val Loss=0.8903, Val Acc=0.7778\n",
      "Epoch 26: Train Loss=0.2745, Train Acc=0.9098 | Val Loss=0.5186, Val Acc=0.8111\n",
      "Epoch 27: Train Loss=0.2712, Train Acc=0.9118 | Val Loss=0.5390, Val Acc=0.8167\n",
      "Epoch 28: Train Loss=0.3158, Train Acc=0.8882 | Val Loss=0.5316, Val Acc=0.8500\n",
      "Epoch 29: Train Loss=0.2712, Train Acc=0.9010 | Val Loss=0.6714, Val Acc=0.8278\n",
      "Epoch 30: Train Loss=0.2488, Train Acc=0.9108 | Val Loss=0.6825, Val Acc=0.8111\n",
      "Epoch 31: Train Loss=0.2310, Train Acc=0.9235 | Val Loss=0.6783, Val Acc=0.8000\n",
      "Epoch 32: Train Loss=0.2531, Train Acc=0.9069 | Val Loss=0.6127, Val Acc=0.7889\n",
      "Epoch 33: Train Loss=0.2236, Train Acc=0.9235 | Val Loss=0.6938, Val Acc=0.7889\n",
      "Epoch 34: Train Loss=0.1972, Train Acc=0.9353 | Val Loss=0.4537, Val Acc=0.8778\n",
      "Epoch 35: Train Loss=0.1980, Train Acc=0.9314 | Val Loss=0.6704, Val Acc=0.8056\n",
      "Epoch 36: Train Loss=0.1823, Train Acc=0.9441 | Val Loss=0.5425, Val Acc=0.7944\n",
      "Epoch 37: Train Loss=0.1911, Train Acc=0.9402 | Val Loss=0.6097, Val Acc=0.8167\n",
      "Epoch 38: Train Loss=0.1737, Train Acc=0.9392 | Val Loss=0.6763, Val Acc=0.8056\n",
      "Epoch 39: Train Loss=0.1527, Train Acc=0.9569 | Val Loss=0.6584, Val Acc=0.8000\n",
      "Epoch 40: Train Loss=0.1706, Train Acc=0.9402 | Val Loss=0.5857, Val Acc=0.8222\n",
      "Epoch 41: Train Loss=0.1680, Train Acc=0.9431 | Val Loss=0.7581, Val Acc=0.8111\n",
      "Epoch 42: Train Loss=0.1605, Train Acc=0.9461 | Val Loss=0.8696, Val Acc=0.8056\n",
      "Epoch 43: Train Loss=0.1640, Train Acc=0.9431 | Val Loss=0.6176, Val Acc=0.8167\n",
      "Epoch 44: Train Loss=0.1358, Train Acc=0.9559 | Val Loss=0.6563, Val Acc=0.7833\n",
      "Epoch 45: Train Loss=0.1307, Train Acc=0.9598 | Val Loss=0.5410, Val Acc=0.8333\n",
      "Epoch 46: Train Loss=0.1483, Train Acc=0.9529 | Val Loss=0.7781, Val Acc=0.8056\n",
      "Epoch 47: Train Loss=0.1256, Train Acc=0.9647 | Val Loss=0.7232, Val Acc=0.8000\n",
      "Epoch 48: Train Loss=0.1401, Train Acc=0.9588 | Val Loss=0.7445, Val Acc=0.7944\n",
      "Epoch 49: Train Loss=0.1362, Train Acc=0.9569 | Val Loss=0.7680, Val Acc=0.8111\n",
      "Epoch 50: Train Loss=0.1206, Train Acc=0.9627 | Val Loss=0.7641, Val Acc=0.8222\n",
      "Epoch 51: Train Loss=0.1002, Train Acc=0.9706 | Val Loss=0.6774, Val Acc=0.8222\n",
      "Epoch 52: Train Loss=0.1026, Train Acc=0.9706 | Val Loss=0.7169, Val Acc=0.8333\n",
      "Epoch 53: Train Loss=0.1428, Train Acc=0.9647 | Val Loss=0.6462, Val Acc=0.8222\n",
      "Epoch 54: Train Loss=0.1240, Train Acc=0.9598 | Val Loss=0.7789, Val Acc=0.8000\n",
      "Epoch 55: Train Loss=0.0915, Train Acc=0.9735 | Val Loss=0.6643, Val Acc=0.8389\n",
      "Epoch 56: Train Loss=0.0865, Train Acc=0.9706 | Val Loss=0.6397, Val Acc=0.8333\n",
      "Epoch 57: Train Loss=0.0859, Train Acc=0.9804 | Val Loss=0.6296, Val Acc=0.8333\n",
      "Epoch 58: Train Loss=0.1023, Train Acc=0.9716 | Val Loss=0.6466, Val Acc=0.8222\n",
      "Epoch 59: Train Loss=0.1093, Train Acc=0.9696 | Val Loss=0.6390, Val Acc=0.8167\n",
      "Epoch 60: Train Loss=0.0844, Train Acc=0.9775 | Val Loss=0.7503, Val Acc=0.8278\n",
      "Epoch 61: Train Loss=0.0836, Train Acc=0.9765 | Val Loss=0.7697, Val Acc=0.8222\n",
      "Epoch 62: Train Loss=0.0734, Train Acc=0.9843 | Val Loss=0.6422, Val Acc=0.8389\n",
      "Epoch 63: Train Loss=0.0574, Train Acc=0.9873 | Val Loss=0.9826, Val Acc=0.8056\n",
      "Epoch 64: Train Loss=0.0967, Train Acc=0.9696 | Val Loss=1.2555, Val Acc=0.7667\n",
      "Epoch 65: Train Loss=0.0943, Train Acc=0.9696 | Val Loss=0.6370, Val Acc=0.8444\n",
      "Epoch 66: Train Loss=0.0592, Train Acc=0.9863 | Val Loss=0.8011, Val Acc=0.8167\n",
      "Epoch 67: Train Loss=0.0537, Train Acc=0.9922 | Val Loss=0.9335, Val Acc=0.8167\n",
      "Epoch 68: Train Loss=0.0764, Train Acc=0.9794 | Val Loss=0.5037, Val Acc=0.8889\n",
      "Epoch 69: Train Loss=0.0646, Train Acc=0.9804 | Val Loss=0.8066, Val Acc=0.8000\n",
      "Epoch 70: Train Loss=0.0565, Train Acc=0.9892 | Val Loss=0.8098, Val Acc=0.8167\n",
      "Epoch 71: Train Loss=0.0546, Train Acc=0.9892 | Val Loss=0.7756, Val Acc=0.8056\n",
      "Epoch 72: Train Loss=0.0460, Train Acc=0.9902 | Val Loss=0.6302, Val Acc=0.8167\n",
      "Epoch 73: Train Loss=0.0535, Train Acc=0.9873 | Val Loss=0.9357, Val Acc=0.7944\n",
      "Epoch 74: Train Loss=0.0418, Train Acc=0.9931 | Val Loss=0.9631, Val Acc=0.8000\n",
      "Epoch 75: Train Loss=0.0377, Train Acc=0.9941 | Val Loss=0.7611, Val Acc=0.8222\n",
      "Epoch 76: Train Loss=0.0392, Train Acc=0.9922 | Val Loss=0.7661, Val Acc=0.8222\n",
      "Epoch 77: Train Loss=0.0682, Train Acc=0.9794 | Val Loss=1.1495, Val Acc=0.7444\n",
      "Epoch 78: Train Loss=0.1935, Train Acc=0.9353 | Val Loss=0.6819, Val Acc=0.8333\n",
      "Epoch 79: Train Loss=0.2602, Train Acc=0.9147 | Val Loss=0.5219, Val Acc=0.8611\n",
      "Epoch 80: Train Loss=0.1015, Train Acc=0.9706 | Val Loss=0.7100, Val Acc=0.8167\n",
      "Epoch 81: Train Loss=0.0679, Train Acc=0.9804 | Val Loss=0.7257, Val Acc=0.8500\n",
      "Epoch 82: Train Loss=0.0580, Train Acc=0.9843 | Val Loss=0.7069, Val Acc=0.8167\n",
      "Epoch 83: Train Loss=0.0399, Train Acc=0.9931 | Val Loss=0.9467, Val Acc=0.7944\n",
      "Epoch 84: Train Loss=0.0367, Train Acc=0.9941 | Val Loss=0.7071, Val Acc=0.8444\n",
      "Epoch 85: Train Loss=0.0302, Train Acc=0.9941 | Val Loss=0.8178, Val Acc=0.8167\n",
      "Epoch 86: Train Loss=0.0300, Train Acc=0.9961 | Val Loss=0.7966, Val Acc=0.8222\n",
      "Epoch 87: Train Loss=0.0262, Train Acc=0.9951 | Val Loss=0.9007, Val Acc=0.8000\n",
      "Epoch 88: Train Loss=0.0295, Train Acc=0.9971 | Val Loss=0.7074, Val Acc=0.8167\n",
      "Epoch 89: Train Loss=0.0218, Train Acc=0.9971 | Val Loss=0.9913, Val Acc=0.8111\n",
      "Epoch 90: Train Loss=0.0246, Train Acc=0.9971 | Val Loss=1.0821, Val Acc=0.7944\n",
      "Epoch 91: Train Loss=0.0235, Train Acc=0.9941 | Val Loss=1.1423, Val Acc=0.7944\n",
      "Epoch 92: Train Loss=0.0241, Train Acc=0.9961 | Val Loss=0.8123, Val Acc=0.8167\n",
      "Epoch 93: Train Loss=0.0227, Train Acc=0.9961 | Val Loss=0.9319, Val Acc=0.8222\n",
      "Epoch 94: Train Loss=0.0227, Train Acc=0.9971 | Val Loss=1.1735, Val Acc=0.8111\n",
      "Epoch 95: Train Loss=0.0378, Train Acc=0.9892 | Val Loss=1.2533, Val Acc=0.7833\n",
      "Epoch 96: Train Loss=0.0940, Train Acc=0.9706 | Val Loss=0.8822, Val Acc=0.8111\n",
      "Epoch 97: Train Loss=0.0465, Train Acc=0.9873 | Val Loss=0.9525, Val Acc=0.8222\n",
      "Epoch 98: Train Loss=0.0582, Train Acc=0.9784 | Val Loss=0.7789, Val Acc=0.8167\n",
      "Epoch 99: Train Loss=0.0345, Train Acc=0.9941 | Val Loss=0.7578, Val Acc=0.8333\n",
      "Epoch 100: Train Loss=0.0280, Train Acc=0.9941 | Val Loss=1.0400, Val Acc=0.7944\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):  # baseline: 3 epochs arbitrarios\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_loss, correct, total = 0.0, 0, 0\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # Val\n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: \"\n",
    "          f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f} | \"\n",
    "          f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626180c3-f523-42af-98d1-7bb32b557dc6",
   "metadata": {},
   "source": [
    "# Modelo 5: Self-Attention Network (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a47a99a-2c62-48dd-81cc-a732af3e2474",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, videos, labels):\n",
    "        self.videos = videos\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.videos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.videos[idx], dtype=torch.float32)  # (16, 258)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac1e4bac-038d-443b-9eb0-697132a444a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionBlock(nn.Module):\n",
    "    def __init__(self, embed_dim=258, num_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, L, E)\n",
    "        attn_out, _ = self.attn(x, x, x)     # self-attention\n",
    "        x = self.norm1(x + attn_out)         # residual\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + ff_out)           # residual\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b855b22-7ba1-42c7-a573-62326bb05efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SANBaseline(nn.Module):\n",
    "    def __init__(self, in_features=258, num_classes=4, num_heads=4, num_layers=2, hidden=128):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(in_features, hidden)\n",
    "        self.layers = nn.ModuleList([\n",
    "            SelfAttentionBlock(embed_dim=hidden, num_heads=num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.head = nn.Linear(hidden, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 16, 258)\n",
    "        x = self.input_proj(x)   # (B, L, hidden)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)         # (B, L, hidden)\n",
    "        x = x.mean(dim=1)        # pooling temporal\n",
    "        return self.head(x)      # (B, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2241c907-0a72-43f5-a459-5f6eb15e12c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VideoDataset(vids_training, labels_training)\n",
    "val_dataset   = VideoDataset(vids_val, labels_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fa5f3cc0-39d5-4311-880d-f26bf84e2ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=1.2901, Train Acc=0.3902 | Val Loss=0.8844, Val Acc=0.6389\n",
      "Epoch 2: Train Loss=1.0178, Train Acc=0.5745 | Val Loss=0.7282, Val Acc=0.7167\n",
      "Epoch 3: Train Loss=0.8707, Train Acc=0.6461 | Val Loss=0.5143, Val Acc=0.8333\n",
      "Epoch 4: Train Loss=0.7400, Train Acc=0.7020 | Val Loss=0.4833, Val Acc=0.8500\n",
      "Epoch 5: Train Loss=0.6838, Train Acc=0.7480 | Val Loss=0.5095, Val Acc=0.8389\n",
      "Epoch 6: Train Loss=0.6234, Train Acc=0.7755 | Val Loss=0.4253, Val Acc=0.8222\n",
      "Epoch 7: Train Loss=0.5020, Train Acc=0.8176 | Val Loss=0.4259, Val Acc=0.8833\n",
      "Epoch 8: Train Loss=0.4132, Train Acc=0.8588 | Val Loss=0.5220, Val Acc=0.8500\n",
      "Epoch 9: Train Loss=0.3921, Train Acc=0.8618 | Val Loss=0.2999, Val Acc=0.9278\n",
      "Epoch 10: Train Loss=0.3566, Train Acc=0.8706 | Val Loss=0.4704, Val Acc=0.8667\n",
      "Epoch 11: Train Loss=0.3156, Train Acc=0.8833 | Val Loss=0.2862, Val Acc=0.9333\n",
      "Epoch 12: Train Loss=0.2811, Train Acc=0.9029 | Val Loss=0.2902, Val Acc=0.9111\n",
      "Epoch 13: Train Loss=0.3234, Train Acc=0.8912 | Val Loss=0.4158, Val Acc=0.8667\n",
      "Epoch 14: Train Loss=0.2803, Train Acc=0.8912 | Val Loss=0.4907, Val Acc=0.8389\n",
      "Epoch 15: Train Loss=0.2734, Train Acc=0.8961 | Val Loss=0.5739, Val Acc=0.8833\n",
      "Epoch 16: Train Loss=0.2558, Train Acc=0.9059 | Val Loss=0.4427, Val Acc=0.8778\n",
      "Epoch 17: Train Loss=0.3045, Train Acc=0.8902 | Val Loss=0.3520, Val Acc=0.8889\n",
      "Epoch 18: Train Loss=0.2129, Train Acc=0.9216 | Val Loss=0.4700, Val Acc=0.8778\n",
      "Epoch 19: Train Loss=0.2062, Train Acc=0.9275 | Val Loss=0.3505, Val Acc=0.9333\n",
      "Epoch 20: Train Loss=0.2311, Train Acc=0.9167 | Val Loss=0.4653, Val Acc=0.8500\n",
      "Epoch 21: Train Loss=0.2634, Train Acc=0.9147 | Val Loss=0.3167, Val Acc=0.9222\n",
      "Epoch 22: Train Loss=0.1852, Train Acc=0.9373 | Val Loss=0.3248, Val Acc=0.9000\n",
      "Epoch 23: Train Loss=0.1530, Train Acc=0.9529 | Val Loss=0.3565, Val Acc=0.9111\n",
      "Epoch 24: Train Loss=0.1327, Train Acc=0.9529 | Val Loss=0.6540, Val Acc=0.8111\n",
      "Epoch 25: Train Loss=0.1625, Train Acc=0.9441 | Val Loss=0.3465, Val Acc=0.9222\n",
      "Epoch 26: Train Loss=0.2011, Train Acc=0.9265 | Val Loss=0.5544, Val Acc=0.8778\n",
      "Epoch 27: Train Loss=0.1535, Train Acc=0.9373 | Val Loss=0.3888, Val Acc=0.8833\n",
      "Epoch 28: Train Loss=0.1260, Train Acc=0.9578 | Val Loss=0.4576, Val Acc=0.8889\n",
      "Epoch 29: Train Loss=0.1190, Train Acc=0.9608 | Val Loss=0.3899, Val Acc=0.9278\n",
      "Epoch 30: Train Loss=0.1322, Train Acc=0.9569 | Val Loss=0.3608, Val Acc=0.9389\n",
      "Epoch 31: Train Loss=0.1367, Train Acc=0.9539 | Val Loss=0.4603, Val Acc=0.8944\n",
      "Epoch 32: Train Loss=0.0709, Train Acc=0.9765 | Val Loss=0.6054, Val Acc=0.8556\n",
      "Epoch 33: Train Loss=0.1145, Train Acc=0.9578 | Val Loss=0.5237, Val Acc=0.8722\n",
      "Epoch 34: Train Loss=0.1441, Train Acc=0.9529 | Val Loss=0.3847, Val Acc=0.9000\n",
      "Epoch 35: Train Loss=0.1405, Train Acc=0.9510 | Val Loss=0.3313, Val Acc=0.9278\n",
      "Epoch 36: Train Loss=0.0831, Train Acc=0.9735 | Val Loss=0.4278, Val Acc=0.9167\n",
      "Epoch 37: Train Loss=0.0787, Train Acc=0.9765 | Val Loss=0.4705, Val Acc=0.9222\n",
      "Epoch 38: Train Loss=0.0546, Train Acc=0.9863 | Val Loss=0.5208, Val Acc=0.8944\n",
      "Epoch 39: Train Loss=0.0384, Train Acc=0.9892 | Val Loss=0.4697, Val Acc=0.8889\n",
      "Epoch 40: Train Loss=0.0565, Train Acc=0.9853 | Val Loss=0.5817, Val Acc=0.8889\n",
      "Epoch 41: Train Loss=0.0943, Train Acc=0.9667 | Val Loss=0.5587, Val Acc=0.8833\n",
      "Epoch 42: Train Loss=0.1318, Train Acc=0.9520 | Val Loss=0.4774, Val Acc=0.8667\n",
      "Epoch 43: Train Loss=0.1193, Train Acc=0.9539 | Val Loss=0.4728, Val Acc=0.8889\n",
      "Epoch 44: Train Loss=0.1009, Train Acc=0.9676 | Val Loss=0.3383, Val Acc=0.9167\n",
      "Epoch 45: Train Loss=0.0583, Train Acc=0.9824 | Val Loss=0.4847, Val Acc=0.9111\n",
      "Epoch 46: Train Loss=0.0587, Train Acc=0.9765 | Val Loss=0.6954, Val Acc=0.8611\n",
      "Epoch 47: Train Loss=0.0765, Train Acc=0.9706 | Val Loss=0.6778, Val Acc=0.8889\n",
      "Epoch 48: Train Loss=0.0898, Train Acc=0.9686 | Val Loss=0.5162, Val Acc=0.8667\n",
      "Epoch 49: Train Loss=0.0675, Train Acc=0.9755 | Val Loss=0.4522, Val Acc=0.8833\n",
      "Epoch 50: Train Loss=0.0677, Train Acc=0.9784 | Val Loss=0.6230, Val Acc=0.8667\n",
      "Epoch 51: Train Loss=0.0385, Train Acc=0.9892 | Val Loss=0.6480, Val Acc=0.8944\n",
      "Epoch 52: Train Loss=0.0466, Train Acc=0.9853 | Val Loss=0.5995, Val Acc=0.8833\n",
      "Epoch 53: Train Loss=0.0669, Train Acc=0.9735 | Val Loss=1.2354, Val Acc=0.7556\n",
      "Epoch 54: Train Loss=0.1335, Train Acc=0.9500 | Val Loss=0.4657, Val Acc=0.9167\n",
      "Epoch 55: Train Loss=0.1153, Train Acc=0.9549 | Val Loss=0.6387, Val Acc=0.8556\n",
      "Epoch 56: Train Loss=0.0966, Train Acc=0.9657 | Val Loss=0.4204, Val Acc=0.9056\n",
      "Epoch 57: Train Loss=0.0657, Train Acc=0.9794 | Val Loss=0.5656, Val Acc=0.9000\n",
      "Epoch 58: Train Loss=0.0529, Train Acc=0.9833 | Val Loss=0.5592, Val Acc=0.8944\n",
      "Epoch 59: Train Loss=0.0184, Train Acc=0.9971 | Val Loss=0.4653, Val Acc=0.9222\n",
      "Epoch 60: Train Loss=0.0133, Train Acc=0.9961 | Val Loss=0.5562, Val Acc=0.9056\n",
      "Epoch 61: Train Loss=0.0181, Train Acc=0.9951 | Val Loss=0.6426, Val Acc=0.8833\n",
      "Epoch 62: Train Loss=0.0146, Train Acc=0.9961 | Val Loss=0.5938, Val Acc=0.9167\n",
      "Epoch 63: Train Loss=0.0218, Train Acc=0.9922 | Val Loss=0.5706, Val Acc=0.9167\n",
      "Epoch 64: Train Loss=0.0339, Train Acc=0.9873 | Val Loss=0.5421, Val Acc=0.9000\n",
      "Epoch 65: Train Loss=0.0268, Train Acc=0.9941 | Val Loss=0.5979, Val Acc=0.8778\n",
      "Epoch 66: Train Loss=0.0385, Train Acc=0.9882 | Val Loss=0.5662, Val Acc=0.9167\n",
      "Epoch 67: Train Loss=0.0286, Train Acc=0.9902 | Val Loss=0.4494, Val Acc=0.9278\n",
      "Epoch 68: Train Loss=0.0472, Train Acc=0.9843 | Val Loss=0.6793, Val Acc=0.8944\n",
      "Epoch 69: Train Loss=0.1611, Train Acc=0.9480 | Val Loss=0.5478, Val Acc=0.8611\n",
      "Epoch 70: Train Loss=0.1483, Train Acc=0.9441 | Val Loss=0.7826, Val Acc=0.8500\n",
      "Epoch 71: Train Loss=0.1423, Train Acc=0.9431 | Val Loss=0.4432, Val Acc=0.9000\n",
      "Epoch 72: Train Loss=0.0960, Train Acc=0.9696 | Val Loss=0.5413, Val Acc=0.9000\n",
      "Epoch 73: Train Loss=0.1636, Train Acc=0.9392 | Val Loss=0.6699, Val Acc=0.8722\n",
      "Epoch 74: Train Loss=0.1084, Train Acc=0.9627 | Val Loss=0.2808, Val Acc=0.9278\n",
      "Epoch 75: Train Loss=0.0460, Train Acc=0.9882 | Val Loss=0.3547, Val Acc=0.9389\n",
      "Epoch 76: Train Loss=0.0343, Train Acc=0.9882 | Val Loss=0.4127, Val Acc=0.9222\n",
      "Epoch 77: Train Loss=0.0223, Train Acc=0.9941 | Val Loss=0.4543, Val Acc=0.8944\n",
      "Epoch 78: Train Loss=0.0080, Train Acc=1.0000 | Val Loss=0.5331, Val Acc=0.9167\n",
      "Epoch 79: Train Loss=0.0048, Train Acc=1.0000 | Val Loss=0.4247, Val Acc=0.9278\n",
      "Epoch 80: Train Loss=0.0031, Train Acc=1.0000 | Val Loss=0.4754, Val Acc=0.9111\n",
      "Epoch 81: Train Loss=0.0023, Train Acc=1.0000 | Val Loss=0.4570, Val Acc=0.9222\n",
      "Epoch 82: Train Loss=0.0020, Train Acc=1.0000 | Val Loss=0.4611, Val Acc=0.9167\n",
      "Epoch 83: Train Loss=0.0017, Train Acc=1.0000 | Val Loss=0.4227, Val Acc=0.9111\n",
      "Epoch 84: Train Loss=0.0016, Train Acc=1.0000 | Val Loss=0.4795, Val Acc=0.9111\n",
      "Epoch 85: Train Loss=0.0014, Train Acc=1.0000 | Val Loss=0.4886, Val Acc=0.9111\n",
      "Epoch 86: Train Loss=0.0012, Train Acc=1.0000 | Val Loss=0.5266, Val Acc=0.9111\n",
      "Epoch 87: Train Loss=0.0013, Train Acc=1.0000 | Val Loss=0.4651, Val Acc=0.9167\n",
      "Epoch 88: Train Loss=0.0013, Train Acc=1.0000 | Val Loss=0.4914, Val Acc=0.9167\n",
      "Epoch 89: Train Loss=0.0012, Train Acc=1.0000 | Val Loss=0.5139, Val Acc=0.9111\n",
      "Epoch 90: Train Loss=0.0010, Train Acc=1.0000 | Val Loss=0.4961, Val Acc=0.9111\n",
      "Epoch 91: Train Loss=0.0008, Train Acc=1.0000 | Val Loss=0.4929, Val Acc=0.9111\n",
      "Epoch 92: Train Loss=0.0008, Train Acc=1.0000 | Val Loss=0.5783, Val Acc=0.9167\n",
      "Epoch 93: Train Loss=0.0008, Train Acc=1.0000 | Val Loss=0.5100, Val Acc=0.9111\n",
      "Epoch 94: Train Loss=0.0006, Train Acc=1.0000 | Val Loss=0.5176, Val Acc=0.9111\n",
      "Epoch 95: Train Loss=0.0006, Train Acc=1.0000 | Val Loss=0.5389, Val Acc=0.9167\n",
      "Epoch 96: Train Loss=0.0005, Train Acc=1.0000 | Val Loss=0.5307, Val Acc=0.9167\n",
      "Epoch 97: Train Loss=0.0006, Train Acc=1.0000 | Val Loss=0.5565, Val Acc=0.9111\n",
      "Epoch 98: Train Loss=0.0073, Train Acc=0.9971 | Val Loss=0.7881, Val Acc=0.8944\n",
      "Epoch 99: Train Loss=0.1009, Train Acc=0.9667 | Val Loss=1.0757, Val Acc=0.7722\n",
      "Epoch 100: Train Loss=0.2642, Train Acc=0.9127 | Val Loss=0.5151, Val Acc=0.8889\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SANBaseline().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(100):  # baseline: 100 epochs arbitrarios\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_loss, correct, total = 0.0, 0, 0\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # Val\n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: \"\n",
    "          f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f} | \"\n",
    "          f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f287f4fd-30f0-49ca-9502-a7ad2747830f",
   "metadata": {},
   "source": [
    "# Modelo 6: Long-Short Term Memory Neural Network (LSTM) (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b34db65e-763e-490c-8f99-e8b115dd54ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, videos, labels):\n",
    "        self.videos = videos\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.videos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.videos[idx], dtype=torch.float32)  # (16, 258)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d64b9809-7cf0-4638-b191-d1fa58f075b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMBaseline(nn.Module):\n",
    "    def __init__(self, in_features=258, hidden1=128, hidden2=128, hidden3=64, num_classes=4):\n",
    "        super().__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=in_features, hidden_size=hidden1,\n",
    "                             batch_first=True, dropout=0.0, bidirectional=False)\n",
    "        self.lstm2 = nn.LSTM(input_size=hidden1, hidden_size=hidden2,\n",
    "                             batch_first=True, dropout=0.0, bidirectional=False)\n",
    "        self.lstm3 = nn.LSTM(input_size=hidden2, hidden_size=hidden3,\n",
    "                             batch_first=True, dropout=0.0, bidirectional=False)\n",
    "        self.fc1 = nn.Linear(hidden3, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Entrada: (B, 16, 258)\n",
    "        out, _ = self.lstm1(x)   # (B, 16, 128)\n",
    "        out, _ = self.lstm2(out) # (B, 16, 128)\n",
    "        out, _ = self.lstm3(out) # (B, 16, 64)\n",
    "        out = out[:, -1, :]      # tomamos el último estado (B, 64)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = torch.relu(self.fc2(out))\n",
    "        return self.fc3(out)     # (B, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a1c587c6-cfcb-442c-b189-96a2ee47218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VideoDataset(vids_training, labels_training)\n",
    "val_dataset   = VideoDataset(vids_val, labels_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78c7b823-b244-45d6-83b8-ba71cdd2636a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMBaseline().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "43bd2e31-7def-4431-ae43-d02f556ae493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=1.3518, Train Acc=0.2794 | Val Loss=1.1952, Val Acc=0.4500\n",
      "Epoch 2: Train Loss=1.1941, Train Acc=0.4765 | Val Loss=1.0593, Val Acc=0.5167\n",
      "Epoch 3: Train Loss=1.0532, Train Acc=0.5412 | Val Loss=0.7380, Val Acc=0.6889\n",
      "Epoch 4: Train Loss=0.9130, Train Acc=0.6020 | Val Loss=0.7635, Val Acc=0.7111\n",
      "Epoch 5: Train Loss=0.8008, Train Acc=0.6618 | Val Loss=0.6037, Val Acc=0.8000\n",
      "Epoch 6: Train Loss=0.7555, Train Acc=0.7049 | Val Loss=0.6000, Val Acc=0.7667\n",
      "Epoch 7: Train Loss=0.6988, Train Acc=0.7206 | Val Loss=0.5488, Val Acc=0.8389\n",
      "Epoch 8: Train Loss=0.5897, Train Acc=0.7804 | Val Loss=0.6156, Val Acc=0.7833\n",
      "Epoch 9: Train Loss=0.7007, Train Acc=0.7245 | Val Loss=0.5133, Val Acc=0.8000\n",
      "Epoch 10: Train Loss=0.5512, Train Acc=0.7990 | Val Loss=0.5718, Val Acc=0.8167\n",
      "Epoch 11: Train Loss=0.4921, Train Acc=0.8216 | Val Loss=0.6406, Val Acc=0.7389\n",
      "Epoch 12: Train Loss=0.4482, Train Acc=0.8363 | Val Loss=0.5308, Val Acc=0.8167\n",
      "Epoch 13: Train Loss=0.4703, Train Acc=0.8412 | Val Loss=0.7216, Val Acc=0.7889\n",
      "Epoch 14: Train Loss=0.4058, Train Acc=0.8529 | Val Loss=0.7095, Val Acc=0.7222\n",
      "Epoch 15: Train Loss=0.4559, Train Acc=0.8363 | Val Loss=0.6291, Val Acc=0.7389\n",
      "Epoch 16: Train Loss=0.4163, Train Acc=0.8529 | Val Loss=0.7868, Val Acc=0.7333\n",
      "Epoch 17: Train Loss=0.3672, Train Acc=0.8647 | Val Loss=0.4963, Val Acc=0.8222\n",
      "Epoch 18: Train Loss=0.3703, Train Acc=0.8637 | Val Loss=0.6962, Val Acc=0.7389\n",
      "Epoch 19: Train Loss=0.3869, Train Acc=0.8578 | Val Loss=0.7781, Val Acc=0.7500\n",
      "Epoch 20: Train Loss=0.2659, Train Acc=0.9167 | Val Loss=0.8438, Val Acc=0.7778\n",
      "Epoch 21: Train Loss=0.3325, Train Acc=0.8804 | Val Loss=0.6901, Val Acc=0.7778\n",
      "Epoch 22: Train Loss=0.3325, Train Acc=0.8892 | Val Loss=0.9409, Val Acc=0.7611\n",
      "Epoch 23: Train Loss=0.3067, Train Acc=0.8922 | Val Loss=0.8851, Val Acc=0.6778\n",
      "Epoch 24: Train Loss=0.2882, Train Acc=0.8931 | Val Loss=0.5140, Val Acc=0.8444\n",
      "Epoch 25: Train Loss=0.2651, Train Acc=0.8990 | Val Loss=0.6915, Val Acc=0.7667\n",
      "Epoch 26: Train Loss=0.2321, Train Acc=0.9127 | Val Loss=0.6412, Val Acc=0.7833\n",
      "Epoch 27: Train Loss=0.1734, Train Acc=0.9422 | Val Loss=0.8997, Val Acc=0.7500\n",
      "Epoch 28: Train Loss=0.1346, Train Acc=0.9510 | Val Loss=1.0803, Val Acc=0.7333\n",
      "Epoch 29: Train Loss=0.1461, Train Acc=0.9441 | Val Loss=1.0776, Val Acc=0.7611\n",
      "Epoch 30: Train Loss=0.1701, Train Acc=0.9412 | Val Loss=1.1396, Val Acc=0.6889\n",
      "Epoch 31: Train Loss=0.1865, Train Acc=0.9461 | Val Loss=0.8915, Val Acc=0.7611\n",
      "Epoch 32: Train Loss=0.1400, Train Acc=0.9549 | Val Loss=0.7961, Val Acc=0.8056\n",
      "Epoch 33: Train Loss=0.1502, Train Acc=0.9500 | Val Loss=0.6879, Val Acc=0.8611\n",
      "Epoch 34: Train Loss=0.2235, Train Acc=0.9265 | Val Loss=1.0427, Val Acc=0.7389\n",
      "Epoch 35: Train Loss=0.1462, Train Acc=0.9520 | Val Loss=0.6750, Val Acc=0.8000\n",
      "Epoch 36: Train Loss=0.0768, Train Acc=0.9775 | Val Loss=0.7634, Val Acc=0.8333\n",
      "Epoch 37: Train Loss=0.1221, Train Acc=0.9647 | Val Loss=0.7514, Val Acc=0.7778\n",
      "Epoch 38: Train Loss=0.1670, Train Acc=0.9441 | Val Loss=0.9785, Val Acc=0.7611\n",
      "Epoch 39: Train Loss=0.1408, Train Acc=0.9490 | Val Loss=0.9521, Val Acc=0.7556\n",
      "Epoch 40: Train Loss=0.0963, Train Acc=0.9676 | Val Loss=0.9325, Val Acc=0.7833\n",
      "Epoch 41: Train Loss=0.0684, Train Acc=0.9784 | Val Loss=1.1429, Val Acc=0.8056\n",
      "Epoch 42: Train Loss=0.0437, Train Acc=0.9892 | Val Loss=1.0183, Val Acc=0.7833\n",
      "Epoch 43: Train Loss=0.0517, Train Acc=0.9843 | Val Loss=1.0977, Val Acc=0.7944\n",
      "Epoch 44: Train Loss=0.0911, Train Acc=0.9735 | Val Loss=0.6976, Val Acc=0.8333\n",
      "Epoch 45: Train Loss=0.1454, Train Acc=0.9559 | Val Loss=1.0948, Val Acc=0.7722\n",
      "Epoch 46: Train Loss=0.0632, Train Acc=0.9824 | Val Loss=0.9853, Val Acc=0.7889\n",
      "Epoch 47: Train Loss=0.0757, Train Acc=0.9794 | Val Loss=0.9830, Val Acc=0.8222\n",
      "Epoch 48: Train Loss=0.0519, Train Acc=0.9824 | Val Loss=1.1550, Val Acc=0.7611\n",
      "Epoch 49: Train Loss=0.1323, Train Acc=0.9637 | Val Loss=0.7521, Val Acc=0.8056\n",
      "Epoch 50: Train Loss=0.2025, Train Acc=0.9343 | Val Loss=0.8607, Val Acc=0.7667\n",
      "Epoch 51: Train Loss=0.0986, Train Acc=0.9667 | Val Loss=0.9548, Val Acc=0.7833\n",
      "Epoch 52: Train Loss=0.0456, Train Acc=0.9882 | Val Loss=0.6409, Val Acc=0.8667\n",
      "Epoch 53: Train Loss=0.0502, Train Acc=0.9814 | Val Loss=0.6562, Val Acc=0.8722\n",
      "Epoch 54: Train Loss=0.0337, Train Acc=0.9922 | Val Loss=0.7280, Val Acc=0.8556\n",
      "Epoch 55: Train Loss=0.0218, Train Acc=0.9941 | Val Loss=0.8775, Val Acc=0.8056\n",
      "Epoch 56: Train Loss=0.0530, Train Acc=0.9882 | Val Loss=0.8053, Val Acc=0.8500\n",
      "Epoch 57: Train Loss=0.1081, Train Acc=0.9627 | Val Loss=0.9601, Val Acc=0.7778\n",
      "Epoch 58: Train Loss=0.0647, Train Acc=0.9804 | Val Loss=1.2729, Val Acc=0.7722\n",
      "Epoch 59: Train Loss=0.0779, Train Acc=0.9686 | Val Loss=0.9097, Val Acc=0.8222\n",
      "Epoch 60: Train Loss=0.1089, Train Acc=0.9618 | Val Loss=0.9922, Val Acc=0.7778\n",
      "Epoch 61: Train Loss=0.1431, Train Acc=0.9529 | Val Loss=0.9445, Val Acc=0.7944\n",
      "Epoch 62: Train Loss=0.0592, Train Acc=0.9843 | Val Loss=1.1299, Val Acc=0.7722\n",
      "Epoch 63: Train Loss=0.1169, Train Acc=0.9657 | Val Loss=0.9136, Val Acc=0.7667\n",
      "Epoch 64: Train Loss=0.0765, Train Acc=0.9755 | Val Loss=0.7379, Val Acc=0.8389\n",
      "Epoch 65: Train Loss=0.0314, Train Acc=0.9922 | Val Loss=0.7565, Val Acc=0.8222\n",
      "Epoch 66: Train Loss=0.0155, Train Acc=0.9941 | Val Loss=0.8014, Val Acc=0.8333\n",
      "Epoch 67: Train Loss=0.0117, Train Acc=0.9951 | Val Loss=0.9074, Val Acc=0.8111\n",
      "Epoch 68: Train Loss=0.0087, Train Acc=0.9971 | Val Loss=0.9960, Val Acc=0.8222\n",
      "Epoch 69: Train Loss=0.0067, Train Acc=0.9980 | Val Loss=1.0057, Val Acc=0.8222\n",
      "Epoch 70: Train Loss=0.0050, Train Acc=0.9980 | Val Loss=1.0388, Val Acc=0.8222\n",
      "Epoch 71: Train Loss=0.0041, Train Acc=0.9980 | Val Loss=1.0731, Val Acc=0.8222\n",
      "Epoch 72: Train Loss=0.0033, Train Acc=0.9990 | Val Loss=1.1130, Val Acc=0.8167\n",
      "Epoch 73: Train Loss=0.0027, Train Acc=0.9990 | Val Loss=1.1533, Val Acc=0.8167\n",
      "Epoch 74: Train Loss=0.0023, Train Acc=1.0000 | Val Loss=1.2000, Val Acc=0.8111\n",
      "Epoch 75: Train Loss=0.0020, Train Acc=1.0000 | Val Loss=1.2295, Val Acc=0.8111\n",
      "Epoch 76: Train Loss=0.0018, Train Acc=1.0000 | Val Loss=1.2647, Val Acc=0.8111\n",
      "Epoch 77: Train Loss=0.0017, Train Acc=1.0000 | Val Loss=1.2897, Val Acc=0.8111\n",
      "Epoch 78: Train Loss=0.0015, Train Acc=1.0000 | Val Loss=1.3069, Val Acc=0.8111\n",
      "Epoch 79: Train Loss=0.0014, Train Acc=1.0000 | Val Loss=1.3331, Val Acc=0.8167\n",
      "Epoch 80: Train Loss=0.0013, Train Acc=1.0000 | Val Loss=1.3470, Val Acc=0.8111\n",
      "Epoch 81: Train Loss=0.0012, Train Acc=1.0000 | Val Loss=1.3672, Val Acc=0.8167\n",
      "Epoch 82: Train Loss=0.0011, Train Acc=1.0000 | Val Loss=1.3824, Val Acc=0.8167\n",
      "Epoch 83: Train Loss=0.0011, Train Acc=1.0000 | Val Loss=1.4022, Val Acc=0.8222\n",
      "Epoch 84: Train Loss=0.0010, Train Acc=1.0000 | Val Loss=1.4131, Val Acc=0.8222\n",
      "Epoch 85: Train Loss=0.0009, Train Acc=1.0000 | Val Loss=1.4269, Val Acc=0.8222\n",
      "Epoch 86: Train Loss=0.0009, Train Acc=1.0000 | Val Loss=1.4401, Val Acc=0.8222\n",
      "Epoch 87: Train Loss=0.0008, Train Acc=1.0000 | Val Loss=1.4502, Val Acc=0.8222\n",
      "Epoch 88: Train Loss=0.0007, Train Acc=1.0000 | Val Loss=1.4595, Val Acc=0.8222\n",
      "Epoch 89: Train Loss=0.0007, Train Acc=1.0000 | Val Loss=1.4696, Val Acc=0.8222\n",
      "Epoch 90: Train Loss=0.0006, Train Acc=1.0000 | Val Loss=1.4842, Val Acc=0.8222\n",
      "Epoch 91: Train Loss=0.0006, Train Acc=1.0000 | Val Loss=1.4948, Val Acc=0.8222\n",
      "Epoch 92: Train Loss=0.0005, Train Acc=1.0000 | Val Loss=1.5006, Val Acc=0.8222\n",
      "Epoch 93: Train Loss=0.0005, Train Acc=1.0000 | Val Loss=1.5098, Val Acc=0.8222\n",
      "Epoch 94: Train Loss=0.0004, Train Acc=1.0000 | Val Loss=1.5231, Val Acc=0.8222\n",
      "Epoch 95: Train Loss=0.0004, Train Acc=1.0000 | Val Loss=1.5296, Val Acc=0.8222\n",
      "Epoch 96: Train Loss=0.0004, Train Acc=1.0000 | Val Loss=1.5381, Val Acc=0.8222\n",
      "Epoch 97: Train Loss=0.0003, Train Acc=1.0000 | Val Loss=1.5461, Val Acc=0.8222\n",
      "Epoch 98: Train Loss=0.0003, Train Acc=1.0000 | Val Loss=1.5491, Val Acc=0.8222\n",
      "Epoch 99: Train Loss=0.0003, Train Acc=1.0000 | Val Loss=1.5588, Val Acc=0.8222\n",
      "Epoch 100: Train Loss=0.0003, Train Acc=1.0000 | Val Loss=1.5671, Val Acc=0.8222\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):  # baseline, 100 epochs arbitrarios\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_loss, correct, total = 0.0, 0, 0\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # Val\n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: \"\n",
    "          f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f} | \"\n",
    "          f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f14075c-0ee4-41a4-830f-70ff2bd00ece",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Notas 3 de octubre\n",
    "* Hacer el cálculo del costo computacional en cada uno de los modelos.\n",
    "* Se puede hacer un análisis de accuracy versus costo computacional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0e10bf-1758-4318-b950-368677924952",
   "metadata": {},
   "source": [
    "# Optimización de Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ed37eca3-6733-4214-b712-83c77008cf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import json\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, balanced_accuracy_score\n",
    ")\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "adbf2a87-2bd4-4f86-9006-95f776fb0897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Almacenamiento de resultados ---\n",
    "all_results = []\n",
    "best_by_model = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "58a47cae-5eec-4a1d-a22c-10ae1c47891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para secuenciales\n",
    "train_loader_seq = DataLoader(VideoDataset(vids_training, labels_training), batch_size=32, shuffle=True)\n",
    "val_loader_seq   = DataLoader(VideoDataset(vids_val, labels_val), batch_size=32, shuffle=False)\n",
    "\n",
    "# Para GCN\n",
    "train_loader_graph = GeoDataLoader(VideoGraphDataset(vids_training, labels_training), batch_size=32, shuffle=True)\n",
    "val_loader_graph   = GeoDataLoader(VideoGraphDataset(vids_val, labels_val), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a4df2e87-0135-4f7d-83cc-b36a42992489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred, labels):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    tn = np.diag(cm)  # diagonal principal son los verdaderos positivos por clase\n",
    "    FP = cm.sum(axis=0) - tn\n",
    "    FN = cm.sum(axis=1) - tn\n",
    "    TP = tn\n",
    "    TN = cm.sum() - (FP + FN + TP)\n",
    "    \n",
    "    # Evitar divisiones por cero\n",
    "    eps = 1e-8\n",
    "    specificity = np.mean(TN / (TN + FP + eps))\n",
    "    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision_macro\": precision,\n",
    "        \"recall_macro\": recall,\n",
    "        \"specificity_macro\": specificity,\n",
    "        \"f1_macro\": f1,\n",
    "        \"balanced_accuracy\": bal_acc,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "996722a1-6d3d-4329-8328-e31237cc3eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, lr, train_loader, val_loader, device, is_graph=False):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    epochs = 100\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            if is_graph:\n",
    "                batch = batch.to(device)\n",
    "                outputs = model(batch.x, batch.edge_index, batch.batch)\n",
    "                loss = criterion(outputs, batch.y)\n",
    "            else:\n",
    "                X, y = batch\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                outputs = model(X)\n",
    "                loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # --- Validación ---\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            if is_graph:\n",
    "                batch = batch.to(device)\n",
    "                outputs = model(batch.x, batch.edge_index, batch.batch)\n",
    "                preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "                y_true.extend(batch.y.cpu().numpy())\n",
    "                y_pred.extend(preds)\n",
    "            else:\n",
    "                X, y = batch\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                outputs = model(X)\n",
    "                preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "                y_true.extend(y.cpu().numpy())\n",
    "                y_pred.extend(preds)\n",
    "\n",
    "    metrics = compute_metrics(y_true, y_pred, labels=list(range(4)))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d7f517e1-0ff3-4100-9106-2140979f0287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Función objective global ---\n",
    "def objective(trial):\n",
    "    model_name = trial.suggest_categorical(\"model\", [\"Transformer\", \"GCN\", \"MS-TCN\", \"TGU\", \"SAN\", \"LSTM\"])\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
    "    hidden = trial.suggest_categorical(\"hidden\", [64, 128, 256])\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if model_name == \"Transformer\":\n",
    "        model = TransformerBaseline(d_model=hidden, dropout=dropout).to(device)\n",
    "        is_graph = False\n",
    "        train_loader, val_loader = train_loader_seq, val_loader_seq\n",
    "\n",
    "    elif model_name == \"GCN\":\n",
    "        model = GCNBaseline(hidden_channels=hidden).to(device)\n",
    "        is_graph = True\n",
    "        train_loader, val_loader = train_loader_graph, val_loader_graph\n",
    "\n",
    "    elif model_name == \"MS-TCN\":\n",
    "        model = MSTCNBaseline().to(device)\n",
    "        is_graph = False\n",
    "        train_loader, val_loader = train_loader_seq, val_loader_seq\n",
    "\n",
    "    elif model_name == \"TGU\":\n",
    "        model = TGUBaseline(hidden=hidden).to(device)\n",
    "        is_graph = False\n",
    "        train_loader, val_loader = train_loader_seq, val_loader_seq\n",
    "\n",
    "    elif model_name == \"SAN\":\n",
    "        model = SANBaseline(hidden=hidden).to(device)\n",
    "        is_graph = False\n",
    "        train_loader, val_loader = train_loader_seq, val_loader_seq\n",
    "\n",
    "    elif model_name == \"LSTM\":\n",
    "        model = LSTMBaseline(hidden1=hidden, hidden2=hidden, hidden3=hidden//2).to(device)\n",
    "        is_graph = False\n",
    "        train_loader, val_loader = train_loader_seq, val_loader_seq\n",
    "\n",
    "    metrics = train_and_validate(model, lr, train_loader, val_loader, device, is_graph)\n",
    "    val_acc = metrics[\"accuracy\"]\n",
    "\n",
    "    # Guardar resultados\n",
    "    result = {\n",
    "        \"trial\": trial.number,\n",
    "        \"model\": model_name,\n",
    "        \"lr\": lr,\n",
    "        \"hidden\": hidden,\n",
    "        \"dropout\": dropout,\n",
    "        \"metrics\": metrics\n",
    "    }\n",
    "    all_results.append(result)\n",
    "\n",
    "    # Mejor por modelo\n",
    "    if model_name not in best_by_model or val_acc > best_by_model[model_name][\"metrics\"][\"accuracy\"]:\n",
    "        best_by_model[model_name] = result\n",
    "\n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9b124644-ec7a-494a-8acc-3cb115814286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 19:59:22,484] A new study created in memory with name: no-name-1c6b8ee8-1ef9-4fd3-b982-372dcbab513c\n",
      "/tmp/ipykernel_5736/1968659885.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
      "[I 2025-10-12 20:02:54,394] Trial 0 finished with value: 0.8777777777777778 and parameters: {'model': 'Transformer', 'lr': 4.46196489907191e-05, 'hidden': 256, 'dropout': 0.37225294478346715}. Best is trial 0 with value: 0.8777777777777778.\n",
      "/tmp/ipykernel_5736/1968659885.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
      "[I 2025-10-12 20:04:31,470] Trial 1 finished with value: 0.7888888888888889 and parameters: {'model': 'LSTM', 'lr': 0.00044484736104396925, 'hidden': 128, 'dropout': 0.118817167731916}. Best is trial 0 with value: 0.8777777777777778.\n",
      "/tmp/ipykernel_5736/1968659885.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
      "[I 2025-10-12 20:06:04,917] Trial 2 finished with value: 0.8055555555555556 and parameters: {'model': 'TGU', 'lr': 0.0003699549264336474, 'hidden': 128, 'dropout': 0.3979163716275135}. Best is trial 0 with value: 0.8777777777777778.\n",
      "/tmp/ipykernel_5736/1968659885.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
      "[I 2025-10-12 20:08:47,597] Trial 3 finished with value: 0.8944444444444445 and parameters: {'model': 'LSTM', 'lr': 1.830140123605193e-05, 'hidden': 256, 'dropout': 0.4481804529215425}. Best is trial 3 with value: 0.8944444444444445.\n",
      "/tmp/ipykernel_5736/1968659885.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
      "[I 2025-10-12 20:12:15,435] Trial 4 finished with value: 0.46111111111111114 and parameters: {'model': 'SAN', 'lr': 0.0036138077222680554, 'hidden': 256, 'dropout': 0.22668264988768452}. Best is trial 3 with value: 0.8944444444444445.\n",
      "/tmp/ipykernel_5736/1968659885.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
      "[I 2025-10-12 20:14:57,337] Trial 5 finished with value: 0.9055555555555556 and parameters: {'model': 'LSTM', 'lr': 0.0010193104006028422, 'hidden': 256, 'dropout': 0.18713696413043307}. Best is trial 5 with value: 0.9055555555555556.\n",
      "/tmp/ipykernel_5736/1968659885.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
      "[I 2025-10-12 20:17:41,131] Trial 6 finished with value: 0.8555555555555555 and parameters: {'model': 'LSTM', 'lr': 4.479041215582013e-05, 'hidden': 256, 'dropout': 0.42138938558042616}. Best is trial 5 with value: 0.9055555555555556.\n",
      "/tmp/ipykernel_5736/1968659885.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
      "[I 2025-10-12 20:19:28,067] Trial 7 finished with value: 0.85 and parameters: {'model': 'MS-TCN', 'lr': 0.00017144126956868794, 'hidden': 256, 'dropout': 0.36585519007395917}. Best is trial 5 with value: 0.9055555555555556.\n",
      "/tmp/ipykernel_5736/1968659885.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
      "[I 2025-10-12 20:21:04,876] Trial 8 finished with value: 0.8055555555555556 and parameters: {'model': 'LSTM', 'lr': 0.0004917408389348053, 'hidden': 128, 'dropout': 0.272064082566343}. Best is trial 5 with value: 0.9055555555555556.\n",
      "/tmp/ipykernel_5736/1968659885.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
      "[I 2025-10-12 20:22:42,297] Trial 9 finished with value: 0.25 and parameters: {'model': 'LSTM', 'lr': 0.0085675413345481, 'hidden': 128, 'dropout': 0.309576955594424}. Best is trial 5 with value: 0.9055555555555556.\n",
      "/tmp/ipykernel_5736/1968659885.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
      "[I 2025-10-12 20:22:57,416] Trial 10 finished with value: 0.7555555555555555 and parameters: {'model': 'GCN', 'lr': 0.0018206753257717615, 'hidden': 64, 'dropout': 0.1427716111694379}. Best is trial 5 with value: 0.9055555555555556.\n",
      "/tmp/ipykernel_5736/1968659885.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
      "[I 2025-10-12 20:25:39,873] Trial 11 finished with value: 0.8833333333333333 and parameters: {'model': 'LSTM', 'lr': 1.3135340922469473e-05, 'hidden': 256, 'dropout': 0.4824300732317479}. Best is trial 5 with value: 0.9055555555555556.\n",
      "/tmp/ipykernel_5736/1968659885.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
      "[I 2025-10-12 20:25:54,866] Trial 12 finished with value: 0.7666666666666667 and parameters: {'model': 'GCN', 'lr': 0.0012208857257881935, 'hidden': 64, 'dropout': 0.19444206989064297}. Best is trial 5 with value: 0.9055555555555556.\n",
      "/tmp/ipykernel_5736/1968659885.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
      "[I 2025-10-12 20:28:12,652] Trial 13 finished with value: 0.7666666666666667 and parameters: {'model': 'TGU', 'lr': 7.825744698900464e-05, 'hidden': 256, 'dropout': 0.4863357261263381}. Best is trial 5 with value: 0.9055555555555556.\n",
      "/tmp/ipykernel_5736/1968659885.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
      "[I 2025-10-12 20:30:00,285] Trial 14 finished with value: 0.8388888888888889 and parameters: {'model': 'MS-TCN', 'lr': 1.4944213608278251e-05, 'hidden': 256, 'dropout': 0.29845065776707735}. Best is trial 5 with value: 0.9055555555555556.\n",
      "/tmp/ipykernel_5736/1968659885.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-12 20:34:12,899] Trial 15 finished with value: 0.9222222222222223 and parameters: {'model': 'Transformer', 'lr': 0.0012043476164750062, 'hidden': 256, 'dropout': 0.17141203879587477}. Best is trial 15 with value: 0.9222222222222223.\n",
      "/tmp/ipykernel_5736/1968659885.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-12 20:36:04,951] Trial 16 finished with value: 0.8722222222222222 and parameters: {'model': 'Transformer', 'lr': 0.0011378812334754903, 'hidden': 64, 'dropout': 0.17816418760733624}. Best is trial 15 with value: 0.9222222222222223.\n",
      "/tmp/ipykernel_5736/1968659885.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-12 20:41:49,701] Trial 17 finished with value: 0.9111111111111111 and parameters: {'model': 'Transformer', 'lr': 0.003658191066246519, 'hidden': 256, 'dropout': 0.23448066817641616}. Best is trial 15 with value: 0.9222222222222223.\n",
      "/tmp/ipykernel_5736/1968659885.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-12 20:46:08,310] Trial 18 finished with value: 0.7888888888888889 and parameters: {'model': 'Transformer', 'lr': 0.008545860727431145, 'hidden': 256, 'dropout': 0.23748459528837063}. Best is trial 15 with value: 0.9222222222222223.\n",
      "/tmp/ipykernel_5736/1968659885.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-12 20:47:58,998] Trial 19 finished with value: 0.9222222222222223 and parameters: {'model': 'Transformer', 'lr': 0.0028713339691158074, 'hidden': 64, 'dropout': 0.10774437284585237}. Best is trial 15 with value: 0.9222222222222223.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Resultados guardados en:\n",
      "  • optuna_all_results.json\n",
      "  • optuna_best_models.json\n",
      "\n",
      "🏆 Mejor modelo global:\n",
      "{'model': 'Transformer', 'lr': 0.0012043476164750062, 'hidden': 256, 'dropout': 0.17141203879587477}\n",
      "Accuracy de validación: 0.9222\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# --- Ejecución ---\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# --- Guardar resultados ---\n",
    "with open(\"optuna_all_results.json\", \"w\") as f:\n",
    "    json.dump(all_results, f, indent=4)\n",
    "3\n",
    "with open(\"optuna_best_models.json\", \"w\") as f:\n",
    "    json.dump(best_by_model, f, indent=4)\n",
    "\n",
    "print(\"\\n✅ Resultados guardados en:\")\n",
    "print(\"  • optuna_all_results.json\")\n",
    "print(\"  • optuna_best_models.json\")\n",
    "print(\"\\n🏆 Mejor modelo global:\")\n",
    "print(study.best_params)\n",
    "print(f\"Accuracy de validación: {study.best_value:.4f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a43a7c3-232a-4c5e-ab75-349867a585ec",
   "metadata": {},
   "source": [
    "# Optimización de hiperparámetros por modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fa8e8b15-54ff-415d-b5f4-b776f8c89a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, math\n",
    "import numpy as np\n",
    "import optuna\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, precision_score, recall_score, f1_score,\n",
    "    accuracy_score, balanced_accuracy_score\n",
    ")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9e022819-89b7-4bfd-baaf-2e9d030983a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= utilidades métricas =========\n",
    "def _metrics_from_preds(y_true, y_pred, n_classes=4):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(n_classes)))\n",
    "    tp = np.diag(cm).astype(float)\n",
    "    fp = cm.sum(axis=0) - tp\n",
    "    fn = cm.sum(axis=1) - tp\n",
    "    tn = cm.sum() - (tp + fp + fn)\n",
    "    eps = 1e-8\n",
    "    specificity_macro = np.mean(tn / (tn + fp + eps))\n",
    "    out = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision_macro\": precision_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"recall_macro\": recall_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"specificity_macro\": float(specificity_macro),\n",
    "        \"f1_macro\": f1_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"balanced_accuracy\": balanced_accuracy_score(y_true, y_pred),\n",
    "        \"confusion_matrix\": cm.tolist()\n",
    "    }\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "95d553f7-c3e9-4e09-a2e2-06b14bebae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= TRANSFORMER =========\n",
    "def optimize_transformer(train_loader_seq, val_loader_seq, n_trials=50, max_epochs=80, save_prefix=\"hpo_transformer\"):\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "\n",
    "    class PositionalEncoding(nn.Module):\n",
    "        def __init__(self, d_model, dropout=0.1, max_len=16):\n",
    "            super().__init__()\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            pe = torch.zeros(max_len, d_model)\n",
    "            pos = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "            div = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32)*(-math.log(10000.0)/d_model))\n",
    "            pe[:, 0::2] = torch.sin(pos*div)\n",
    "            pe[:, 1::2] = torch.cos(pos*div)\n",
    "            self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
    "        def forward(self, x):\n",
    "            return self.dropout(x + self.pe[:, :x.size(1), :])\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        def __init__(self, in_feat=258, n_classes=4, d_model=128, nhead=4, nl=2, ff=256, dropout=0.1, pool=\"mean\"):\n",
    "            super().__init__()\n",
    "            self.proj = nn.Linear(in_feat, d_model)\n",
    "            self.pe = PositionalEncoding(d_model, dropout, max_len=16)\n",
    "            enc_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n",
    "                                                   dim_feedforward=ff, dropout=dropout,\n",
    "                                                   batch_first=True, norm_first=True)\n",
    "            self.enc = nn.TransformerEncoder(enc_layer, num_layers=nl)\n",
    "            self.norm = nn.LayerNorm(d_model)\n",
    "            self.head = nn.Linear(d_model, n_classes)\n",
    "            self.pool = pool\n",
    "        def forward(self, x):\n",
    "            x = self.proj(x)\n",
    "            x = self.pe(x)\n",
    "            x = self.enc(x)\n",
    "            if self.pool == \"mean\":\n",
    "                x = x.mean(dim=1)\n",
    "            else:\n",
    "                x = x[:, -1, :]\n",
    "            x = self.norm(x)\n",
    "            return self.head(x)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner())\n",
    "    all_trials = []\n",
    "\n",
    "    def objective(trial):\n",
    "        d_model  = trial.suggest_categorical(\"d_model\", [128, 192, 256])\n",
    "        nhead    = trial.suggest_categorical(\"nhead\",  [2, 4, 8])\n",
    "        nlayers  = trial.suggest_int(\"num_layers\", 1, 4)\n",
    "        ff       = trial.suggest_categorical(\"ff\",     [256, 384, 512, 768])\n",
    "        dropout  = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
    "        lr       = trial.suggest_float(\"lr\", 5e-5, 5e-3, log=True)\n",
    "        wd       = trial.suggest_float(\"weight_decay\", 1e-10, 1e-3, log=True)\n",
    "        pool     = trial.suggest_categorical(\"pool\", [\"mean\", \"last\"])\n",
    "\n",
    "        model = Model(d_model=d_model, nhead=nhead, nl=nlayers, ff=ff, dropout=dropout, pool=pool).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "        best_val = 0.0\n",
    "        for epoch in range(max_epochs):\n",
    "            model.train()\n",
    "            for X, y in train_loader_seq:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(X)\n",
    "                loss = criterion(logits, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            # val\n",
    "            model.eval()\n",
    "            y_true, y_pred = [], []\n",
    "            with torch.no_grad():\n",
    "                for X, y in val_loader_seq:\n",
    "                    X, y = X.to(device), y.to(device)\n",
    "                    pred = model(X).argmax(1)\n",
    "                    y_true.extend(y.cpu().numpy()); y_pred.extend(pred.cpu().numpy())\n",
    "            metrics = _metrics_from_preds(y_true, y_pred, n_classes=4)\n",
    "            val_acc = metrics[\"accuracy\"]\n",
    "            trial.report(val_acc, epoch)\n",
    "            if val_acc > best_val: best_val = val_acc\n",
    "            if trial.should_prune(): raise optuna.TrialPruned()\n",
    "\n",
    "        all_trials.append({\"trial\": trial.number, \"params\": trial.params, \"best_val_acc\": best_val})\n",
    "        return best_val\n",
    "\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    # guardar\n",
    "    with open(f\"{save_prefix}_all.json\", \"w\") as f: json.dump(all_trials, f, indent=2)\n",
    "    with open(f\"{save_prefix}_best.json\", \"w\") as f: json.dump({\"best_params\": study.best_params, \"best_value\": study.best_value}, f, indent=2)\n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b379a0ab-fadb-4220-9d2d-4387c20eda55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= SAN (Self-Attention Network) =========\n",
    "def optimize_san(train_loader_seq, val_loader_seq, n_trials=50, max_epochs=80, save_prefix=\"hpo_san\"):\n",
    "    class Block(nn.Module):\n",
    "        def __init__(self, dim, heads, dropout):\n",
    "            super().__init__()\n",
    "            self.attn = nn.MultiheadAttention(dim, heads, dropout=dropout, batch_first=True)\n",
    "            self.ff = nn.Sequential(nn.Linear(dim, dim), nn.ReLU(), nn.Dropout(dropout))\n",
    "            self.n1 = nn.LayerNorm(dim); self.n2 = nn.LayerNorm(dim)\n",
    "        def forward(self, x):\n",
    "            a,_ = self.attn(x, x, x); x = self.n1(x + a)\n",
    "            f = self.ff(x);            x = self.n2(x + f)\n",
    "            return x\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        def __init__(self, in_features=258, hidden=128, heads=4, layers=2, dropout=0.1, pool=\"mean\"):\n",
    "            super().__init__()\n",
    "            self.proj = nn.Linear(in_features, hidden)\n",
    "            self.blocks = nn.ModuleList([Block(hidden, heads, dropout) for _ in range(layers)])\n",
    "            self.head = nn.Linear(hidden, 4)\n",
    "            self.pool = pool\n",
    "        def forward(self, x):\n",
    "            x = self.proj(x)\n",
    "            for b in self.blocks: x = b(x)\n",
    "            x = x.mean(1) if self.pool == \"mean\" else x[:, -1, :]\n",
    "            return self.head(x)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner())\n",
    "    all_trials = []\n",
    "\n",
    "    def objective(trial):\n",
    "        hidden = trial.suggest_categorical(\"hidden\", [128, 192, 256])\n",
    "        heads  = trial.suggest_categorical(\"heads\",  [2, 4, 8])\n",
    "        layers = trial.suggest_int(\"layers\", 1, 4)\n",
    "        dropout= trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
    "        lr     = trial.suggest_float(\"lr\", 5e-5, 5e-3, log=True)\n",
    "        wd     = trial.suggest_float(\"weight_decay\", 1e-10, 1e-3, log=True)\n",
    "        pool   = trial.suggest_categorical(\"pool\", [\"mean\", \"last\"])\n",
    "\n",
    "        model = Model(hidden=hidden, heads=heads, layers=layers, dropout=dropout, pool=pool).to(device)\n",
    "        crit = nn.CrossEntropyLoss()\n",
    "        opt  = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "        best_val = 0.0\n",
    "        for epoch in range(max_epochs):\n",
    "            model.train()\n",
    "            for X,y in train_loader_seq:\n",
    "                X,y = X.to(device), y.to(device)\n",
    "                opt.zero_grad(); loss = crit(model(X), y); loss.backward(); opt.step()\n",
    "            # val\n",
    "            model.eval()\n",
    "            y_true, y_pred = [], []\n",
    "            with torch.no_grad():\n",
    "                for X,y in val_loader_seq:\n",
    "                    X,y = X.to(device), y.to(device)\n",
    "                    pred = model(X).argmax(1)\n",
    "                    y_true.extend(y.cpu().numpy()); y_pred.extend(pred.cpu().numpy())\n",
    "            val_acc = _metrics_from_preds(y_true, y_pred)[\"accuracy\"]\n",
    "            if val_acc > best_val: best_val = val_acc\n",
    "            trial.report(val_acc, epoch)\n",
    "            if trial.should_prune(): raise optuna.TrialPruned()\n",
    "\n",
    "        all_trials.append({\"trial\": trial.number, \"params\": trial.params, \"best_val_acc\": best_val})\n",
    "        return best_val\n",
    "\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    with open(f\"{save_prefix}_all.json\",\"w\") as f: json.dump(all_trials,f,indent=2)\n",
    "    with open(f\"{save_prefix}_best.json\",\"w\") as f: json.dump({\"best_params\":study.best_params,\"best_value\":study.best_value},f,indent=2)\n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1fc86cb9-04f9-49d9-a6b5-63867333bc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= MS-TCN =========\n",
    "def optimize_mstcn(train_loader_seq, val_loader_seq, n_trials=50, max_epochs=80, save_prefix=\"hpo_mstcn\"):\n",
    "    from pytorch_tcn import TCN\n",
    "    class Model(nn.Module):\n",
    "        def __init__(self, in_features=258, channels=[128,128], k=3, dropout=0.2, pool=\"mean\"):\n",
    "            super().__init__()\n",
    "            self.tcn = TCN(num_inputs=in_features, num_channels=channels, kernel_size=k, dropout=dropout)\n",
    "            self.head = nn.Linear(channels[-1], 4)\n",
    "            self.pool = pool\n",
    "        def forward(self, x):\n",
    "            x = x.transpose(1,2)         # (B, C, L)\n",
    "            feat = self.tcn(x)           # (B, H, L)\n",
    "            feat = feat.mean(2) if self.pool==\"mean\" else feat[:,:,-1]\n",
    "            return self.head(feat)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner())\n",
    "    all_trials = []\n",
    "\n",
    "    def objective(trial):\n",
    "        depth   = trial.suggest_int(\"depth\", 2, 5)\n",
    "        width   = trial.suggest_categorical(\"width\", [64, 96, 128, 160])\n",
    "        channels= [width]*(depth)\n",
    "        ksize   = trial.suggest_categorical(\"kernel_size\", [2,3,5,7])\n",
    "        dropout = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
    "        lr      = trial.suggest_float(\"lr\", 5e-5, 5e-3, log=True)\n",
    "        wd      = trial.suggest_float(\"weight_decay\", 1e-10, 1e-3, log=True)\n",
    "        pool    = trial.suggest_categorical(\"pool\", [\"mean\",\"last\"])\n",
    "\n",
    "        model = Model(channels=channels, k=ksize, dropout=dropout, pool=pool).to(device)\n",
    "        crit = nn.CrossEntropyLoss()\n",
    "        opt  = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "        best_val = 0.0\n",
    "        for epoch in range(max_epochs):\n",
    "            model.train()\n",
    "            for X,y in train_loader_seq:\n",
    "                X,y = X.to(device), y.to(device)\n",
    "                opt.zero_grad(); loss = crit(model(X), y); loss.backward(); opt.step()\n",
    "            model.eval()\n",
    "            y_true, y_pred = [], []\n",
    "            with torch.no_grad():\n",
    "                for X,y in val_loader_seq:\n",
    "                    X,y = X.to(device), y.to(device)\n",
    "                    pred = model(X).argmax(1)\n",
    "                    y_true.extend(y.cpu().numpy()); y_pred.extend(pred.cpu().numpy())\n",
    "            val_acc = _metrics_from_preds(y_true, y_pred)[\"accuracy\"]\n",
    "            if val_acc > best_val: best_val = val_acc\n",
    "            trial.report(val_acc, epoch)\n",
    "            if trial.should_prune(): raise optuna.TrialPruned()\n",
    "\n",
    "        all_trials.append({\"trial\": trial.number, \"params\": trial.params, \"best_val_acc\": best_val})\n",
    "        return best_val\n",
    "\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    with open(f\"{save_prefix}_all.json\",\"w\") as f: json.dump(all_trials,f,indent=2)\n",
    "    with open(f\"{save_prefix}_best.json\",\"w\") as f: json.dump({\"best_params\":study.best_params,\"best_value\":study.best_value},f,indent=2)\n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8ec6f2b0-0068-475c-9615-130053701cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= TGU =========\n",
    "def optimize_tgu(train_loader_seq, val_loader_seq, n_trials=50, max_epochs=80, save_prefix=\"hpo_tgu\"):\n",
    "    class TGUBlock(nn.Module):\n",
    "        def __init__(self, in_ch, out_ch, k=3, d=1, drop=0.2):\n",
    "            super().__init__()\n",
    "            pad = (k-1)//2 * d\n",
    "            self.f = nn.Conv1d(in_ch, out_ch, k, padding=pad, dilation=d)\n",
    "            self.g = nn.Conv1d(in_ch, out_ch, k, padding=pad, dilation=d)\n",
    "            self.drop = nn.Dropout(drop)\n",
    "        def forward(self, x):\n",
    "            return self.drop(torch.tanh(self.f(x))*torch.sigmoid(self.g(x)))\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        def __init__(self, in_features=258, hidden=128, blocks=2, k=3, drop=0.2, pool=\"mean\", dilations=(1,2,4,8)):\n",
    "            super().__init__()\n",
    "            layers = []\n",
    "            ch_in = in_features\n",
    "            for i in range(blocks):\n",
    "                d = dilations[i % len(dilations)]\n",
    "                layers.append(TGUBlock(ch_in, hidden, k=k, d=d, drop=drop))\n",
    "                ch_in = hidden\n",
    "            self.net = nn.Sequential(*layers)\n",
    "            self.head = nn.Linear(hidden, 4)\n",
    "            self.pool = pool\n",
    "        def forward(self, x):\n",
    "            x = x.transpose(1,2)             # (B, C, L)\n",
    "            x = self.net(x)                   # (B, H, L)\n",
    "            x = x.mean(2) if self.pool==\"mean\" else x[:,:,-1]\n",
    "            return self.head(x)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner())\n",
    "    all_trials = []\n",
    "\n",
    "    def objective(trial):\n",
    "        hidden   = trial.suggest_categorical(\"hidden\", [96,128,160,192])\n",
    "        blocks   = trial.suggest_int(\"blocks\", 2, 5)\n",
    "        k        = trial.suggest_categorical(\"kernel\", [3,5,7])\n",
    "        drop     = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
    "        pool     = trial.suggest_categorical(\"pool\", [\"mean\",\"last\"])\n",
    "        lr       = trial.suggest_float(\"lr\", 5e-5, 5e-3, log=True)\n",
    "        wd       = trial.suggest_float(\"weight_decay\", 1e-10, 1e-3, log=True)\n",
    "\n",
    "        model = Model(hidden=hidden, blocks=blocks, k=k, drop=drop, pool=pool).to(device)\n",
    "        crit = nn.CrossEntropyLoss()\n",
    "        opt  = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "        best_val = 0.0\n",
    "        for epoch in range(max_epochs):\n",
    "            model.train()\n",
    "            for X,y in train_loader_seq:\n",
    "                X,y = X.to(device), y.to(device)\n",
    "                opt.zero_grad(); loss = crit(model(X), y); loss.backward(); opt.step()\n",
    "            model.eval()\n",
    "            y_true, y_pred = [], []\n",
    "            with torch.no_grad():\n",
    "                for X,y in val_loader_seq:\n",
    "                    X,y = X.to(device), y.to(device)\n",
    "                    pred = model(X).argmax(1)\n",
    "                    y_true.extend(y.cpu().numpy()); y_pred.extend(pred.cpu().numpy())\n",
    "            val_acc = _metrics_from_preds(y_true, y_pred)[\"accuracy\"]\n",
    "            if val_acc > best_val: best_val = val_acc\n",
    "            trial.report(val_acc, epoch)\n",
    "            if trial.should_prune(): raise optuna.TrialPruned()\n",
    "\n",
    "        all_trials.append({\"trial\": trial.number, \"params\": trial.params, \"best_val_acc\": best_val})\n",
    "        return best_val\n",
    "\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    with open(f\"{save_prefix}_all.json\",\"w\") as f: json.dump(all_trials,f,indent=2)\n",
    "    with open(f\"{save_prefix}_best.json\",\"w\") as f: json.dump({\"best_params\":study.best_params,\"best_value\":study.best_value},f,indent=2)\n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e38c74b7-78d5-4430-8a37-ae1a76dcb93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= LSTM =========\n",
    "def optimize_lstm(train_loader_seq, val_loader_seq, n_trials=50, max_epochs=80, save_prefix=\"hpo_lstm\"):\n",
    "    class Model(nn.Module):\n",
    "        def __init__(self, in_features=258, h1=128, h2=128, h3=64, bidir=False, drop=0.0, dense=64):\n",
    "            super().__init__()\n",
    "            self.bidir = bidir\n",
    "            self.l1 = nn.LSTM(in_features, h1, batch_first=True, bidirectional=bidir, dropout=0.0)\n",
    "            in2 = h1*(2 if bidir else 1)\n",
    "            self.l2 = nn.LSTM(in2, h2, batch_first=True, bidirectional=bidir, dropout=0.0)\n",
    "            in3 = h2*(2 if bidir else 1)\n",
    "            self.l3 = nn.LSTM(in3, h3, batch_first=True, bidirectional=bidir, dropout=0.0)\n",
    "            last = h3*(2 if bidir else 1)\n",
    "            self.dropout = nn.Dropout(drop)\n",
    "            self.fc1 = nn.Linear(last, dense)\n",
    "            self.fc2 = nn.Linear(dense, 32)\n",
    "            self.fc3 = nn.Linear(32, 4)\n",
    "        def forward(self, x):\n",
    "            x,_ = self.l1(x); x,_ = self.l2(x); x,_ = self.l3(x)\n",
    "            x = x[:, -1, :]\n",
    "            x = self.dropout(torch.relu(self.fc1(x)))\n",
    "            x = torch.relu(self.fc2(x))\n",
    "            return self.fc3(x)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner())\n",
    "    all_trials = []\n",
    "\n",
    "    def objective(trial):\n",
    "        h1 = trial.suggest_categorical(\"h1\",[96,128,160,192])\n",
    "        h2 = trial.suggest_categorical(\"h2\",[96,128,160,192])\n",
    "        h3 = trial.suggest_categorical(\"h3\",[48,64,96,128])\n",
    "        bidir = trial.suggest_categorical(\"bidirectional\",[False, True])\n",
    "        dense = trial.suggest_categorical(\"dense\",[32,64,96,128])\n",
    "        drop = trial.suggest_float(\"dropout\",0.0,0.5)\n",
    "        lr  = trial.suggest_float(\"lr\",5e-5,5e-3,log=True)\n",
    "        wd  = trial.suggest_float(\"weight_decay\", 1e-10,1e-3,log=True)\n",
    "\n",
    "        model = Model(h1=h1,h2=h2,h3=h3,bidir=bidir,drop=drop,dense=dense).to(device)\n",
    "        crit = nn.CrossEntropyLoss()\n",
    "        opt  = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "        best_val=0.0\n",
    "        for epoch in range(max_epochs):\n",
    "            model.train()\n",
    "            for X,y in train_loader_seq:\n",
    "                X,y = X.to(device), y.to(device)\n",
    "                opt.zero_grad(); loss = crit(model(X), y); loss.backward(); opt.step()\n",
    "            model.eval()\n",
    "            y_true, y_pred = [], []\n",
    "            with torch.no_grad():\n",
    "                for X,y in val_loader_seq:\n",
    "                    X,y = X.to(device), y.to(device)\n",
    "                    pred = model(X).argmax(1)\n",
    "                    y_true.extend(y.cpu().numpy()); y_pred.extend(pred.cpu().numpy())\n",
    "            val_acc = _metrics_from_preds(y_true, y_pred)[\"accuracy\"]\n",
    "            if val_acc > best_val: best_val = val_acc\n",
    "            trial.report(val_acc, epoch)\n",
    "            if trial.should_prune(): raise optuna.TrialPruned()\n",
    "\n",
    "        all_trials.append({\"trial\": trial.number, \"params\": trial.params, \"best_val_acc\": best_val})\n",
    "        return best_val\n",
    "\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    with open(f\"{save_prefix}_all.json\",\"w\") as f: json.dump(all_trials,f,indent=2)\n",
    "    with open(f\"{save_prefix}_best.json\",\"w\") as f: json.dump({\"best_params\":study.best_params,\"best_value\":study.best_value},f,indent=2)\n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0ef6b626-8ef1-49f9-a583-6e94026fbdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= GCN =========\n",
    "def optimize_gcn(train_loader_graph, val_loader_graph, n_trials=50, max_epochs=80, save_prefix=\"hpo_gcn\"):\n",
    "    from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        def __init__(self, in_ch=258, hidden=128, layers=2, dropout=0.1):\n",
    "            super().__init__()\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.layers = nn.ModuleList()\n",
    "            self.layers.append(GCNConv(in_ch, hidden))\n",
    "            for _ in range(layers-1):\n",
    "                self.layers.append(GCNConv(hidden, hidden))\n",
    "            self.lin = nn.Linear(hidden, 4)\n",
    "        def forward(self, x, edge_index, batch):\n",
    "            for i,conv in enumerate(self.layers):\n",
    "                x = conv(x, edge_index)\n",
    "                x = torch.relu(x)\n",
    "                x = self.dropout(x)\n",
    "            x = global_mean_pool(x, batch)\n",
    "            return self.lin(x)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner())\n",
    "    all_trials = []\n",
    "\n",
    "    def objective(trial):\n",
    "        hidden  = trial.suggest_categorical(\"hidden\",[96,128,160,192])\n",
    "        layers  = trial.suggest_int(\"layers\", 2, 5)\n",
    "        drop    = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
    "        lr      = trial.suggest_float(\"lr\", 5e-5, 5e-3, log=True)\n",
    "        wd      = trial.suggest_float(\"weight_decay\", 1e-10, 1e-3, log=True)\n",
    "\n",
    "        model = Model(hidden=hidden, layers=layers, dropout=drop).to(device)\n",
    "        crit = nn.CrossEntropyLoss()\n",
    "        opt  = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "        best_val=0.0\n",
    "        for epoch in range(max_epochs):\n",
    "            model.train()\n",
    "            for batch in train_loader_graph:\n",
    "                batch = batch.to(device)\n",
    "                opt.zero_grad()\n",
    "                out = model(batch.x, batch.edge_index, batch.batch)\n",
    "                loss = crit(out, batch.y)\n",
    "                loss.backward(); opt.step()\n",
    "            # val\n",
    "            model.eval()\n",
    "            y_true, y_pred = [], []\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader_graph:\n",
    "                    batch = batch.to(device)\n",
    "                    pred = model(batch.x, batch.edge_index, batch.batch).argmax(1)\n",
    "                    y_true.extend(batch.y.cpu().numpy()); y_pred.extend(pred.cpu().numpy())\n",
    "            val_acc = _metrics_from_preds(y_true, y_pred)[\"accuracy\"]\n",
    "            if val_acc > best_val: best_val = val_acc\n",
    "            trial.report(val_acc, epoch)\n",
    "            if trial.should_prune(): raise optuna.TrialPruned()\n",
    "\n",
    "        all_trials.append({\"trial\": trial.number, \"params\": trial.params, \"best_val_acc\": best_val})\n",
    "        return best_val\n",
    "\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    with open(f\"{save_prefix}_all.json\",\"w\") as f: json.dump(all_trials,f,indent=2)\n",
    "    with open(f\"{save_prefix}_best.json\",\"w\") as f: json.dump({\"best_params\":study.best_params,\"best_value\":study.best_value},f,indent=2)\n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "904ab5ca-59f1-4217-b917-1a67a62852b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-13 12:52:30,632] A new study created in memory with name: no-name-d2f5b581-c3f9-4453-acdf-98702efb4a60\n",
      "[I 2025-10-13 12:54:21,611] Trial 0 finished with value: 0.9277777777777778 and parameters: {'d_model': 128, 'nhead': 8, 'num_layers': 1, 'ff': 384, 'dropout': 0.03435628388273493, 'lr': 0.0009937840388937165, 'weight_decay': 8.122583154571129e-10, 'pool': 'last'}. Best is trial 0 with value: 0.9277777777777778.\n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 12:59:09,977] Trial 1 finished with value: 0.9 and parameters: {'d_model': 192, 'nhead': 2, 'num_layers': 3, 'ff': 512, 'dropout': 0.33442943209867587, 'lr': 0.00011574651308542461, 'weight_decay': 2.967641556059196e-05, 'pool': 'last'}. Best is trial 0 with value: 0.9277777777777778.\n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:02:44,363] Trial 2 finished with value: 0.9111111111111111 and parameters: {'d_model': 128, 'nhead': 4, 'num_layers': 2, 'ff': 768, 'dropout': 0.07635445365344684, 'lr': 0.00016461025786795473, 'weight_decay': 0.00010527532171827261, 'pool': 'mean'}. Best is trial 0 with value: 0.9277777777777778.\n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:11:16,765] Trial 3 finished with value: 0.7111111111111111 and parameters: {'d_model': 192, 'nhead': 4, 'num_layers': 4, 'ff': 384, 'dropout': 0.3968740539714857, 'lr': 0.004645990386739962, 'weight_decay': 1.3001418002534027e-08, 'pool': 'last'}. Best is trial 0 with value: 0.9277777777777778.\n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:13:29,724] Trial 4 finished with value: 0.9166666666666666 and parameters: {'d_model': 128, 'nhead': 8, 'num_layers': 1, 'ff': 512, 'dropout': 0.42330855563971415, 'lr': 0.0006441065666346115, 'weight_decay': 3.1611008915027446e-06, 'pool': 'mean'}. Best is trial 0 with value: 0.9277777777777778.\n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:13:31,014] Trial 5 pruned. \n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:13:32,422] Trial 6 pruned. \n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:13:33,770] Trial 7 pruned. \n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:13:35,844] Trial 8 pruned. \n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:13:38,758] Trial 9 pruned. \n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:13:41,888] Trial 10 pruned. \n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:13:43,468] Trial 11 pruned. \n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:13:44,821] Trial 12 pruned. \n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:16:45,199] Trial 13 finished with value: 0.9444444444444444 and parameters: {'d_model': 128, 'nhead': 8, 'num_layers': 2, 'ff': 512, 'dropout': 0.14130069060222333, 'lr': 0.0019836797667691277, 'weight_decay': 1.2886096768199835e-08, 'pool': 'mean'}. Best is trial 13 with value: 0.9444444444444444.\n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:16:47,435] Trial 14 pruned. \n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:16:51,247] Trial 15 pruned. \n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:16:54,608] Trial 16 pruned. \n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:17:35,318] Trial 17 pruned. \n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:17:37,732] Trial 18 pruned. \n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:17:40,105] Trial 19 pruned. \n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:17:41,338] Trial 20 pruned. \n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:19:49,324] Trial 21 finished with value: 0.9222222222222223 and parameters: {'d_model': 128, 'nhead': 8, 'num_layers': 1, 'ff': 512, 'dropout': 0.27943098353215723, 'lr': 0.0005662506815624966, 'weight_decay': 1.718436519569361e-06, 'pool': 'mean'}. Best is trial 13 with value: 0.9444444444444444.\n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:19:50,605] Trial 22 pruned. \n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:19:52,401] Trial 23 pruned. \n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:19:54,890] Trial 24 pruned. \n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:19:58,215] Trial 25 pruned. \n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:19:59,569] Trial 26 pruned. \n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:20:01,932] Trial 27 pruned. \n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:20:04,369] Trial 28 pruned. \n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:20:09,861] Trial 29 pruned. \n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:20:11,192] Trial 30 pruned. \n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:20:12,529] Trial 31 pruned. \n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:20:13,860] Trial 32 pruned. \n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:20:15,698] Trial 33 pruned. \n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:20:16,975] Trial 34 pruned. \n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:20:19,957] Trial 35 pruned. \n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:20:21,303] Trial 36 pruned. \n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:20:24,131] Trial 37 pruned. \n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:20:25,320] Trial 38 pruned. \n",
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 13:20:26,848] Trial 39 pruned. \n"
     ]
    }
   ],
   "source": [
    "study_tf  = optimize_transformer(train_loader_seq, val_loader_seq, n_trials=40, max_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3178a554-5ab9-447c-a96d-f635a09557b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-13 13:27:57,387] A new study created in memory with name: no-name-b5c28b08-0136-42c8-81d3-cfa0f3c16238\n",
      "[I 2025-10-13 13:31:14,386] Trial 0 finished with value: 0.9444444444444444 and parameters: {'hidden': 256, 'heads': 2, 'layers': 3, 'dropout': 0.09228891164213615, 'lr': 0.000223814062092218, 'weight_decay': 5.456686692227437e-06, 'pool': 'mean'}. Best is trial 0 with value: 0.9444444444444444.\n",
      "[I 2025-10-13 13:33:29,553] Trial 1 finished with value: 0.9111111111111111 and parameters: {'hidden': 192, 'heads': 4, 'layers': 2, 'dropout': 0.24858524705564422, 'lr': 5.044241589123786e-05, 'weight_decay': 0.0001078192872518563, 'pool': 'last'}. Best is trial 0 with value: 0.9444444444444444.\n",
      "[I 2025-10-13 13:35:52,184] Trial 2 finished with value: 0.9222222222222223 and parameters: {'hidden': 192, 'heads': 8, 'layers': 2, 'dropout': 0.45809627493388444, 'lr': 0.0002666899425838391, 'weight_decay': 5.4398945921073874e-05, 'pool': 'mean'}. Best is trial 0 with value: 0.9444444444444444.\n",
      "[I 2025-10-13 13:38:22,791] Trial 3 finished with value: 0.9166666666666666 and parameters: {'hidden': 256, 'heads': 8, 'layers': 1, 'dropout': 0.19873570697348764, 'lr': 0.0031832589449244395, 'weight_decay': 9.321202519703227e-10, 'pool': 'mean'}. Best is trial 0 with value: 0.9444444444444444.\n",
      "[I 2025-10-13 13:44:15,375] Trial 4 finished with value: 0.8277777777777777 and parameters: {'hidden': 256, 'heads': 2, 'layers': 4, 'dropout': 0.30857164381539465, 'lr': 0.0010392005045136686, 'weight_decay': 6.836530049170249e-09, 'pool': 'last'}. Best is trial 0 with value: 0.9444444444444444.\n",
      "[I 2025-10-13 13:44:17,065] Trial 5 pruned. \n",
      "[I 2025-10-13 13:44:19,173] Trial 6 pruned. \n",
      "[I 2025-10-13 13:44:20,409] Trial 7 pruned. \n",
      "[I 2025-10-13 13:44:22,092] Trial 8 pruned. \n",
      "[I 2025-10-13 13:46:53,585] Trial 9 finished with value: 0.9277777777777778 and parameters: {'hidden': 128, 'heads': 4, 'layers': 4, 'dropout': 0.08359370279580514, 'lr': 0.00037765563749700783, 'weight_decay': 1.239703508690095e-09, 'pool': 'last'}. Best is trial 0 with value: 0.9444444444444444.\n",
      "[I 2025-10-13 13:46:54,746] Trial 10 pruned. \n",
      "[I 2025-10-13 13:46:56,264] Trial 11 pruned. \n",
      "[I 2025-10-13 13:49:08,256] Trial 12 finished with value: 0.9055555555555556 and parameters: {'hidden': 128, 'heads': 4, 'layers': 3, 'dropout': 0.08757578191137852, 'lr': 0.00035806190534986887, 'weight_decay': 1.0018445792461337e-06, 'pool': 'last'}. Best is trial 0 with value: 0.9444444444444444.\n",
      "[I 2025-10-13 13:49:10,781] Trial 13 pruned. \n",
      "[I 2025-10-13 13:49:12,138] Trial 14 pruned. \n",
      "[I 2025-10-13 13:49:22,322] Trial 15 pruned. \n",
      "[I 2025-10-13 13:51:35,252] Trial 16 finished with value: 0.95 and parameters: {'hidden': 128, 'heads': 4, 'layers': 3, 'dropout': 0.16109724023463168, 'lr': 0.0006942474856216337, 'weight_decay': 0.0009089520470960192, 'pool': 'mean'}. Best is trial 16 with value: 0.95.\n",
      "[I 2025-10-13 13:51:36,624] Trial 17 pruned. \n",
      "[I 2025-10-13 13:51:38,215] Trial 18 pruned. \n",
      "[I 2025-10-13 13:54:30,222] Trial 19 finished with value: 0.9444444444444444 and parameters: {'hidden': 192, 'heads': 8, 'layers': 3, 'dropout': 0.18637820572876612, 'lr': 0.0006144643834455591, 'weight_decay': 8.165997650382793e-06, 'pool': 'mean'}. Best is trial 16 with value: 0.95.\n",
      "[I 2025-10-13 13:54:31,822] Trial 20 pruned. \n",
      "[I 2025-10-13 13:54:35,265] Trial 21 pruned. \n",
      "[I 2025-10-13 13:57:27,610] Trial 22 finished with value: 0.9333333333333333 and parameters: {'hidden': 192, 'heads': 8, 'layers': 3, 'dropout': 0.13969578394082272, 'lr': 0.00023064618046362512, 'weight_decay': 8.90505753628109e-08, 'pool': 'mean'}. Best is trial 16 with value: 0.95.\n",
      "[I 2025-10-13 13:57:29,366] Trial 23 pruned. \n",
      "[I 2025-10-13 13:57:32,961] Trial 24 pruned. \n",
      "[I 2025-10-13 13:57:34,347] Trial 25 pruned. \n",
      "[I 2025-10-13 13:57:39,643] Trial 26 pruned. \n",
      "[I 2025-10-13 13:57:41,701] Trial 27 pruned. \n",
      "[I 2025-10-13 13:57:42,895] Trial 28 pruned. \n",
      "[I 2025-10-13 13:57:44,273] Trial 29 pruned. \n",
      "[I 2025-10-13 14:01:06,576] Trial 30 finished with value: 0.9277777777777778 and parameters: {'hidden': 192, 'heads': 4, 'layers': 4, 'dropout': 0.04301611428167476, 'lr': 7.447248227707333e-05, 'weight_decay': 3.7336699317491776e-06, 'pool': 'mean'}. Best is trial 16 with value: 0.95.\n",
      "[I 2025-10-13 14:01:08,617] Trial 31 pruned. \n",
      "[I 2025-10-13 14:01:12,391] Trial 32 pruned. \n",
      "[I 2025-10-13 14:01:14,575] Trial 33 pruned. \n",
      "[I 2025-10-13 14:01:19,262] Trial 34 pruned. \n",
      "[I 2025-10-13 14:01:21,393] Trial 35 pruned. \n",
      "[I 2025-10-13 14:01:24,153] Trial 36 pruned. \n",
      "[I 2025-10-13 14:01:25,500] Trial 37 pruned. \n",
      "[I 2025-10-13 14:01:33,582] Trial 38 pruned. \n",
      "[I 2025-10-13 14:01:48,497] Trial 39 pruned. \n"
     ]
    }
   ],
   "source": [
    "study_san = optimize_san(train_loader_seq, val_loader_seq, n_trials=40, max_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "32ae33f7-2a5c-444d-b0fa-cea3e1981b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-13 14:01:48,504] A new study created in memory with name: no-name-c6080afa-1ec8-4a1d-bad2-6c5ac40a286b\n",
      "[I 2025-10-13 14:04:45,944] Trial 0 finished with value: 0.9111111111111111 and parameters: {'depth': 5, 'width': 128, 'kernel_size': 2, 'dropout': 0.22436922973443996, 'lr': 0.001325731770375339, 'weight_decay': 0.00025610437619332275, 'pool': 'last'}. Best is trial 0 with value: 0.9111111111111111.\n",
      "[I 2025-10-13 14:07:16,386] Trial 1 finished with value: 0.9 and parameters: {'depth': 5, 'width': 64, 'kernel_size': 5, 'dropout': 0.41698800945745235, 'lr': 0.00017903798852132868, 'weight_decay': 1.3948167581014725e-05, 'pool': 'mean'}. Best is trial 0 with value: 0.9111111111111111.\n",
      "[I 2025-10-13 14:10:29,203] Trial 2 finished with value: 0.9333333333333333 and parameters: {'depth': 3, 'width': 128, 'kernel_size': 7, 'dropout': 0.14250338040497074, 'lr': 0.001740373489282973, 'weight_decay': 1.4063216434301287e-10, 'pool': 'mean'}. Best is trial 2 with value: 0.9333333333333333.\n",
      "[I 2025-10-13 14:13:56,776] Trial 3 finished with value: 0.9111111111111111 and parameters: {'depth': 5, 'width': 160, 'kernel_size': 2, 'dropout': 0.33806535028895573, 'lr': 0.0028871938804112996, 'weight_decay': 1.2262800812183218e-06, 'pool': 'last'}. Best is trial 2 with value: 0.9333333333333333.\n",
      "[I 2025-10-13 14:16:25,257] Trial 4 finished with value: 0.9222222222222223 and parameters: {'depth': 5, 'width': 96, 'kernel_size': 2, 'dropout': 0.07389912439903129, 'lr': 0.0002701896949955564, 'weight_decay': 7.776582687362592e-08, 'pool': 'mean'}. Best is trial 2 with value: 0.9333333333333333.\n",
      "[I 2025-10-13 14:19:05,228] Trial 5 finished with value: 0.9 and parameters: {'depth': 2, 'width': 160, 'kernel_size': 5, 'dropout': 0.40477517407304797, 'lr': 0.0018388676305167136, 'weight_decay': 6.31901893909589e-10, 'pool': 'last'}. Best is trial 2 with value: 0.9333333333333333.\n",
      "[I 2025-10-13 14:19:07,051] Trial 6 pruned. \n",
      "[I 2025-10-13 14:19:08,694] Trial 7 pruned. \n",
      "[I 2025-10-13 14:19:09,788] Trial 8 pruned. \n",
      "[I 2025-10-13 14:19:11,137] Trial 9 pruned. \n",
      "[I 2025-10-13 14:19:12,920] Trial 10 pruned. \n",
      "[I 2025-10-13 14:19:15,558] Trial 11 pruned. \n",
      "[I 2025-10-13 14:19:17,132] Trial 12 pruned. \n",
      "[I 2025-10-13 14:19:24,150] Trial 13 pruned. \n",
      "[I 2025-10-13 14:19:25,626] Trial 14 pruned. \n",
      "[I 2025-10-13 14:19:27,176] Trial 15 pruned. \n",
      "[I 2025-10-13 14:19:28,463] Trial 16 pruned. \n",
      "[I 2025-10-13 14:19:30,130] Trial 17 pruned. \n",
      "[I 2025-10-13 14:19:34,740] Trial 18 pruned. \n",
      "[I 2025-10-13 14:19:39,281] Trial 19 pruned. \n",
      "[I 2025-10-13 14:19:44,336] Trial 20 pruned. \n",
      "[I 2025-10-13 14:19:46,136] Trial 21 pruned. \n",
      "[I 2025-10-13 14:19:53,222] Trial 22 pruned. \n",
      "[I 2025-10-13 14:19:56,294] Trial 23 pruned. \n",
      "[I 2025-10-13 14:19:58,124] Trial 24 pruned. \n",
      "[I 2025-10-13 14:22:13,493] Trial 25 finished with value: 0.9333333333333333 and parameters: {'depth': 4, 'width': 96, 'kernel_size': 2, 'dropout': 0.049324717448423316, 'lr': 0.0017984183683030913, 'weight_decay': 3.953430092925795e-06, 'pool': 'mean'}. Best is trial 2 with value: 0.9333333333333333.\n",
      "[I 2025-10-13 14:22:14,750] Trial 26 pruned. \n",
      "[I 2025-10-13 14:22:17,897] Trial 27 pruned. \n",
      "[I 2025-10-13 14:22:19,127] Trial 28 pruned. \n",
      "[I 2025-10-13 14:22:20,504] Trial 29 pruned. \n",
      "[I 2025-10-13 14:22:23,306] Trial 30 pruned. \n",
      "[I 2025-10-13 14:22:27,348] Trial 31 pruned. \n",
      "[I 2025-10-13 14:22:28,665] Trial 32 pruned. \n",
      "[I 2025-10-13 14:22:52,889] Trial 33 pruned. \n",
      "[I 2025-10-13 14:25:25,297] Trial 34 finished with value: 0.9444444444444444 and parameters: {'depth': 5, 'width': 96, 'kernel_size': 2, 'dropout': 0.12684252199550655, 'lr': 0.0016479374297599736, 'weight_decay': 6.851143697922016e-10, 'pool': 'mean'}. Best is trial 34 with value: 0.9444444444444444.\n",
      "[I 2025-10-13 14:25:26,834] Trial 35 pruned. \n",
      "[I 2025-10-13 14:25:29,812] Trial 36 pruned. \n",
      "[I 2025-10-13 14:25:32,522] Trial 37 pruned. \n",
      "[I 2025-10-13 14:25:33,929] Trial 38 pruned. \n",
      "[I 2025-10-13 14:25:37,927] Trial 39 pruned. \n"
     ]
    }
   ],
   "source": [
    "study_tcn = optimize_mstcn(train_loader_seq, val_loader_seq, n_trials=40, max_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d496021f-c923-4059-ad6a-110eba919333",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-13 14:25:37,934] A new study created in memory with name: no-name-8e4d05ec-ce8e-46e1-b3a0-db900f712fb4\n",
      "[I 2025-10-13 14:28:42,706] Trial 0 finished with value: 0.8277777777777777 and parameters: {'hidden': 128, 'blocks': 5, 'kernel': 5, 'dropout': 0.12750410955633024, 'pool': 'last', 'lr': 0.0004582958477769386, 'weight_decay': 0.0008309063574448469}. Best is trial 0 with value: 0.8277777777777777.\n",
      "[I 2025-10-13 14:31:54,608] Trial 1 finished with value: 0.8444444444444444 and parameters: {'hidden': 160, 'blocks': 3, 'kernel': 7, 'dropout': 0.09139646604833113, 'pool': 'last', 'lr': 9.303623239731108e-05, 'weight_decay': 2.8353456172477696e-05}. Best is trial 1 with value: 0.8444444444444444.\n",
      "[I 2025-10-13 14:35:53,591] Trial 2 finished with value: 0.8444444444444444 and parameters: {'hidden': 192, 'blocks': 3, 'kernel': 7, 'dropout': 0.4903379591094786, 'pool': 'mean', 'lr': 0.0007408980181069967, 'weight_decay': 6.938805810037711e-07}. Best is trial 1 with value: 0.8444444444444444.\n",
      "[I 2025-10-13 14:37:40,547] Trial 3 finished with value: 0.7166666666666667 and parameters: {'hidden': 96, 'blocks': 2, 'kernel': 5, 'dropout': 0.43048928010047677, 'pool': 'last', 'lr': 0.00010607598079969751, 'weight_decay': 1.5279228484640577e-09}. Best is trial 1 with value: 0.8444444444444444.\n",
      "[I 2025-10-13 14:41:06,598] Trial 4 finished with value: 0.8333333333333334 and parameters: {'hidden': 160, 'blocks': 3, 'kernel': 7, 'dropout': 0.4127868190977168, 'pool': 'last', 'lr': 0.0022082613958436604, 'weight_decay': 8.631524536689925e-07}. Best is trial 1 with value: 0.8444444444444444.\n",
      "[I 2025-10-13 14:41:08,564] Trial 5 pruned. \n",
      "[I 2025-10-13 14:41:11,629] Trial 6 pruned. \n",
      "[I 2025-10-13 14:43:12,242] Trial 7 finished with value: 0.8722222222222222 and parameters: {'hidden': 128, 'blocks': 2, 'kernel': 5, 'dropout': 0.10177710856201305, 'pool': 'mean', 'lr': 0.0011524021214122685, 'weight_decay': 4.949953903138425e-09}. Best is trial 7 with value: 0.8722222222222222.\n",
      "[I 2025-10-13 14:43:21,926] Trial 8 pruned. \n",
      "[I 2025-10-13 14:45:08,814] Trial 9 finished with value: 0.8666666666666667 and parameters: {'hidden': 96, 'blocks': 2, 'kernel': 5, 'dropout': 0.3412788737762248, 'pool': 'mean', 'lr': 0.00040693492250752666, 'weight_decay': 6.473418262810394e-05}. Best is trial 7 with value: 0.8722222222222222.\n",
      "[I 2025-10-13 14:45:09,996] Trial 10 pruned. \n",
      "[I 2025-10-13 14:45:12,429] Trial 11 pruned. \n",
      "[I 2025-10-13 14:45:13,633] Trial 12 pruned. \n",
      "[I 2025-10-13 14:45:17,921] Trial 13 pruned. \n",
      "[I 2025-10-13 14:45:19,439] Trial 14 pruned. \n",
      "[I 2025-10-13 14:47:20,584] Trial 15 finished with value: 0.8833333333333333 and parameters: {'hidden': 128, 'blocks': 2, 'kernel': 5, 'dropout': 0.3154388200064646, 'pool': 'mean', 'lr': 0.0007151655746790033, 'weight_decay': 0.0005174402230886625}. Best is trial 15 with value: 0.8833333333333333.\n",
      "[I 2025-10-13 14:47:23,442] Trial 16 pruned. \n",
      "[I 2025-10-13 14:47:24,882] Trial 17 pruned. \n",
      "[I 2025-10-13 14:49:26,352] Trial 18 finished with value: 0.9111111111111111 and parameters: {'hidden': 128, 'blocks': 2, 'kernel': 5, 'dropout': 0.13439060474055683, 'pool': 'mean', 'lr': 0.0007350505756917321, 'weight_decay': 6.95276206231164e-06}. Best is trial 18 with value: 0.9111111111111111.\n",
      "[I 2025-10-13 14:49:27,625] Trial 19 pruned. \n",
      "[I 2025-10-13 14:49:28,980] Trial 20 pruned. \n",
      "[I 2025-10-13 14:49:31,386] Trial 21 pruned. \n",
      "[I 2025-10-13 14:49:32,583] Trial 22 pruned. \n",
      "[I 2025-10-13 14:49:33,796] Trial 23 pruned. \n",
      "[I 2025-10-13 14:52:44,760] Trial 24 finished with value: 0.8333333333333334 and parameters: {'hidden': 192, 'blocks': 3, 'kernel': 5, 'dropout': 0.14210099927575115, 'pool': 'mean', 'lr': 0.0006120229137619684, 'weight_decay': 1.0011328191218174e-08}. Best is trial 18 with value: 0.9111111111111111.\n",
      "[I 2025-10-13 14:52:46,005] Trial 25 pruned. \n",
      "[I 2025-10-13 14:52:48,834] Trial 26 pruned. \n",
      "[I 2025-10-13 14:52:51,259] Trial 27 pruned. \n",
      "[I 2025-10-13 14:52:53,161] Trial 28 pruned. \n",
      "[I 2025-10-13 14:52:54,879] Trial 29 pruned. \n",
      "[I 2025-10-13 14:53:00,348] Trial 30 pruned. \n",
      "[I 2025-10-13 14:53:01,446] Trial 31 pruned. \n",
      "[I 2025-10-13 14:53:05,734] Trial 32 pruned. \n",
      "[I 2025-10-13 14:53:07,279] Trial 33 pruned. \n",
      "[I 2025-10-13 14:53:08,546] Trial 34 pruned. \n",
      "[I 2025-10-13 14:53:11,686] Trial 35 pruned. \n",
      "[I 2025-10-13 14:53:13,610] Trial 36 pruned. \n",
      "[I 2025-10-13 14:53:15,006] Trial 37 pruned. \n",
      "[I 2025-10-13 14:53:16,259] Trial 38 pruned. \n",
      "[I 2025-10-13 14:53:17,622] Trial 39 pruned. \n"
     ]
    }
   ],
   "source": [
    "study_tgu = optimize_tgu(train_loader_seq, val_loader_seq, n_trials=40, max_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "211cbacb-46ce-4d32-9c5a-12fd4e6deef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-13 14:53:17,629] A new study created in memory with name: no-name-6ee8af51-8222-42cc-b343-552b82289b20\n",
      "[I 2025-10-13 14:55:27,956] Trial 0 finished with value: 0.8944444444444445 and parameters: {'h1': 192, 'h2': 128, 'h3': 96, 'bidirectional': False, 'dense': 96, 'dropout': 0.46570867055380855, 'lr': 0.0001948545988142792, 'weight_decay': 4.57655127220711e-05}. Best is trial 0 with value: 0.8944444444444445.\n",
      "[I 2025-10-13 14:57:58,761] Trial 1 finished with value: 0.8444444444444444 and parameters: {'h1': 96, 'h2': 128, 'h3': 48, 'bidirectional': True, 'dense': 64, 'dropout': 0.07536366867991445, 'lr': 6.01608705214315e-05, 'weight_decay': 1.0468824926347064e-05}. Best is trial 0 with value: 0.8944444444444445.\n",
      "[I 2025-10-13 15:00:06,917] Trial 2 finished with value: 0.9111111111111111 and parameters: {'h1': 160, 'h2': 160, 'h3': 96, 'bidirectional': False, 'dense': 64, 'dropout': 0.16126507565338144, 'lr': 6.889203750503244e-05, 'weight_decay': 4.394533926102311e-06}. Best is trial 2 with value: 0.9111111111111111.\n",
      "[I 2025-10-13 15:02:13,119] Trial 3 finished with value: 0.9166666666666666 and parameters: {'h1': 160, 'h2': 128, 'h3': 128, 'bidirectional': False, 'dense': 64, 'dropout': 0.4307094737751196, 'lr': 0.001958907259206594, 'weight_decay': 4.295149722643274e-08}. Best is trial 3 with value: 0.9166666666666666.\n",
      "[I 2025-10-13 15:04:57,508] Trial 4 finished with value: 0.8388888888888889 and parameters: {'h1': 128, 'h2': 96, 'h3': 96, 'bidirectional': True, 'dense': 96, 'dropout': 0.4978717558594501, 'lr': 0.0002746151310505234, 'weight_decay': 1.40581594047494e-10}. Best is trial 3 with value: 0.9166666666666666.\n",
      "[I 2025-10-13 15:05:01,623] Trial 5 pruned. \n",
      "[I 2025-10-13 15:05:34,525] Trial 6 pruned. \n",
      "[I 2025-10-13 15:06:48,866] Trial 7 pruned. \n",
      "[I 2025-10-13 15:06:53,203] Trial 8 pruned. \n",
      "[I 2025-10-13 15:07:20,071] Trial 9 pruned. \n",
      "[I 2025-10-13 15:07:21,540] Trial 10 pruned. \n",
      "[I 2025-10-13 15:09:33,542] Trial 11 finished with value: 0.9055555555555556 and parameters: {'h1': 160, 'h2': 160, 'h3': 128, 'bidirectional': False, 'dense': 64, 'dropout': 0.19180665198028252, 'lr': 0.001747185118723242, 'weight_decay': 1.891067237317302e-07}. Best is trial 3 with value: 0.9166666666666666.\n",
      "[I 2025-10-13 15:11:45,551] Trial 12 finished with value: 0.9222222222222223 and parameters: {'h1': 160, 'h2': 160, 'h3': 128, 'bidirectional': False, 'dense': 64, 'dropout': 0.17341150848087816, 'lr': 0.0019721730346766537, 'weight_decay': 8.398139499395018e-08}. Best is trial 12 with value: 0.9222222222222223.\n",
      "[I 2025-10-13 15:11:48,200] Trial 13 pruned. \n",
      "[I 2025-10-13 15:11:50,741] Trial 14 pruned. \n",
      "[I 2025-10-13 15:14:01,963] Trial 15 finished with value: 0.8944444444444445 and parameters: {'h1': 160, 'h2': 160, 'h3': 128, 'bidirectional': False, 'dense': 64, 'dropout': 0.13610317446923478, 'lr': 0.001037031015226246, 'weight_decay': 3.21875929142288e-07}. Best is trial 12 with value: 0.9222222222222223.\n",
      "[I 2025-10-13 15:14:07,783] Trial 16 pruned. \n",
      "[I 2025-10-13 15:14:28,610] Trial 17 pruned. \n",
      "[I 2025-10-13 15:14:30,009] Trial 18 pruned. \n",
      "[I 2025-10-13 15:16:36,501] Trial 19 finished with value: 0.8777777777777778 and parameters: {'h1': 160, 'h2': 128, 'h3': 128, 'bidirectional': False, 'dense': 64, 'dropout': 0.11514314460005562, 'lr': 0.0003064123251505579, 'weight_decay': 3.924818503540222e-08}. Best is trial 12 with value: 0.9222222222222223.\n",
      "[I 2025-10-13 15:16:38,991] Trial 20 pruned. \n",
      "[I 2025-10-13 15:16:40,293] Trial 21 pruned. \n",
      "[I 2025-10-13 15:16:41,587] Trial 22 pruned. \n",
      "[I 2025-10-13 15:16:42,918] Trial 23 pruned. \n",
      "[I 2025-10-13 15:16:45,574] Trial 24 pruned. \n",
      "[I 2025-10-13 15:16:46,824] Trial 25 pruned. \n",
      "[I 2025-10-13 15:16:57,682] Trial 26 pruned. \n",
      "[I 2025-10-13 15:16:58,937] Trial 27 pruned. \n",
      "[I 2025-10-13 15:17:00,168] Trial 28 pruned. \n",
      "[I 2025-10-13 15:17:01,499] Trial 29 pruned. \n",
      "[I 2025-10-13 15:19:04,371] Trial 30 finished with value: 0.8833333333333333 and parameters: {'h1': 160, 'h2': 128, 'h3': 96, 'bidirectional': False, 'dense': 64, 'dropout': 0.4516172541339223, 'lr': 0.0008392552453928757, 'weight_decay': 1.6906223986108265e-09}. Best is trial 12 with value: 0.9222222222222223.\n",
      "[I 2025-10-13 15:19:05,706] Trial 31 pruned. \n",
      "[I 2025-10-13 15:19:07,041] Trial 32 pruned. \n",
      "[I 2025-10-13 15:19:09,685] Trial 33 pruned. \n",
      "[I 2025-10-13 15:19:11,029] Trial 34 pruned. \n",
      "[I 2025-10-13 15:19:12,550] Trial 35 pruned. \n",
      "[I 2025-10-13 15:19:13,748] Trial 36 pruned. \n",
      "[I 2025-10-13 15:19:15,863] Trial 37 pruned. \n",
      "[I 2025-10-13 15:19:17,113] Trial 38 pruned. \n",
      "[I 2025-10-13 15:19:19,437] Trial 39 pruned. \n"
     ]
    }
   ],
   "source": [
    "study_lstm= optimize_lstm(train_loader_seq, val_loader_seq, n_trials=40, max_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f6359cc5-7801-48eb-9d29-cd741744d1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-13 15:19:19,443] A new study created in memory with name: no-name-4ca841aa-a724-46b0-a3c0-5655218f0686\n",
      "[I 2025-10-13 15:19:54,619] Trial 0 finished with value: 0.8777777777777778 and parameters: {'hidden': 128, 'layers': 3, 'dropout': 0.039493792770526726, 'lr': 0.00014986488393252883, 'weight_decay': 1.7651207864991065e-07}. Best is trial 0 with value: 0.8777777777777778.\n",
      "[I 2025-10-13 15:20:29,986] Trial 1 finished with value: 0.8944444444444445 and parameters: {'hidden': 192, 'layers': 2, 'dropout': 0.03072330581040994, 'lr': 0.0044483684902061815, 'weight_decay': 9.811324510588843e-07}. Best is trial 1 with value: 0.8944444444444445.\n",
      "[I 2025-10-13 15:21:26,022] Trial 2 finished with value: 0.85 and parameters: {'hidden': 128, 'layers': 5, 'dropout': 0.30914367360466427, 'lr': 0.0008551047009904848, 'weight_decay': 5.9182554121248435e-05}. Best is trial 1 with value: 0.8944444444444445.\n",
      "[I 2025-10-13 15:22:11,112] Trial 3 finished with value: 0.8777777777777778 and parameters: {'hidden': 160, 'layers': 3, 'dropout': 0.3628956370532507, 'lr': 0.0005491174130066744, 'weight_decay': 3.346805309927337e-05}. Best is trial 1 with value: 0.8944444444444445.\n",
      "[I 2025-10-13 15:23:22,969] Trial 4 finished with value: 0.9 and parameters: {'hidden': 192, 'layers': 5, 'dropout': 0.1579299208856086, 'lr': 0.00024644498733872316, 'weight_decay': 2.0820125063337135e-07}. Best is trial 4 with value: 0.9.\n",
      "[I 2025-10-13 15:23:24,370] Trial 5 pruned. \n",
      "[I 2025-10-13 15:23:30,746] Trial 6 pruned. \n",
      "[I 2025-10-13 15:23:31,047] Trial 7 pruned. \n",
      "[I 2025-10-13 15:23:31,533] Trial 8 pruned. \n",
      "[I 2025-10-13 15:23:32,029] Trial 9 pruned. \n",
      "[I 2025-10-13 15:23:32,542] Trial 10 pruned. \n",
      "[I 2025-10-13 15:23:32,904] Trial 11 pruned. \n",
      "[I 2025-10-13 15:24:45,134] Trial 12 finished with value: 0.8833333333333333 and parameters: {'hidden': 192, 'layers': 5, 'dropout': 0.13984996684773546, 'lr': 0.001158016044667737, 'weight_decay': 5.0907176568273775e-06}. Best is trial 4 with value: 0.9.\n",
      "[I 2025-10-13 15:24:45,718] Trial 13 pruned. \n",
      "[I 2025-10-13 15:24:46,811] Trial 14 pruned. \n",
      "[I 2025-10-13 15:24:47,212] Trial 15 pruned. \n",
      "[I 2025-10-13 15:24:47,584] Trial 16 pruned. \n",
      "[I 2025-10-13 15:24:49,613] Trial 17 pruned. \n",
      "[I 2025-10-13 15:24:50,213] Trial 18 pruned. \n",
      "[I 2025-10-13 15:24:50,553] Trial 19 pruned. \n",
      "[I 2025-10-13 15:24:51,353] Trial 20 pruned. \n",
      "[I 2025-10-13 15:26:07,579] Trial 21 finished with value: 0.8833333333333333 and parameters: {'hidden': 192, 'layers': 5, 'dropout': 0.13852254326914037, 'lr': 0.001158939391428457, 'weight_decay': 6.069973679816261e-06}. Best is trial 4 with value: 0.9.\n",
      "[I 2025-10-13 15:26:09,796] Trial 22 pruned. \n",
      "[I 2025-10-13 15:26:10,521] Trial 23 pruned. \n",
      "[I 2025-10-13 15:26:11,109] Trial 24 pruned. \n",
      "[I 2025-10-13 15:26:22,032] Trial 25 pruned. \n",
      "[I 2025-10-13 15:26:22,635] Trial 26 pruned. \n",
      "[I 2025-10-13 15:26:22,912] Trial 27 pruned. \n",
      "[I 2025-10-13 15:27:24,575] Trial 28 finished with value: 0.8833333333333333 and parameters: {'hidden': 160, 'layers': 5, 'dropout': 0.031916599865659936, 'lr': 0.0008498196438919889, 'weight_decay': 2.333850753506789e-06}. Best is trial 4 with value: 0.9.\n",
      "[I 2025-10-13 15:27:24,951] Trial 29 pruned. \n",
      "[I 2025-10-13 15:27:25,461] Trial 30 pruned. \n",
      "[I 2025-10-13 15:27:27,708] Trial 31 pruned. \n",
      "[I 2025-10-13 15:27:28,430] Trial 32 pruned. \n",
      "[I 2025-10-13 15:27:29,236] Trial 33 pruned. \n",
      "[I 2025-10-13 15:27:30,020] Trial 34 pruned. \n",
      "[I 2025-10-13 15:27:30,578] Trial 35 pruned. \n",
      "[I 2025-10-13 15:27:32,416] Trial 36 pruned. \n",
      "[I 2025-10-13 15:28:46,979] Trial 37 finished with value: 0.8777777777777778 and parameters: {'hidden': 192, 'layers': 5, 'dropout': 0.22042833400946166, 'lr': 0.0014935397665330964, 'weight_decay': 1.0409806654858839e-07}. Best is trial 4 with value: 0.9.\n",
      "[I 2025-10-13 15:28:47,570] Trial 38 pruned. \n",
      "[I 2025-10-13 15:28:48,295] Trial 39 pruned. \n"
     ]
    }
   ],
   "source": [
    "study_gcn = optimize_gcn(train_loader_graph, val_loader_graph, n_trials=40, max_epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4574b11c-6838-463e-934d-8a0502dfde88",
   "metadata": {},
   "source": [
    "# Entrenamiento de modelos con hiperparámetros encontrados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "05b2991c-e75c-4edc-8859-9a3f12b38f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, math, random, numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, precision_score, recall_score, f1_score,\n",
    "    accuracy_score, balanced_accuracy_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ec992ddc-1075-423b-8fee-77e676af7917",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42  # o el número que prefieras\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Opcional pero recomendado para reproducibilidad completa:\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7277ccd3-7628-4227-9fc2-edefe1a59c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_metrics(y_true, y_pred, n_classes=4):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(n_classes)))\n",
    "    tp = np.diag(cm).astype(float)\n",
    "    fp = cm.sum(axis=0) - tp\n",
    "    fn = cm.sum(axis=1) - tp\n",
    "    tn = cm.sum() - (tp + fp + fn)\n",
    "    eps = 1e-8\n",
    "    specificity_macro = np.mean(tn / (tn + fp + eps))\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision_macro\": precision_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"recall_macro\": recall_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"specificity_macro\": float(specificity_macro),\n",
    "        \"f1_macro\": f1_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"balanced_accuracy\": balanced_accuracy_score(y_true, y_pred),\n",
    "        \"confusion_matrix\": cm.tolist()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2a6183c7-3bbf-48a2-a5e8-e2ad9a61d9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=16):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))  # (1, L, d_model)\n",
    "    def forward(self, x):\n",
    "        return self.dropout(x + self.pe[:, :x.size(1), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ffb5b600-24bc-48fa-82ce-33595eb5bb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Modelo EXACTO usado en HPO (no el baseline) ===\n",
    "class HPOTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Coincide con el modelo definido en optimize_transformer:\n",
    "    - input_proj -> PE -> TransformerEncoder(nlayers, nhead, dim_feedforward=ff, dropout)\n",
    "    - pooling: 'mean' o 'last'\n",
    "    - LayerNorm + Linear final\n",
    "    \"\"\"\n",
    "    def __init__(self, in_feat=258, seq_len=16, num_classes=4,\n",
    "                 d_model=128, nhead=4, num_layers=2, ff=256,\n",
    "                 dropout=0.1, pool=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.pool = pool\n",
    "        self.proj = nn.Linear(in_feat, d_model)\n",
    "        self.pe = PositionalEncoding(d_model, dropout, max_len=seq_len)\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=ff,\n",
    "            dropout=dropout, batch_first=True, norm_first=True\n",
    "        )\n",
    "        self.enc = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 16, 258)\n",
    "        x = self.proj(x)\n",
    "        x = self.pe(x)\n",
    "        x = self.enc(x)  # (B, L, d)\n",
    "        if self.pool == \"mean\":\n",
    "            x = x.mean(dim=1)\n",
    "        else:  # 'last'\n",
    "            x = x[:, -1, :]\n",
    "        x = self.norm(x)\n",
    "        return self.head(x)  # (B, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b2553512-6a4b-4c6e-86da-7749460b78c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_save_transformer_hpo_from_json(\n",
    "    params_path: str,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    device: torch.device,\n",
    "    out_prefix: str = \"Transformer_HPO_final\",\n",
    "    default_epochs: int = 90,\n",
    "    patience: int = 12  # número de epochs sin mejora antes de detener\n",
    "):\n",
    "    with open(params_path, \"r\") as f:\n",
    "        blob = json.load(f)\n",
    "    hp = blob.get(\"best_params\", blob)\n",
    "\n",
    "    required = [\"d_model\", \"nhead\", \"num_layers\", \"dropout\"]\n",
    "    missing = [k for k in required if k not in hp]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Faltan hiperparámetros en JSON: {missing}\")\n",
    "\n",
    "    ff = hp.get(\"ff\", hp.get(\"dim_feedforward\", None))\n",
    "    if ff is None:\n",
    "        raise ValueError(\"El JSON no contiene 'ff' ni 'dim_feedforward'.\")\n",
    "\n",
    "    pool = hp.get(\"pool\", \"mean\")\n",
    "    lr = hp.get(\"lr\", 1e-3)\n",
    "    weight_decay = hp.get(\"weight_decay\", 0.0)\n",
    "    epochs = int(hp.get(\"epochs\", hp.get(\"max_epochs\", default_epochs)))\n",
    "\n",
    "    model = HPOTransformer(\n",
    "        in_feat=258, seq_len=16, num_classes=4,\n",
    "        d_model=hp[\"d_model\"], nhead=hp[\"nhead\"], num_layers=hp[\"num_layers\"],\n",
    "        ff=ff, dropout=hp[\"dropout\"], pool=pool\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    train_losses, val_losses, val_accuracies = [], [], []\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_epoch, best_state = 0, None\n",
    "    epochs_no_improve = 0  # 👈 contador de paciencia\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        correct_train, total_train, running_loss_train = 0, 0, 0.0\n",
    "\n",
    "        for X, y in train_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss_train += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total_train += y.size(0)\n",
    "            correct_train += predicted.eq(y).sum().item()\n",
    "\n",
    "        train_loss = running_loss_train / len(train_loader)\n",
    "        train_acc = 100. * correct_train / total_train\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        correct_val, total_val, running_loss_val = 0, 0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for X, y in val_loader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                outputs = model(X)\n",
    "                loss = criterion(outputs, y)\n",
    "                running_loss_val += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total_val += y.size(0)\n",
    "                correct_val += predicted.eq(y).sum().item()\n",
    "\n",
    "        val_loss = running_loss_val / len(val_loader)\n",
    "\n",
    "        y_true, y_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for X, y in val_loader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                outputs = model(X)\n",
    "                preds = outputs.argmax(1).cpu().numpy()\n",
    "                y_true.extend(y.cpu().numpy())\n",
    "                y_pred.extend(preds)\n",
    "        \n",
    "        epoch_metrics = compute_metrics(y_true, y_pred, labels=list(range(4)))\n",
    "        val_acc = 100. * epoch_metrics[\"accuracy\"]  # para mantener el mismo formato %\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        # --- Seguimiento del mejor modelo ---\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            best_train_metrics = {\"accuracy\": train_acc}\n",
    "            best_val_metrics = {\"accuracy\": val_acc}\n",
    "            best_state = model.state_dict().copy()\n",
    "            epochs_no_improve = 0  # 👈 reset paciencia\n",
    "        else:\n",
    "            epochs_no_improve += 1  # 👈 aumenta paciencia\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"\\n⏹️ Early stopping en epoch {epoch+1} (sin mejora en {patience} epochs).\")\n",
    "                break  # 👈 detiene el entrenamiento\n",
    "\n",
    "        marker = \"*\" if epoch == best_epoch else \"\"\n",
    "        print(f\"[Transformer HPO] Epoch {epoch+1}/{epochs} {marker} | \"\n",
    "              f\"TrainLoss={train_loss:.4f} | ValLoss={val_loss:.4f}\")\n",
    "\n",
    "    if best_state is None:\n",
    "        best_state = model.state_dict()\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    print(f\"\\n🏆 Mejor epoch: {best_epoch+1} | ValLoss={best_val_loss:.4f}\")\n",
    "\n",
    "    def _metrics_on(loader):\n",
    "        model.eval()\n",
    "        y_true, y_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for X, y in loader:\n",
    "                X = X.to(device)\n",
    "                logits = model(X)\n",
    "                preds = logits.argmax(1).cpu().numpy()\n",
    "                y_true.extend(y.numpy())\n",
    "                y_pred.extend(preds)\n",
    "        return _compute_metrics(y_true, y_pred, n_classes=4)\n",
    "\n",
    "    metrics_train = _metrics_on(train_loader)\n",
    "    metrics_val = _metrics_on(val_loader)\n",
    "\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(train_losses, label=\"Train Loss\", color=\"blue\")\n",
    "    plt.plot(val_losses, label=\"Val Loss\", color=\"orange\")\n",
    "    plt.axvline(best_epoch, color='red', linestyle='--', label=f\"Best Epoch ({best_epoch+1})\")\n",
    "    plt.title(\"Transformer (HPO) - Train vs Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\")\n",
    "    plt.legend(); plt.grid(True); plt.tight_layout()\n",
    "    plt.savefig(f\"{out_prefix}_loss.png\")\n",
    "    plt.close()\n",
    "\n",
    "    torch.save(model.state_dict(), f\"{out_prefix}_best.pt\")\n",
    "    report = {\n",
    "        \"hyperparams\": hp,\n",
    "        \"optimizer\": {\"lr\": lr, \"weight_decay\": weight_decay, \"epochs\": epochs},\n",
    "        \"best_epoch\": best_epoch + 1,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "        \"train_metrics\": metrics_train,\n",
    "        \"val_metrics\": metrics_val,\n",
    "        \"loss_curves\": {\"train\": train_losses, \"val\": val_losses, \"val_acc\": val_accuracies},\n",
    "        \"artifacts\": {\n",
    "            \"weights_pt\": f\"{out_prefix}_best.pt\",\n",
    "            \"loss_png\": f\"{out_prefix}_loss.png\"\n",
    "        }\n",
    "    }\n",
    "    with open(f\"{out_prefix}_metrics.json\", \"w\") as f:\n",
    "        json.dump(report, f, indent=4)\n",
    "\n",
    "    print(f\"✅ Guardado: {out_prefix}_best.pt | {out_prefix}_loss.png | {out_prefix}_metrics.json\")\n",
    "    print(f\"📈 Val Acc (best): {metrics_val['accuracy']:.4f} | Train Acc (final): {metrics_train['accuracy']:.4f}\")\n",
    "    return model, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "483c05cf-1173-46f5-9714-e58acf238230",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsforerobiomed/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Transformer HPO] Epoch 1/90 * | TrainLoss=1.5729 | ValLoss=1.4000\n",
      "[Transformer HPO] Epoch 2/90 * | TrainLoss=1.3899 | ValLoss=1.3331\n",
      "[Transformer HPO] Epoch 3/90 * | TrainLoss=1.3109 | ValLoss=1.2243\n",
      "[Transformer HPO] Epoch 4/90 * | TrainLoss=1.1013 | ValLoss=0.7647\n",
      "[Transformer HPO] Epoch 5/90 * | TrainLoss=0.8571 | ValLoss=0.6187\n",
      "[Transformer HPO] Epoch 6/90 * | TrainLoss=0.7537 | ValLoss=0.5559\n",
      "[Transformer HPO] Epoch 7/90  | TrainLoss=0.6469 | ValLoss=0.5631\n",
      "[Transformer HPO] Epoch 8/90 * | TrainLoss=0.6248 | ValLoss=0.4622\n",
      "[Transformer HPO] Epoch 9/90  | TrainLoss=0.6482 | ValLoss=0.5061\n",
      "[Transformer HPO] Epoch 10/90  | TrainLoss=0.5708 | ValLoss=0.4742\n",
      "[Transformer HPO] Epoch 11/90  | TrainLoss=0.5746 | ValLoss=0.6043\n",
      "[Transformer HPO] Epoch 12/90  | TrainLoss=0.5088 | ValLoss=0.5396\n",
      "[Transformer HPO] Epoch 13/90 * | TrainLoss=0.5021 | ValLoss=0.4095\n",
      "[Transformer HPO] Epoch 14/90  | TrainLoss=0.4698 | ValLoss=0.4631\n",
      "[Transformer HPO] Epoch 15/90  | TrainLoss=0.4158 | ValLoss=0.4709\n",
      "[Transformer HPO] Epoch 16/90  | TrainLoss=0.4516 | ValLoss=0.4218\n",
      "[Transformer HPO] Epoch 17/90  | TrainLoss=0.4253 | ValLoss=0.6810\n",
      "[Transformer HPO] Epoch 18/90 * | TrainLoss=0.4061 | ValLoss=0.4063\n",
      "[Transformer HPO] Epoch 19/90  | TrainLoss=0.3596 | ValLoss=0.6821\n",
      "[Transformer HPO] Epoch 20/90  | TrainLoss=0.4098 | ValLoss=0.4424\n",
      "[Transformer HPO] Epoch 21/90  | TrainLoss=0.3524 | ValLoss=0.4794\n",
      "[Transformer HPO] Epoch 22/90  | TrainLoss=0.3090 | ValLoss=0.4674\n",
      "[Transformer HPO] Epoch 23/90  | TrainLoss=0.2956 | ValLoss=0.4499\n",
      "[Transformer HPO] Epoch 24/90  | TrainLoss=0.3324 | ValLoss=0.6563\n",
      "[Transformer HPO] Epoch 25/90 * | TrainLoss=0.4031 | ValLoss=0.3637\n",
      "[Transformer HPO] Epoch 26/90 * | TrainLoss=0.3389 | ValLoss=0.3543\n",
      "[Transformer HPO] Epoch 27/90  | TrainLoss=0.2700 | ValLoss=0.5108\n",
      "[Transformer HPO] Epoch 28/90  | TrainLoss=0.2388 | ValLoss=0.5170\n",
      "[Transformer HPO] Epoch 29/90  | TrainLoss=0.2620 | ValLoss=0.4122\n",
      "[Transformer HPO] Epoch 30/90  | TrainLoss=0.2195 | ValLoss=0.4073\n",
      "[Transformer HPO] Epoch 31/90  | TrainLoss=0.1949 | ValLoss=0.4713\n",
      "[Transformer HPO] Epoch 32/90  | TrainLoss=0.2617 | ValLoss=0.4263\n",
      "[Transformer HPO] Epoch 33/90 * | TrainLoss=0.2378 | ValLoss=0.3509\n",
      "[Transformer HPO] Epoch 34/90  | TrainLoss=0.2168 | ValLoss=0.4330\n",
      "[Transformer HPO] Epoch 35/90  | TrainLoss=0.2031 | ValLoss=0.5733\n",
      "[Transformer HPO] Epoch 36/90  | TrainLoss=0.1952 | ValLoss=0.3686\n",
      "[Transformer HPO] Epoch 37/90  | TrainLoss=0.1704 | ValLoss=0.5601\n",
      "[Transformer HPO] Epoch 38/90 * | TrainLoss=0.2655 | ValLoss=0.3406\n",
      "[Transformer HPO] Epoch 39/90  | TrainLoss=0.2082 | ValLoss=0.3449\n",
      "[Transformer HPO] Epoch 40/90  | TrainLoss=0.1637 | ValLoss=0.6067\n",
      "[Transformer HPO] Epoch 41/90  | TrainLoss=0.1586 | ValLoss=0.3857\n",
      "[Transformer HPO] Epoch 42/90  | TrainLoss=0.1930 | ValLoss=0.5779\n",
      "[Transformer HPO] Epoch 43/90  | TrainLoss=0.1774 | ValLoss=0.4288\n",
      "[Transformer HPO] Epoch 44/90 * | TrainLoss=0.1804 | ValLoss=0.3062\n",
      "[Transformer HPO] Epoch 45/90  | TrainLoss=0.1486 | ValLoss=0.3521\n",
      "[Transformer HPO] Epoch 46/90 * | TrainLoss=0.1612 | ValLoss=0.2768\n",
      "[Transformer HPO] Epoch 47/90  | TrainLoss=0.1597 | ValLoss=0.5547\n",
      "[Transformer HPO] Epoch 48/90  | TrainLoss=0.2059 | ValLoss=0.4991\n",
      "[Transformer HPO] Epoch 49/90  | TrainLoss=0.2137 | ValLoss=0.6817\n",
      "[Transformer HPO] Epoch 50/90  | TrainLoss=0.1847 | ValLoss=0.3282\n",
      "[Transformer HPO] Epoch 51/90  | TrainLoss=0.1946 | ValLoss=0.4432\n",
      "[Transformer HPO] Epoch 52/90  | TrainLoss=0.1465 | ValLoss=0.4317\n",
      "[Transformer HPO] Epoch 53/90  | TrainLoss=0.1367 | ValLoss=0.5721\n",
      "[Transformer HPO] Epoch 54/90  | TrainLoss=0.1339 | ValLoss=0.4432\n",
      "[Transformer HPO] Epoch 55/90  | TrainLoss=0.1068 | ValLoss=0.5342\n",
      "[Transformer HPO] Epoch 56/90  | TrainLoss=0.1159 | ValLoss=0.5638\n",
      "[Transformer HPO] Epoch 57/90  | TrainLoss=0.1726 | ValLoss=0.6381\n",
      "\n",
      "⏹️ Early stopping en epoch 58 (sin mejora en 12 epochs).\n",
      "\n",
      "🏆 Mejor epoch: 46 | ValLoss=0.2768\n",
      "✅ Guardado: Transformer_HPO_best_best.pt | Transformer_HPO_best_loss.png | Transformer_HPO_best_metrics.json\n",
      "📈 Val Acc (best): 0.8833 | Train Acc (final): 0.9775\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_tf, report_tf = train_eval_save_transformer_hpo_from_json(\n",
    "    params_path=\"hpo_transformer_best.json\",  # el que guardó optimize_transformer\n",
    "    train_loader=train_loader_seq,\n",
    "    val_loader=val_loader_seq,\n",
    "    device=device,\n",
    "    out_prefix=\"Transformer_HPO_best\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f712ce46-f0c9-48ed-ac23-77df6d6ef2c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
